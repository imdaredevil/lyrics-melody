{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_vanilla_performer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "41EWXAeL2nYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925e290b-742b-43c5-aaac-282d2f6c342f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install /content/drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
        "!git clone https://github.com/gulnazaki/performer-pytorch.git\n",
        "!pip install ./performer-pytorch\n",
        "!pip install deepspeed==0.3.10\n",
        "!pip install allennlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.3.0\n",
            "Cloning into 'performer-pytorch'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 511 (delta 55), reused 58 (delta 29), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (511/511), 35.02 MiB | 48.45 MiB/s, done.\n",
            "Resolving deltas: 100% (338/338), done.\n",
            "Processing ./performer-pytorch\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/86/f1df73868c1c433a9184d94e86cdd970951ecf14d8b556b41302febb9a12/local_attention-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (1.7.1+cu101)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (1.19.5)\n",
            "Building wheels for collected packages: performer-pytorch, axial-positional-embedding\n",
            "  Building wheel for performer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for performer-pytorch: filename=performer_pytorch-0.15.0-cp37-none-any.whl size=12495 sha256=e89b330694d02923c20f4f3ecb60a9ec7b39af9c5505b5af105478727903ecfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/73/93/041f7dd55e6f33ef90455a36e217ed2811faeb9dd9fe343159\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp37-none-any.whl size=2905 sha256=777358dd41aa551d933faadaa606d231a8f120185522d15edd0cff66b5675972\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built performer-pytorch axial-positional-embedding\n",
            "Installing collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 local-attention-1.2.2 performer-pytorch-0.15.0\n",
            "Collecting deepspeed==0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bd/b2b544ca1286252e9a559b1508e64d0d61af7a73b6bf6737568858128e11/deepspeed-0.3.10.tar.gz (281kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (0.8.2+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 40.6MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed==0.3.10) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed==0.3.10) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed==0.3.10) (54.0.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.10-cp37-none-any.whl size=272627 sha256=30a03c391e74be7941c802a515d80021ee0e59829e1e0dbbd1d22e2894f2dc9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/3c/9c/39a16330874a2c55f61fe2c501e120258975d509177ffdcda7\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.10 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/bd/c75fa01e3deb9322b637fe0be45164b40d43747661aca9195b5fb334947c/allennlp-2.1.0-py3-none-any.whl (585kB)\n",
            "\u001b[K     |████████████████████████████████| 593kB 18.9MB/s \n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: torchvision<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.8.2+cu101)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/52/0b762e2ae28bc880ff2b5cd0c4515c6cb86d0d07fa92fe342beb06bea66d/boto3-1.17.19-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.8)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Collecting transformers<4.4,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: torch<1.8.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.7.1+cu101)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.9.0,>=0.8.1->allennlp) (7.0.0)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/84/3f9f99ff67e7f146fb8b785db0920365eb10ba477db5b25a3d8e04b6d8d9/botocore-1.20.19-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 54.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (54.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.19->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.4,>=4.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.4,>=4.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<4.4,>=4.1->allennlp) (3.4.0)\n",
            "Building wheels for collected packages: overrides, jsonnet, sacremoses\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=b5ea19fdcfaa065fa176b21dd6a1fec2af6aaedf6f8d88007499a95082ae594e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388779 sha256=14f9cc40df1329f7d6e97702f70c3b9608bb665f726524411e5e14885bfa9b32\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GPe_Vj5fUYd",
        "outputId": "c06cc485-7d97-4d12-9139-ebf558aa51c2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar  2 16:50:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChU7fY2jfdH5",
        "outputId": "3659a32d-db99-464d-ce95-758f47fce624"
      },
      "source": [
        "%%writefile ds_config.json\n",
        "\n",
        "{\n",
        "  \"train_batch_size\": 8,\n",
        "  \"gradient_accumulation_steps\": 8,\n",
        "  \"steps_per_print\": 20,\n",
        "  \"gradient_clipping\": 0.5,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"Adam\",\n",
        "    \"params\": {\n",
        "      \"lr\": 0.001,\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.98\n",
        "      ],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\" : 0.1\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 0.001,\n",
        "      \"warmup_num_steps\": 100\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ds_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvcn21V2FFq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d897b833-f265-4fc2-e7c0-27c7f30df6f7"
      },
      "source": [
        "%%writefile train_vanilla_performer.py\n",
        "\n",
        "import deepspeed\n",
        "from performer_pytorch import PerformerEncDec\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from allennlp.training.metrics import BLEU\n",
        "from itertools import cycle\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Train Vanilla Performer on Lakh Midi Dataset Instruments-Lyrics-Vocal Melody')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, required=True,\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-instrumental-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer instrumental sequences')\n",
        "    \n",
        "    parser.add_argument('--max-vocal-sequence-length', '-maxv', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer vocal melody sequences')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--epochs', '-e', type=int, default=20,\n",
        "                        help='Number of epochs')\n",
        "    \n",
        "    parser.add_argument('--validate-every', '-ve', type=int, default=200,\n",
        "                        help='Validate every n batches')\n",
        "    \n",
        "    parser.add_argument('--generate-every', '-ge', type=int, default=400,\n",
        "                        help='Generate every n batches')\n",
        "\n",
        "    parser.add_argument('--print-training-loss-every', '-ptle', type=int, default=20,\n",
        "                        help='It will average training loss and print it every n steps')\n",
        "\n",
        "    parser.add_argument('--validate-size', '-vs', type=int, default=40,\n",
        "                        help='Will calculate average of validation loss for n batches')\n",
        "\n",
        "    parser.add_argument('--validate-batch-size', '-vss', type=int, default=1,\n",
        "                        help='Batch size for validation dataset')\n",
        "\n",
        "    parser.add_argument('--checkpoints-per-epoch', '-cpp', type=int, default=3,\n",
        "                        help='How many checkpoints to keep per epoch')\n",
        "    \n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank passed from distributed launcher')\n",
        "    \n",
        "    parser = deepspeed.add_config_arguments(parser)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_instrumental_length, max_vocal_length):\n",
        "        super().__init__()\n",
        "        instrumental_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.instrumental_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_instrumental_vocab = {l: w for w, l in self.instrumental_vocab.items()}\n",
        "            self.vocal_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_vocal_vocab = {l: w for w, l in self.vocal_vocab.items()}\n",
        "            \n",
        "        df = pd.read_parquet(dataset_file)\n",
        "\n",
        "        self.files = list(df['file'])\n",
        "        self.instrumental = [self.encode(json.loads(f), seq_type='instrumental', max_length=max_instrumental_length) for f in df[instrumental_type]]\n",
        "        self.vocals = [self.encode(json.loads(v), seq_type='vocals', max_length=max_vocal_length) for v in df['vocal']]\n",
        "\n",
        "        self.max_instrumental_length = max([len(f) for f in self.instrumental])\n",
        "        self.max_vocal_length = max([len(f) for f in self.vocals])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instrumental[index], self.vocals[index]), self.files[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def truncate(self, sequence, max_length):\n",
        "        if max_length >= 0:\n",
        "            return sequence[:max_length]\n",
        "        return sequence\n",
        "\n",
        "    def encode(self, event_sequence, seq_type, max_length=-1):\n",
        "        if seq_type == 'instrumental':\n",
        "            return torch.tensor([self.instrumental_vocab[e] for e in self.truncate(event_sequence, max_length - 1)] + [self.instrumental_vocab['<eos>']])\n",
        "        else:\n",
        "            return torch.tensor([self.vocal_vocab['<bos>']] + [self.vocal_vocab[e] for e in self.truncate(event_sequence, max_length - 2)] + [self.vocal_vocab['<eos>']])\n",
        "\n",
        "    def decode(self, event_sequence, seq_type, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "            true_size = size\n",
        "        if seq_type == 'instrumental':\n",
        "            return [self.reverse_instrumental_vocab[i.item()] for i in event_sequence[:true_size]]\n",
        "        else:\n",
        "            return [self.reverse_vocal_vocab[o.item()] for o in event_sequence[:true_size]]\n",
        "\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    data, files = zip(*batch)\n",
        "    instrumental, vocals = zip(*data)\n",
        "    batch_size = len(files)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        instrumental = instrumental[0].view(1, -1)\n",
        "        vocals = vocals[0].view(1, -1)\n",
        "        instrumental_masks = torch.ones_like(instrumental).bool()\n",
        "        vocal_masks = torch.ones_like(vocals).bool()\n",
        "        return (instrumental.long(), instrumental_masks), (vocals.long(), vocal_masks), files[0]\n",
        "\n",
        "    instrumental_lengths = [seq.size(0) for seq in instrumental]\n",
        "    instrumental_max_length = max(instrumental_lengths)\n",
        "    instrumental_masks = torch.arange(instrumental_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(instrumental_lengths).view(-1, 1)\n",
        "    padded_instrumental = torch.zeros(batch_size, instrumental_max_length)\n",
        "    for i, l in enumerate(instrumental_lengths):\n",
        "        padded_instrumental[i, :l] = instrumental[i]\n",
        "\n",
        "    vocal_lengths = [seq.size(0) for seq in vocals]\n",
        "    vocal_max_length = max(vocal_lengths)\n",
        "    vocal_masks = torch.arange(vocal_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(vocal_lengths).view(-1, 1)\n",
        "    padded_vocals = torch.zeros(batch_size, vocal_max_length)\n",
        "    for i, l in enumerate(vocal_lengths):\n",
        "        padded_vocals[i, :l] = vocals[i]\n",
        "\n",
        "    return (padded_instrumental.long(), instrumental_masks), (padded_vocals.long(), vocal_masks), files\n",
        "\n",
        "\n",
        "def valid_structure_metric(sequence, vocab):\n",
        "    def get_valids_for_next(e, note_was_on):\n",
        "        if e == waits[-1]:\n",
        "            valid_events = waits + offs + boundaries + phonemes + ons\n",
        "        elif e in waits:\n",
        "            valid_events = offs + boundaries + phonemes\n",
        "        elif e in ons:\n",
        "            note_was_on = True\n",
        "            valid_events = waits\n",
        "        elif e in offs:\n",
        "            note_was_on = False\n",
        "            valid_events = waits + boundaries + phonemes\n",
        "        elif e in boundaries:\n",
        "            if e == boundaries[-1]:\n",
        "                valid_events = boundaries[:-1] + phonemes\n",
        "            else:\n",
        "                valid_events = phonemes\n",
        "        else:\n",
        "            valid_events = ons\n",
        "        return valid_events, note_was_on\n",
        "\n",
        "    sequence = sequence.tolist()\n",
        "    waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "    ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "    offs = [vocab['_OFF_']]\n",
        "    boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "    phonemes = [i for e, i in vocab.items() if not '_' in e or e == '_R_']\n",
        "    \n",
        "    valid_count = 0\n",
        "    valid_events = waits + phonemes + boundaries\n",
        "    note_was_on = False\n",
        "    for e in sequence:\n",
        "        if e in valid_events and \\\n",
        "        (e not in ons or note_was_on == False) and \\\n",
        "        (e not in offs or note_was_on == True):\n",
        "            valid_count += 1\n",
        "        valid_events, note_was_on = get_valids_for_next(e, note_was_on)\n",
        "\n",
        "    size = len(sequence) - 1 if sequence[-1] == 2 else len(sequence)\n",
        "    if size == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return valid_count / size\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = MidiDataset(dataset_file=args.dataset_file,\n",
        "                          monophonic=args.monophonic,\n",
        "                          vocabulary_prefix=args.vocabulary_prefix,\n",
        "                          max_instrumental_length=args.max_instrumental_sequence_length,\n",
        "                          max_vocal_length=args.max_vocal_sequence_length)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_log_dir = os.path.join(args.save_dir, 'train')\n",
        "    val_log_dir = os.path.join(args.save_dir, 'val')\n",
        "    Path(train_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(val_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    writer_train = SummaryWriter(log_dir=train_log_dir)\n",
        "    writer_val = SummaryWriter(log_dir=val_log_dir)\n",
        "    \n",
        "    bleu = BLEU()\n",
        "\n",
        "    model = PerformerEncDec(\n",
        "        dim = 512,\n",
        "        enc_heads = 8,\n",
        "        dec_heads = 8,\n",
        "        enc_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_ff_chunks = 10,\n",
        "        dec_ff_chunks = 10,\n",
        "        enc_num_tokens = len(dataset.instrumental_vocab),\n",
        "        dec_num_tokens = len(dataset.vocal_vocab),\n",
        "        enc_max_seq_len = dataset.max_instrumental_length,\n",
        "        dec_max_seq_len = dataset.max_vocal_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_tie_embed = True,\n",
        "        dec_tie_embed = True,\n",
        "        enc_reversible = True,\n",
        "        dec_reversible = True\n",
        "    ).to(device)\n",
        "\n",
        "    model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=model, model_parameters=model.parameters(),  training_data=train_dataset, collate_fn=collate_fn_zero_pad)\n",
        "    device = model_engine.local_rank\n",
        "\n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=args.validate_batch_size, shuffle=True, collate_fn=collate_fn_zero_pad)\n",
        "    val_loader = cycle(val_loader_)\n",
        "\n",
        "    num_batches = (len(train_dataset) + trainloader.batch_size - 1) // trainloader.batch_size\n",
        "\n",
        "    save_every = num_batches // args.checkpoints_per_epoch\n",
        "    save_at = 0\n",
        "    saving_steps = []\n",
        "    for _ in range(args.checkpoints_per_epoch - 1):\n",
        "        save_at += save_every\n",
        "        saving_steps.append(save_at)\n",
        "    saving_steps.append(num_batches - 1)\n",
        "\n",
        "    print(\"\\n\", \"Dataset maximum sequence lengths - Instrumental: {}, Vocal: {}\".format(dataset.max_instrumental_length, dataset.max_vocal_length), \"\\n\")\n",
        "    print(\"\\n\", \"Train Dataset - size: {}, batches: {}\".format(len(train_dataset), num_batches), \"\\n\")\n",
        "    print(\"\\n\", \"Validate Dataset - size: {}, batches: {}\".format(len(val_dataset), len(val_loader_)), \"\\n\")\n",
        "\n",
        "    checkpoint_name, client_state = model_engine.load_checkpoint(args.save_dir, load_module_strict=False)\n",
        "    # checkpoint_name = None\n",
        "\n",
        "    if checkpoint_name is not None:\n",
        "        print(\"\\nLoaded checkpoint: {}\\n\".format(checkpoint_name))        \n",
        "        i = client_state['i']\n",
        "        i += 1\n",
        "        epoch, step = divmod(i, num_batches)\n",
        "        print(\"Epoch: {}, step: {}, i: {}\".format(epoch, step, i))\n",
        "        if step == 0:\n",
        "            print(\"Starting next epoch...\")\n",
        "            rng = torch.get_rng_state()\n",
        "            trainloader = iter(trainloader)\n",
        "        else:\n",
        "            rng = torch.load(os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "            torch.set_rng_state(rng)\n",
        "            trainloader = iter(trainloader)\n",
        "            print(\"Advancing dataloader...\")\n",
        "            for _ in range(step):\n",
        "                next(trainloader)\n",
        "    else:\n",
        "        print(\"\\nNo checkpoint found, training from scratch\\n\")\n",
        "        i = 0\n",
        "        step = 0\n",
        "        epoch = 0\n",
        "        rng = torch.get_rng_state()\n",
        "        trainloader = iter(trainloader)\n",
        "\n",
        "\n",
        "    for e in range(args.epochs - epoch):\n",
        "        running_loss = 0\n",
        "        running_loss_steps = 0\n",
        "        print(\"EPOCH: {}\".format(e + epoch))\n",
        "        while True:\n",
        "            try:\n",
        "                data = next(trainloader)\n",
        "            except StopIteration:\n",
        "                step = 0\n",
        "                rng = torch.get_rng_state()\n",
        "                trainloader = iter(trainloader)\n",
        "                break\n",
        "\n",
        "            model_engine.train()\n",
        "            (instrumental, instrumental_mask), (vocals, vocals_mask), _ = data\n",
        "            loss = model_engine(instrumental.to(device),\n",
        "                                vocals.to(device),\n",
        "                                enc_mask=instrumental_mask.to(device),\n",
        "                                dec_mask=vocals_mask.to(device))\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            running_loss_steps += 1\n",
        "            if running_loss_steps == args.print_training_loss_every or step == 0:\n",
        "                avg_loss = running_loss / running_loss_steps\n",
        "                print(\"training loss: {}\".format(avg_loss))\n",
        "                writer_train.add_scalar(\"Loss\", avg_loss, i)\n",
        "                writer_train.flush()\n",
        "                running_loss = 0\n",
        "                running_loss_steps = 0\n",
        "\n",
        "            if step % args.validate_every == 0:\n",
        "                model_engine.eval()\n",
        "                with torch.no_grad():\n",
        "                    running_eval_loss = 0\n",
        "                    for _ in range(args.validate_size):\n",
        "                        (instrumental, instrumental_mask), (vocals, vocals_mask), _ = next(val_loader)\n",
        "                        loss = model_engine(instrumental.to(device),\n",
        "                                            vocals.to(device),\n",
        "                                            enc_mask=instrumental_mask.to(device),\n",
        "                                            dec_mask=vocals_mask.to(device))\n",
        "                        running_eval_loss += loss.item()\n",
        "                    avg_eval_loss = running_eval_loss / args.validate_size\n",
        "                    print('\\n', f'validation loss: {avg_eval_loss}', '\\n')\n",
        "                    writer_val.add_scalar(\"Loss\", avg_eval_loss, i)\n",
        "                    writer_val.flush()\n",
        "                    running_eval_loss = 0\n",
        "\n",
        "            if step % args.generate_every == 0:\n",
        "                (instrumental, instrumental_mask), (expected_vocals, expected_vocals_mask), file = next(val_loader)\n",
        "                decoded_expected_vocals = dataset.decode(expected_vocals[0][1:], seq_type='vocals', mask=expected_vocals_mask[0][1:])\n",
        "\n",
        "                instrumental = instrumental[0].view(1, -1)\n",
        "                instrumental_mask = instrumental_mask[0].view(1, -1)\n",
        "                \n",
        "                # <bos> token\n",
        "                vocals_start = torch.ones(1,1).long()\n",
        "\n",
        "                vocals = model_engine.module.generate(instrumental.to(device),\n",
        "                                                      vocals_start.to(device),\n",
        "                                                      seq_len=dataset.max_vocal_length//8,\n",
        "                                                      enc_mask=instrumental_mask.to(device),\n",
        "                                                      eos_token=2)\n",
        "                decoded_vocals = dataset.decode(vocals[0], seq_type='vocals')\n",
        "\n",
        "                with open(os.path.join(args.save_dir, 'outputs.txt'), 'a') as f:\n",
        "                    f.write(\"{}:\\n\\n{}\\n----------------\\n{}\\n----------------\\n\\n\"\\\n",
        "                                    .format(file, decoded_expected_vocals, decoded_vocals))\n",
        "                \n",
        "                bleu(vocals.to(device), expected_vocals[:, 1:].to(device))\n",
        "                b = bleu.get_metric(reset=True)['BLEU']\n",
        "                print(\"BLEU metric: {}\".format(b))\n",
        "\n",
        "                vsm = valid_structure_metric(vocals[0], dataset.vocal_vocab)\n",
        "                print(\"Valid Structure Metric: {}\".format(vsm))\n",
        "                # expected_vsm = valid_structure_metric(expected_vocals[0][1:], dataset.vocal_vocab)\n",
        "                # print(\"Expected Valid Structure Metric: {} (for control)\".format(expected_vsm))\n",
        "                writer_val.add_scalar(\"BLEU\", b, i)\n",
        "                writer_val.add_scalar(\"VSM\", vsm, i)\n",
        "                writer_val.flush()\n",
        "\n",
        "            if step in saving_steps:\n",
        "                loss_to_ckpt = avg_eval_loss if avg_eval_loss is not None else loss.item()\n",
        "                ckpt_id = \"{}-{}-{}\".format(e + epoch, i, loss_to_ckpt)\n",
        "                model_engine.save_checkpoint(args.save_dir, tag='latest_ckpt', client_state = {'i': i, 'step': step, 'epoch': e + epoch})\n",
        "                print(\"\\n{}\\n\".format(ckpt_id))\n",
        "                torch.save(rng, os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "                torch.save(model_engine.module.state_dict(), os.path.join(args.save_dir, 'model.pt'))\n",
        "\n",
        "            i += 1\n",
        "            step += 1\n",
        "            "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train_vanilla_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbQEF6i8kD0h",
        "outputId": "7f97ba57-b1f5-4252-e39b-bc062efcb9a5"
      },
      "source": [
        "!deepspeed train_vanilla_performer.py -df drive/MyDrive/vanilla_performer/dataset_chords.parquet -v drive/MyDrive/vanilla_performer/vanilla_chords_ -sd drive/MyDrive/vanilla_performer/chords -ve 400 -ge 600 -cpp 6 --deepspeed --deepspeed_config ds_config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-02 16:50:50,744] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-03-02 16:50:50,778] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 train_vanilla_performer.py -df drive/MyDrive/vanilla_performer/dataset_chords.parquet -v drive/MyDrive/vanilla_performer/vanilla_chords_ -sd drive/MyDrive/vanilla_performer/chords -ve 400 -ge 600 -cpp 6 --deepspeed --deepspeed_config ds_config.json\n",
            "[2021-03-02 16:50:51,655] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.8.3\n",
            "[2021-03-02 16:50:51,655] [INFO] [launch.py:78:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-03-02 16:50:51,655] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-03-02 16:50:51,655] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-03-02 16:50:51,655] [INFO] [launch.py:100:main] dist_world_size=1\n",
            "[2021-03-02 16:50:51,655] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2021-03-02 16:51:12.027577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "[2021-03-02 16:51:28,538] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.10, git-hash=unknown, git-branch=unknown\n",
            "[2021-03-02 16:51:28,538] [INFO] [distributed.py:40:init_distributed] Initializing torch distributed with backend: nccl\n",
            "[2021-03-02 16:51:28,620] [INFO] [engine.py:72:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/fused_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
            "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 22.69339656829834 seconds\n",
            "[2021-03-02 16:51:52,498] [INFO] [engine.py:518:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2021-03-02 16:51:52,498] [INFO] [engine.py:521:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.9, 0.98]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "[2021-03-02 16:51:52,499] [INFO] [engine.py:551:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.9, 0.98]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "[2021-03-02 16:51:52,499] [INFO] [engine.py:382:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-03-02 16:51:52,499] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f30b5720990>\n",
            "[2021-03-02 16:51:52,499] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:705:print] DeepSpeedEngine configuration:\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7f313d8549d0>\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   allreduce_always_fp32 ........ False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   amp_enabled .................. False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   amp_params ................... False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   disable_allgather ............ False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   dump_state ................... False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   dynamic_loss_scale_args ...... None\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   elasticity_enabled ........... False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   fp16_enabled ................. False\n",
            "[2021-03-02 16:51:52,499] [INFO] [config.py:709:print]   global_rank .................. 0\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   gradient_accumulation_steps .. 8\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   gradient_clipping ............ 0.5\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   loss_scale ................... 0\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   memory_breakdown ............. False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   optimizer_name ............... adam\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.98], 'eps': 1e-08, 'weight_decay': 0.1, 'adam_w_mode': True}\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   pld_enabled .................. False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   pld_params ................... False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   prescale_gradients ........... False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   scheduler_name ............... WarmupLR\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 100}\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   sparse_attention ............. None\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   sparse_gradients_enabled ..... False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   steps_per_print .............. 20\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   tensorboard_enabled .......... False\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   tensorboard_output_path ...... \n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   train_batch_size ............. 8\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   train_micro_batch_size_per_gpu  1\n",
            "[2021-03-02 16:51:52,500] [INFO] [config.py:709:print]   wall_clock_breakdown ......... False\n",
            "[2021-03-02 16:51:52,501] [INFO] [config.py:709:print]   world_size ................... 1\n",
            "[2021-03-02 16:51:52,501] [INFO] [config.py:709:print]   zero_allow_untested_optimizer  False\n",
            "[2021-03-02 16:51:52,501] [INFO] [config.py:709:print]   zero_config .................. {\n",
            "    \"allgather_bucket_size\": 500000000,\n",
            "    \"allgather_partitions\": true,\n",
            "    \"contiguous_gradients\": false,\n",
            "    \"cpu_offload\": false,\n",
            "    \"elastic_checkpoint\": true,\n",
            "    \"load_from_fp32_weights\": true,\n",
            "    \"overlap_comm\": false,\n",
            "    \"reduce_bucket_size\": 500000000,\n",
            "    \"reduce_scatter\": true,\n",
            "    \"stage\": 0\n",
            "}\n",
            "[2021-03-02 16:51:52,501] [INFO] [config.py:709:print]   zero_enabled ................. False\n",
            "[2021-03-02 16:51:52,501] [INFO] [config.py:709:print]   zero_optimization_stage ...... 0\n",
            "[2021-03-02 16:51:52,501] [INFO] [config.py:715:print]   json = {\n",
            "    \"gradient_accumulation_steps\":8,\n",
            "    \"gradient_clipping\":0.5,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"adam_w_mode\":true,\n",
            "            \"betas\":[\n",
            "                0.9,\n",
            "                0.98\n",
            "            ],\n",
            "            \"eps\":1e-08,\n",
            "            \"lr\":0.001,\n",
            "            \"weight_decay\":0.1\n",
            "        },\n",
            "        \"type\":\"Adam\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":0.001,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":100\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"steps_per_print\":20,\n",
            "    \"train_batch_size\":8\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/utils...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 12.083811521530151 seconds\n",
            "\n",
            " Dataset maximum sequence lengths - Instrumental: 11731, Vocal: 6117 \n",
            "\n",
            "\n",
            " Train Dataset - size: 7654, batches: 7654 \n",
            "\n",
            "\n",
            " Validate Dataset - size: 851, batches: 851 \n",
            "\n",
            "[2021-03-02 16:52:04,864] [INFO] [engine.py:1284:_load_checkpoint] rank: 0 loading checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "Loaded checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "Epoch: 3, step: 6376, i: 29338\n",
            "Advancing dataloader...\n",
            "EPOCH: 3\n",
            "[2021-03-02 16:53:15,909] [INFO] [timer.py:166:stop] 0/20, SamplesPerSec=0.4328950398561461\n",
            "training loss: 3.108395886421204\n",
            "\n",
            " validation loss: 3.0793746650218963 \n",
            "\n",
            "[2021-03-02 16:54:14,322] [INFO] [timer.py:166:stop] 0/40, SamplesPerSec=0.4244795710935396\n",
            "training loss: 3.011098486185074\n",
            "[2021-03-02 16:54:59,445] [INFO] [timer.py:166:stop] 0/60, SamplesPerSec=0.4307695513565964\n",
            "training loss: 3.1357961297035217\n",
            "[2021-03-02 16:55:44,917] [INFO] [timer.py:166:stop] 0/80, SamplesPerSec=0.43306419356714126\n",
            "training loss: 2.987252748012543\n",
            "[2021-03-02 16:56:27,185] [INFO] [timer.py:166:stop] 0/100, SamplesPerSec=0.44069289881854806\n",
            "training loss: 3.080582523345947\n",
            "[2021-03-02 16:56:55,408] [INFO] [logging.py:60:log_dist] [Rank 0] step=3680, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 16:57:13,692] [INFO] [timer.py:166:stop] 0/120, SamplesPerSec=0.4388548914418512\n",
            "training loss: 3.182778036594391\n",
            "[2021-03-02 16:57:58,031] [INFO] [timer.py:166:stop] 0/140, SamplesPerSec=0.44058856567205457\n",
            "training loss: 3.053481924533844\n",
            "[2021-03-02 16:58:49,553] [INFO] [timer.py:166:stop] 0/160, SamplesPerSec=0.4331890696575684\n",
            "training loss: 3.0127992987632752\n",
            "[2021-03-02 16:59:38,757] [INFO] [timer.py:166:stop] 0/180, SamplesPerSec=0.4300156963371858\n",
            "training loss: 3.1644758343696595\n",
            "[2021-03-02 17:00:31,165] [INFO] [timer.py:166:stop] 0/200, SamplesPerSec=0.42457975277597143\n",
            "training loss: 3.3415579915046694\n",
            "[2021-03-02 17:01:21,150] [INFO] [timer.py:166:stop] 0/220, SamplesPerSec=0.42221386875190553\n",
            "training loss: 3.070850020647049\n",
            "BLEU metric: 0.020694391802144534\n",
            "Valid Structure Metric: 0.9785604900459418\n",
            "[2021-03-02 17:02:38,921] [INFO] [timer.py:166:stop] 0/240, SamplesPerSec=0.4237622292655565\n",
            "training loss: 3.0717536926269533\n",
            "[2021-03-02 17:03:23,342] [INFO] [timer.py:166:stop] 0/260, SamplesPerSec=0.4257046136225087\n",
            "training loss: 3.212851756811142\n",
            "[2021-03-02 17:03:50,081] [INFO] [logging.py:60:log_dist] [Rank 0] step=3700, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:04:09,443] [INFO] [timer.py:166:stop] 0/280, SamplesPerSec=0.42628045675802784\n",
            "training loss: 3.102716314792633\n",
            "[2021-03-02 17:04:56,599] [INFO] [timer.py:166:stop] 0/300, SamplesPerSec=0.42613700946246263\n",
            "training loss: 3.1878146111965178\n",
            "[2021-03-02 17:05:47,990] [INFO] [timer.py:166:stop] 0/320, SamplesPerSec=0.4236077734230132\n",
            "training loss: 3.0703212440013887\n",
            "[2021-03-02 17:06:31,387] [INFO] [timer.py:166:stop] 0/340, SamplesPerSec=0.42564534886097205\n",
            "training loss: 3.239377510547638\n",
            "[2021-03-02 17:07:15,811] [INFO] [timer.py:166:stop] 0/360, SamplesPerSec=0.42694760991978087\n",
            "training loss: 3.050030291080475\n",
            "[2021-03-02 17:07:58,432] [INFO] [timer.py:166:stop] 0/380, SamplesPerSec=0.4289955422339554\n",
            "training loss: 3.1112186670303346\n",
            "[2021-03-02 17:08:45,275] [INFO] [timer.py:166:stop] 0/400, SamplesPerSec=0.4288939225247636\n",
            "training loss: 3.125102436542511\n",
            "[2021-03-02 17:09:28,188] [INFO] [timer.py:166:stop] 0/420, SamplesPerSec=0.43053764341863704\n",
            "training loss: 3.0684587180614473\n",
            "\n",
            " validation loss: 3.134233272075653 \n",
            "\n",
            "[2021-03-02 17:10:03,494] [INFO] [logging.py:60:log_dist] [Rank 0] step=3720, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:10:24,992] [INFO] [timer.py:166:stop] 0/440, SamplesPerSec=0.43080545591135566\n",
            "training loss: 3.074930989742279\n",
            "[2021-03-02 17:11:12,654] [INFO] [timer.py:166:stop] 0/460, SamplesPerSec=0.43030534500921824\n",
            "training loss: 3.3205873727798463\n",
            "[2021-03-02 17:12:03,630] [INFO] [timer.py:166:stop] 0/480, SamplesPerSec=0.4285712910904152\n",
            "training loss: 3.0901893615722655\n",
            "[2021-03-02 17:12:50,702] [INFO] [timer.py:166:stop] 0/500, SamplesPerSec=0.4284227960203157\n",
            "training loss: 3.1684403240680696\n",
            "[2021-03-02 17:13:33,906] [INFO] [timer.py:166:stop] 0/520, SamplesPerSec=0.4296600817473034\n",
            "training loss: 3.1304631352424623\n",
            "[2021-03-02 17:14:21,896] [INFO] [timer.py:166:stop] 0/540, SamplesPerSec=0.42916654164404494\n",
            "training loss: 3.204600977897644\n",
            "[2021-03-02 17:15:10,867] [INFO] [timer.py:166:stop] 0/560, SamplesPerSec=0.42838689823025883\n",
            "training loss: 3.1124976754188536\n",
            "[2021-03-02 17:15:57,517] [INFO] [timer.py:166:stop] 0/580, SamplesPerSec=0.4283995289256508\n",
            "training loss: 3.079698920249939\n",
            "[2021-03-02 17:16:24,750] [INFO] [logging.py:60:log_dist] [Rank 0] step=3740, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:16:41,490] [INFO] [timer.py:166:stop] 0/600, SamplesPerSec=0.42923441199355805\n",
            "training loss: 2.892333996295929\n",
            "[2021-03-02 17:17:28,049] [INFO] [timer.py:166:stop] 0/620, SamplesPerSec=0.4292454995315801\n",
            "training loss: 3.1295719265937807\n",
            "[2021-03-02 17:18:15,219] [INFO] [timer.py:166:stop] 0/640, SamplesPerSec=0.429079812407017\n",
            "training loss: 3.1629501581192017\n",
            "[2021-03-02 17:19:00,495] [INFO] [timer.py:166:stop] 0/660, SamplesPerSec=0.429454421419139\n",
            "training loss: 3.1125497698783873\n",
            "[2021-03-02 17:19:47,623] [INFO] [timer.py:166:stop] 0/680, SamplesPerSec=0.42930367646080897\n",
            "training loss: 2.981667160987854\n",
            "[2021-03-02 17:20:34,849] [INFO] [timer.py:166:stop] 0/700, SamplesPerSec=0.4291357639482219\n",
            "training loss: 3.106215661764145\n",
            "[2021-03-02 17:21:19,683] [INFO] [timer.py:166:stop] 0/720, SamplesPerSec=0.42959112390511933\n",
            "training loss: 3.1102246224880217\n",
            "[2021-03-02 17:22:05,176] [INFO] [timer.py:166:stop] 0/740, SamplesPerSec=0.4298576899211825\n",
            "training loss: 3.1313734650611877\n",
            "[2021-03-02 17:22:31,568] [INFO] [logging.py:60:log_dist] [Rank 0] step=3760, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:22:47,500] [INFO] [timer.py:166:stop] 0/760, SamplesPerSec=0.43088533642750787\n",
            "training loss: 3.1351583361625672\n",
            "[2021-03-02 17:23:40,233] [INFO] [timer.py:166:stop] 0/780, SamplesPerSec=0.4293836369949979\n",
            "training loss: 3.17333722114563\n",
            "[2021-03-02 17:24:27,330] [INFO] [timer.py:166:stop] 0/800, SamplesPerSec=0.42926437832060843\n",
            "training loss: 3.123540997505188\n",
            "[2021-03-02 17:25:09,976] [INFO] [timer.py:166:stop] 0/820, SamplesPerSec=0.43015565920460974\n",
            "training loss: 3.1365073680877686\n",
            "\n",
            " validation loss: 3.13407601416111 \n",
            "\n",
            "BLEU metric: 0.0085915905020225\n",
            "Valid Structure Metric: 0.92\n",
            "[2021-03-02 17:26:52,658] [INFO] [timer.py:166:stop] 0/840, SamplesPerSec=0.4298623835258867\n",
            "training loss: 3.1863971829414366\n",
            "[2021-03-02 17:27:37,683] [INFO] [timer.py:166:stop] 0/860, SamplesPerSec=0.4301864446829169\n",
            "training loss: 3.052141034603119\n",
            "[2021-03-02 17:28:26,373] [INFO] [timer.py:166:stop] 0/880, SamplesPerSec=0.4297240617117649\n",
            "training loss: 3.2108321487903595\n",
            "[2021-03-02 17:29:12,912] [INFO] [timer.py:166:stop] 0/900, SamplesPerSec=0.4297250825733499\n",
            "training loss: 3.2698960304260254\n",
            "[2021-03-02 17:29:37,734] [INFO] [logging.py:60:log_dist] [Rank 0] step=3780, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:29:57,628] [INFO] [timer.py:166:stop] 0/920, SamplesPerSec=0.43009313673850263\n",
            "training loss: 3.2479727029800416\n",
            "[2021-03-02 17:30:39,724] [INFO] [timer.py:166:stop] 0/940, SamplesPerSec=0.4309642830480387\n",
            "training loss: 3.062272220849991\n",
            "[2021-03-02 17:31:24,102] [INFO] [timer.py:166:stop] 0/960, SamplesPerSec=0.431358667717306\n",
            "training loss: 3.084143102169037\n",
            "[2021-03-02 17:32:08,906] [INFO] [timer.py:166:stop] 0/980, SamplesPerSec=0.43165636180667966\n",
            "training loss: 2.909446197748184\n",
            "[2021-03-02 17:32:56,171] [INFO] [timer.py:166:stop] 0/1000, SamplesPerSec=0.431483011368058\n",
            "training loss: 3.2451805591583254\n",
            "[2021-03-02 17:33:41,913] [INFO] [timer.py:166:stop] 0/1020, SamplesPerSec=0.43159491034911956\n",
            "training loss: 3.2330365061759947\n",
            "[2021-03-02 17:34:25,614] [INFO] [timer.py:166:stop] 0/1040, SamplesPerSec=0.4320693673514607\n",
            "training loss: 3.178231883049011\n",
            "[2021-03-02 17:35:06,068] [INFO] [timer.py:166:stop] 0/1060, SamplesPerSec=0.43310194378347755\n",
            "training loss: 3.207500159740448\n",
            "[2021-03-02 17:35:33,278] [INFO] [logging.py:60:log_dist] [Rank 0] step=3800, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:35:49,480] [INFO] [timer.py:166:stop] 0/1080, SamplesPerSec=0.43358419537492693\n",
            "training loss: 2.996843773126602\n",
            "[2021-03-02 17:36:40,926] [INFO] [timer.py:166:stop] 0/1100, SamplesPerSec=0.43267586178590284\n",
            "training loss: 3.3039660930633543\n",
            "[2021-03-02 17:37:25,894] [INFO] [timer.py:166:stop] 0/1120, SamplesPerSec=0.4328866922370866\n",
            "training loss: 2.9882467687129974\n",
            "[2021-03-02 17:38:10,585] [INFO] [timer.py:166:stop] 0/1140, SamplesPerSec=0.4331360141158698\n",
            "training loss: 3.222990894317627\n",
            "[2021-03-02 17:38:54,343] [INFO] [timer.py:166:stop] 0/1160, SamplesPerSec=0.4335282928917964\n",
            "training loss: 3.1637990832328797\n",
            "[2021-03-02 17:39:37,867] [INFO] [timer.py:166:stop] 0/1180, SamplesPerSec=0.43394536144138424\n",
            "training loss: 3.120444822311401\n",
            "[2021-03-02 17:40:25,163] [INFO] [timer.py:166:stop] 0/1200, SamplesPerSec=0.43375604189580513\n",
            "training loss: 3.087450361251831\n",
            "[2021-03-02 17:41:10,571] [INFO] [timer.py:166:stop] 0/1220, SamplesPerSec=0.43386468281712953\n",
            "training loss: 3.4306731104850767\n",
            "\n",
            " validation loss: 3.080263376235962 \n",
            "\n",
            "[2021-03-02 17:41:46,912] [INFO] [logging.py:60:log_dist] [Rank 0] step=3820, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-02 17:42:04,732] [INFO] [timer.py:166:stop] 0/1240, SamplesPerSec=0.43407389617619596\n",
            "training loss: 3.1798045158386232\n",
            "[2021-03-02 17:42:51,658] [INFO] [timer.py:166:stop] 0/1260, SamplesPerSec=0.43394674469565325\n",
            "training loss: 3.157326412200928\n",
            "[2021-03-02 17:43:28,693] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "3-30615-3.080263376235962\n",
            "\n",
            "EPOCH: 4\n",
            "training loss: 3.131054639816284\n",
            "\n",
            " validation loss: 3.194584384560585 \n",
            "\n",
            "BLEU metric: 0.06676251990366067\n",
            "Valid Structure Metric: 0.9790575916230366\n",
            "[2021-03-02 17:44:37,950] [INFO] [timer.py:166:stop] 0/1280, SamplesPerSec=0.4347366990631594\n",
            "training loss: 3.2253366708755493\n",
            "[2021-03-02 17:45:24,761] [INFO] [timer.py:166:stop] 0/1300, SamplesPerSec=0.43461981871980776\n",
            "training loss: 3.1470210552215576\n",
            "[2021-03-02 17:46:14,338] [INFO] [timer.py:166:stop] 0/1320, SamplesPerSec=0.43411052348368157\n",
            "training loss: 3.098112154006958\n",
            "[2021-03-02 17:46:57,051] [INFO] [timer.py:166:stop] 0/1340, SamplesPerSec=0.4345844600638606\n",
            "training loss: 3.1287763714790344\n",
            "[2021-03-02 17:47:42,232] [INFO] [timer.py:166:stop] 0/1360, SamplesPerSec=0.4347016100722633\n",
            "training loss: 3.039785611629486\n",
            "[2021-03-02 17:48:28,384] [INFO] [timer.py:166:stop] 0/1380, SamplesPerSec=0.4346822520382988\n",
            "[2021-03-02 17:48:54,765] [INFO] [logging.py:60:log_dist] [Rank 0] step=3840, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1156298518180847\n",
            "[2021-03-02 17:49:12,530] [INFO] [timer.py:166:stop] 0/1400, SamplesPerSec=0.43493478129640917\n",
            "training loss: 3.088582181930542\n",
            "[2021-03-02 17:49:59,138] [INFO] [timer.py:166:stop] 0/1420, SamplesPerSec=0.434851868220297\n",
            "training loss: 3.0498174905776976\n",
            "[2021-03-02 17:50:47,541] [INFO] [timer.py:166:stop] 0/1440, SamplesPerSec=0.4345355192653961\n",
            "training loss: 2.982250118255615\n",
            "[2021-03-02 17:51:22,311] [INFO] [timer.py:166:stop] 0/1460, SamplesPerSec=0.4359984762772148\n",
            "training loss: 3.1239917278289795\n",
            "[2021-03-02 17:52:12,626] [INFO] [timer.py:166:stop] 0/1480, SamplesPerSec=0.4354280099731125\n",
            "training loss: 3.262128782272339\n",
            "[2021-03-02 17:52:50,321] [INFO] [timer.py:166:stop] 0/1500, SamplesPerSec=0.43647327509175754\n",
            "training loss: 2.8576696515083313\n",
            "[2021-03-02 17:53:33,700] [INFO] [timer.py:166:stop] 0/1520, SamplesPerSec=0.4367803875065368\n",
            "training loss: 3.0163882672786713\n",
            "[2021-03-02 17:54:19,532] [INFO] [timer.py:166:stop] 0/1540, SamplesPerSec=0.43677547475359985\n",
            "[2021-03-02 17:54:40,150] [INFO] [logging.py:60:log_dist] [Rank 0] step=3860, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.211431622505188\n",
            "[2021-03-02 17:54:58,168] [INFO] [timer.py:166:stop] 0/1560, SamplesPerSec=0.4376535615565422\n",
            "training loss: 2.9814767479896545\n",
            "[2021-03-02 17:55:44,063] [INFO] [timer.py:166:stop] 0/1580, SamplesPerSec=0.4376299403626914\n",
            "training loss: 3.329012852907181\n",
            "[2021-03-02 17:56:34,846] [INFO] [timer.py:166:stop] 0/1600, SamplesPerSec=0.4370219215981813\n",
            "training loss: 3.1834262013435364\n",
            "[2021-03-02 17:57:20,549] [INFO] [timer.py:166:stop] 0/1620, SamplesPerSec=0.43702954439611497\n",
            "training loss: 3.2769129037857057\n",
            "[2021-03-02 17:58:09,058] [INFO] [timer.py:166:stop] 0/1640, SamplesPerSec=0.43670993787667844\n",
            "training loss: 3.077014410495758\n",
            "[2021-03-02 17:58:56,791] [INFO] [timer.py:166:stop] 0/1660, SamplesPerSec=0.43648761368479944\n",
            "training loss: 3.155554234981537\n",
            "\n",
            " validation loss: 3.132918208837509 \n",
            "\n",
            "[2021-03-02 17:59:51,674] [INFO] [timer.py:166:stop] 0/1680, SamplesPerSec=0.4364472814914572\n",
            "training loss: 3.092400473356247\n",
            "[2021-03-02 18:00:38,697] [INFO] [timer.py:166:stop] 0/1700, SamplesPerSec=0.43631320284743125\n",
            "[2021-03-02 18:01:04,812] [INFO] [logging.py:60:log_dist] [Rank 0] step=3880, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9069553196430205\n",
            "[2021-03-02 18:01:23,106] [INFO] [timer.py:166:stop] 0/1720, SamplesPerSec=0.4364718957431433\n",
            "training loss: 3.03648784160614\n",
            "[2021-03-02 18:02:09,691] [INFO] [timer.py:166:stop] 0/1740, SamplesPerSec=0.43638853774435554\n",
            "training loss: 3.135741639137268\n",
            "[2021-03-02 18:02:54,995] [INFO] [timer.py:166:stop] 0/1760, SamplesPerSec=0.4364459527693499\n",
            "training loss: 3.195453906059265\n",
            "[2021-03-02 18:03:42,150] [INFO] [timer.py:166:stop] 0/1780, SamplesPerSec=0.43630368550532833\n",
            "training loss: 3.1631885409355163\n",
            "[2021-03-02 18:04:34,743] [INFO] [timer.py:166:stop] 0/1800, SamplesPerSec=0.43559007892179086\n",
            "training loss: 3.1978851318359376\n",
            "[2021-03-02 18:05:21,371] [INFO] [timer.py:166:stop] 0/1820, SamplesPerSec=0.4355160278663738\n",
            "training loss: 3.1029225826263427\n",
            "[2021-03-02 18:06:12,717] [INFO] [timer.py:166:stop] 0/1840, SamplesPerSec=0.4349573126403261\n",
            "training loss: 3.2057900369167327\n",
            "[2021-03-02 18:06:58,104] [INFO] [timer.py:166:stop] 0/1860, SamplesPerSec=0.43501807073973847\n",
            "[2021-03-02 18:07:26,372] [INFO] [logging.py:60:log_dist] [Rank 0] step=3900, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1068002700805666\n",
            "BLEU metric: 1.0452636671114437e-22\n",
            "Valid Structure Metric: 1.024390243902439\n",
            "[2021-03-02 18:07:44,095] [INFO] [timer.py:166:stop] 0/1880, SamplesPerSec=0.4352090746591496\n",
            "training loss: 3.1066776394844053\n",
            "[2021-03-02 18:08:26,192] [INFO] [timer.py:166:stop] 0/1900, SamplesPerSec=0.43559462082747596\n",
            "training loss: 3.1101284265518188\n",
            "[2021-03-02 18:09:13,887] [INFO] [timer.py:166:stop] 0/1920, SamplesPerSec=0.43541876009196706\n",
            "training loss: 3.059196043014526\n",
            "[2021-03-02 18:09:57,287] [INFO] [timer.py:166:stop] 0/1940, SamplesPerSec=0.435666917428462\n",
            "training loss: 3.052641248703003\n",
            "[2021-03-02 18:10:50,293] [INFO] [timer.py:166:stop] 0/1960, SamplesPerSec=0.43498005252188987\n",
            "training loss: 3.1497321009635924\n",
            "[2021-03-02 18:11:34,036] [INFO] [timer.py:166:stop] 0/1980, SamplesPerSec=0.4351943002133641\n",
            "training loss: 3.104875832796097\n",
            "[2021-03-02 18:12:23,317] [INFO] [timer.py:166:stop] 0/2000, SamplesPerSec=0.4348796277374238\n",
            "training loss: 3.0900054454803465\n",
            "[2021-03-02 18:13:12,969] [INFO] [timer.py:166:stop] 0/2020, SamplesPerSec=0.4345369805630166\n",
            "[2021-03-02 18:13:43,201] [INFO] [logging.py:60:log_dist] [Rank 0] step=3920, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1708172798156737\n",
            "[2021-03-02 18:13:57,664] [INFO] [timer.py:166:stop] 0/2040, SamplesPerSec=0.43466058879940933\n",
            "training loss: 3.0003253877162934\n",
            "[2021-03-02 18:14:44,303] [INFO] [timer.py:166:stop] 0/2060, SamplesPerSec=0.4346032837373164\n",
            "training loss: 2.8728422284126283\n",
            "\n",
            " validation loss: 3.137852391600609 \n",
            "\n",
            "[2021-03-02 18:15:38,815] [INFO] [timer.py:166:stop] 0/2080, SamplesPerSec=0.4347142368150755\n",
            "training loss: 2.973499524593353\n",
            "[2021-03-02 18:16:28,843] [INFO] [timer.py:166:stop] 0/2100, SamplesPerSec=0.43435262807634145\n",
            "training loss: 2.987714636325836\n",
            "[2021-03-02 18:17:12,553] [INFO] [timer.py:166:stop] 0/2120, SamplesPerSec=0.4345609987328796\n",
            "training loss: 3.008734440803528\n",
            "[2021-03-02 18:18:00,062] [INFO] [timer.py:166:stop] 0/2140, SamplesPerSec=0.43443003715312944\n",
            "training loss: 3.301787185668945\n",
            "[2021-03-02 18:18:43,985] [INFO] [timer.py:166:stop] 0/2160, SamplesPerSec=0.43461521805493614\n",
            "training loss: 3.13900066614151\n",
            "[2021-03-02 18:19:31,234] [INFO] [timer.py:166:stop] 0/2180, SamplesPerSec=0.4345086943991282\n",
            "[2021-03-02 18:20:00,653] [INFO] [logging.py:60:log_dist] [Rank 0] step=3940, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.934684318304062\n",
            "[2021-03-02 18:20:20,052] [INFO] [timer.py:166:stop] 0/2200, SamplesPerSec=0.4342694902118211\n",
            "training loss: 3.180446135997772\n",
            "[2021-03-02 18:21:14,169] [INFO] [timer.py:166:stop] 0/2220, SamplesPerSec=0.43358524275800453\n",
            "training loss: 2.901863318681717\n",
            "[2021-03-02 18:22:03,598] [INFO] [timer.py:166:stop] 0/2240, SamplesPerSec=0.43330823642593863\n",
            "training loss: 3.19517502784729\n",
            "[2021-03-02 18:22:49,318] [INFO] [timer.py:166:stop] 0/2260, SamplesPerSec=0.4333447692367944\n",
            "training loss: 3.3123242259025574\n",
            "[2021-03-02 18:23:41,314] [INFO] [timer.py:166:stop] 0/2280, SamplesPerSec=0.43286376645423164\n",
            "training loss: 3.19992915391922\n",
            "[2021-03-02 18:24:29,799] [INFO] [timer.py:166:stop] 0/2300, SamplesPerSec=0.43267806720818447\n",
            "training loss: 3.0957781434059144\n",
            "[2021-03-02 18:25:09,402] [INFO] [timer.py:166:stop] 0/2320, SamplesPerSec=0.43321364773663573\n",
            "training loss: 3.0582329511642454\n",
            "[2021-03-02 18:25:46,959] [INFO] [timer.py:166:stop] 0/2340, SamplesPerSec=0.4339060242071902\n",
            "[2021-03-02 18:26:12,866] [INFO] [logging.py:60:log_dist] [Rank 0] step=3960, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.017585241794586\n",
            "[2021-03-02 18:26:32,237] [INFO] [timer.py:166:stop] 0/2360, SamplesPerSec=0.43397133595172166\n",
            "training loss: 3.021470379829407\n",
            "[2021-03-02 18:27:12,352] [INFO] [timer.py:166:stop] 0/2380, SamplesPerSec=0.4344449316042435\n",
            "training loss: 3.0104328095912933\n",
            "[2021-03-02 18:27:59,158] [INFO] [timer.py:166:stop] 0/2400, SamplesPerSec=0.43438448409294367\n",
            "training loss: 3.1924031734466554\n",
            "[2021-03-02 18:28:48,525] [INFO] [timer.py:166:stop] 0/2420, SamplesPerSec=0.43412532354131456\n",
            "training loss: 3.268026256561279\n",
            "[2021-03-02 18:29:36,406] [INFO] [timer.py:166:stop] 0/2440, SamplesPerSec=0.43398554174492465\n",
            "training loss: 3.2025009632110595\n",
            "[2021-03-02 18:30:28,678] [INFO] [timer.py:166:stop] 0/2460, SamplesPerSec=0.4335121795680832\n",
            "training loss: 3.1449139773845673\n",
            "\n",
            " validation loss: 3.1710638046264648 \n",
            "\n",
            "BLEU metric: 4.3322977905472585e-06\n",
            "Valid Structure Metric: 0.9528795811518325\n",
            "[2021-03-02 18:32:19,522] [INFO] [timer.py:166:stop] 0/2480, SamplesPerSec=0.4333586819632707\n",
            "training loss: 3.2618136167526246\n",
            "[2021-03-02 18:33:05,955] [INFO] [timer.py:166:stop] 0/2500, SamplesPerSec=0.43333765949903275\n",
            "[2021-03-02 18:33:33,773] [INFO] [logging.py:60:log_dist] [Rank 0] step=3980, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.2318783521652223\n",
            "[2021-03-02 18:33:51,422] [INFO] [timer.py:166:stop] 0/2520, SamplesPerSec=0.433389073435256\n",
            "training loss: 3.1289509534835815\n",
            "[2021-03-02 18:34:34,178] [INFO] [timer.py:166:stop] 0/2540, SamplesPerSec=0.4336404130119971\n",
            "[2021-03-02 18:35:05,787] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "4-31891-3.1710638046264648\n",
            "\n",
            "training loss: 3.185784709453583\n",
            "[2021-03-02 18:35:27,468] [INFO] [timer.py:166:stop] 0/2560, SamplesPerSec=0.43355969396844735\n",
            "training loss: 3.0438031017780305\n",
            "[2021-03-02 18:36:17,695] [INFO] [timer.py:166:stop] 0/2580, SamplesPerSec=0.4332613293523463\n",
            "training loss: 3.1617045640945434\n",
            "[2021-03-02 18:37:05,130] [INFO] [timer.py:166:stop] 0/2600, SamplesPerSec=0.43316952768981454\n",
            "training loss: 3.1126828849315644\n",
            "[2021-03-02 18:37:54,834] [INFO] [timer.py:166:stop] 0/2620, SamplesPerSec=0.4329166507065429\n",
            "training loss: 2.668734163045883\n",
            "[2021-03-02 18:38:38,201] [INFO] [timer.py:166:stop] 0/2640, SamplesPerSec=0.4331180701263687\n",
            "training loss: 3.2626170754432677\n",
            "[2021-03-02 18:39:24,258] [INFO] [timer.py:166:stop] 0/2660, SamplesPerSec=0.4331266771300734\n",
            "[2021-03-02 18:39:54,610] [INFO] [logging.py:60:log_dist] [Rank 0] step=4000, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1034903526306152\n",
            "[2021-03-02 18:40:12,640] [INFO] [timer.py:166:stop] 0/2680, SamplesPerSec=0.4329723642881404\n",
            "training loss: 2.926879847049713\n",
            "[2021-03-02 18:40:58,924] [INFO] [timer.py:166:stop] 0/2700, SamplesPerSec=0.43296623919859534\n",
            "training loss: 3.0774088859558106\n",
            "[2021-03-02 18:41:44,727] [INFO] [timer.py:166:stop] 0/2720, SamplesPerSec=0.43299330078404913\n",
            "training loss: 2.9908038139343263\n",
            "[2021-03-02 18:42:27,630] [INFO] [timer.py:166:stop] 0/2740, SamplesPerSec=0.43321869051426937\n",
            "training loss: 3.1197693943977356\n",
            "[2021-03-02 18:43:15,596] [INFO] [timer.py:166:stop] 0/2760, SamplesPerSec=0.4330964233959753\n",
            "training loss: 3.1844974637031553\n",
            "[2021-03-02 18:44:00,332] [INFO] [timer.py:166:stop] 0/2780, SamplesPerSec=0.43319400147513487\n",
            "training loss: 3.0714700639247896\n",
            "[2021-03-02 18:44:44,225] [INFO] [timer.py:166:stop] 0/2800, SamplesPerSec=0.43334689607181864\n",
            "training loss: 3.2223628759384155\n",
            "[2021-03-02 18:45:32,420] [INFO] [timer.py:166:stop] 0/2820, SamplesPerSec=0.4332109465128204\n",
            "[2021-03-02 18:45:59,109] [INFO] [logging.py:60:log_dist] [Rank 0] step=4020, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.8888195514678956\n",
            "[2021-03-02 18:46:17,663] [INFO] [timer.py:166:stop] 0/2840, SamplesPerSec=0.433272207422856\n",
            "training loss: 3.111038661003113\n",
            "[2021-03-02 18:47:01,993] [INFO] [timer.py:166:stop] 0/2860, SamplesPerSec=0.43339264323746535\n",
            "training loss: 3.0735725581645967\n",
            "\n",
            " validation loss: 3.060590785741806 \n",
            "\n",
            "[2021-03-02 18:47:54,584] [INFO] [timer.py:166:stop] 0/2880, SamplesPerSec=0.43365993482472776\n",
            "training loss: 3.1476613163948057\n",
            "[2021-03-02 18:48:36,792] [INFO] [timer.py:166:stop] 0/2900, SamplesPerSec=0.4339140351300015\n",
            "training loss: 3.184974706172943\n",
            "[2021-03-02 18:49:23,210] [INFO] [timer.py:166:stop] 0/2920, SamplesPerSec=0.43389317714423503\n",
            "training loss: 3.146343541145325\n",
            "[2021-03-02 18:50:14,606] [INFO] [timer.py:166:stop] 0/2940, SamplesPerSec=0.43355386639382\n",
            "training loss: 3.1377831220626833\n",
            "[2021-03-02 18:50:59,940] [INFO] [timer.py:166:stop] 0/2960, SamplesPerSec=0.4336046229524407\n",
            "training loss: 3.124208229780197\n",
            "[2021-03-02 18:51:47,273] [INFO] [timer.py:166:stop] 0/2980, SamplesPerSec=0.43352855924908873\n",
            "[2021-03-02 18:52:16,242] [INFO] [logging.py:60:log_dist] [Rank 0] step=4040, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1032575488090517\n",
            "[2021-03-02 18:52:38,087] [INFO] [timer.py:166:stop] 0/3000, SamplesPerSec=0.4332354731047306\n",
            "training loss: 3.1385855674743652\n",
            "[2021-03-02 18:53:24,651] [INFO] [timer.py:166:stop] 0/3020, SamplesPerSec=0.4332107416378045\n",
            "training loss: 3.096524405479431\n",
            "[2021-03-02 18:54:09,743] [INFO] [timer.py:166:stop] 0/3040, SamplesPerSec=0.43327728394981285\n",
            "training loss: 3.1611350774765015\n",
            "[2021-03-02 18:54:58,040] [INFO] [timer.py:166:stop] 0/3060, SamplesPerSec=0.4331463282738219\n",
            "training loss: 3.067519497871399\n",
            "BLEU metric: 0.03628515841619406\n",
            "Valid Structure Metric: 0.9675675675675676\n",
            "[2021-03-02 18:56:29,338] [INFO] [timer.py:166:stop] 0/3080, SamplesPerSec=0.4331173154204597\n",
            "training loss: 3.029668128490448\n",
            "[2021-03-02 18:57:15,996] [INFO] [timer.py:166:stop] 0/3100, SamplesPerSec=0.4330882890304355\n",
            "training loss: 3.1080021619796754\n",
            "[2021-03-02 18:57:58,699] [INFO] [timer.py:166:stop] 0/3120, SamplesPerSec=0.4332977116451524\n",
            "training loss: 3.3043509483337403\n",
            "[2021-03-02 18:58:42,016] [INFO] [timer.py:166:stop] 0/3140, SamplesPerSec=0.43346786715740376\n",
            "[2021-03-02 18:59:08,391] [INFO] [logging.py:60:log_dist] [Rank 0] step=4060, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9826757669448853\n",
            "[2021-03-02 18:59:31,346] [INFO] [timer.py:166:stop] 0/3160, SamplesPerSec=0.43327825822105814\n",
            "training loss: 3.259372901916504\n",
            "[2021-03-02 19:00:20,326] [INFO] [timer.py:166:stop] 0/3180, SamplesPerSec=0.4331118584011044\n",
            "training loss: 2.9689373850822447\n",
            "[2021-03-02 19:01:03,333] [INFO] [timer.py:166:stop] 0/3200, SamplesPerSec=0.4332980793823112\n",
            "training loss: 3.147136890888214\n",
            "[2021-03-02 19:01:46,619] [INFO] [timer.py:166:stop] 0/3220, SamplesPerSec=0.4334658444889362\n",
            "training loss: 3.102522039413452\n",
            "[2021-03-02 19:02:31,183] [INFO] [timer.py:166:stop] 0/3240, SamplesPerSec=0.4335574151318457\n",
            "training loss: 3.051280748844147\n",
            "[2021-03-02 19:03:19,887] [INFO] [timer.py:166:stop] 0/3260, SamplesPerSec=0.4334090858711133\n",
            "training loss: 3.0991347074508666\n",
            "\n",
            " validation loss: 3.061924231052399 \n",
            "\n",
            "[2021-03-02 19:04:20,948] [INFO] [timer.py:166:stop] 0/3280, SamplesPerSec=0.4331102170572592\n",
            "training loss: 3.1605945825576782\n",
            "[2021-03-02 19:05:15,447] [INFO] [timer.py:166:stop] 0/3300, SamplesPerSec=0.43263757009042564\n",
            "[2021-03-02 19:05:43,558] [INFO] [logging.py:60:log_dist] [Rank 0] step=4080, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1879112362861632\n",
            "[2021-03-02 19:05:57,343] [INFO] [timer.py:166:stop] 0/3320, SamplesPerSec=0.43288221587597125\n",
            "training loss: 3.0386414170265197\n",
            "[2021-03-02 19:06:45,488] [INFO] [timer.py:166:stop] 0/3340, SamplesPerSec=0.4327733327546127\n",
            "training loss: 2.9256898522377015\n",
            "[2021-03-02 19:07:27,115] [INFO] [timer.py:166:stop] 0/3360, SamplesPerSec=0.43302941301075304\n",
            "training loss: 3.109920781850815\n",
            "[2021-03-02 19:08:13,673] [INFO] [timer.py:166:stop] 0/3380, SamplesPerSec=0.43300897305131786\n",
            "training loss: 2.999240005016327\n",
            "[2021-03-02 19:09:01,132] [INFO] [timer.py:166:stop] 0/3400, SamplesPerSec=0.4329389962349723\n",
            "training loss: 2.7969213008880613\n",
            "[2021-03-02 19:09:47,308] [INFO] [timer.py:166:stop] 0/3420, SamplesPerSec=0.43294020851472137\n",
            "training loss: 3.0172633826732635\n",
            "[2021-03-02 19:10:31,228] [INFO] [timer.py:166:stop] 0/3440, SamplesPerSec=0.4330644213097181\n",
            "training loss: 3.1389772415161135\n",
            "[2021-03-02 19:11:29,205] [INFO] [timer.py:166:stop] 0/3460, SamplesPerSec=0.43242580803045927\n",
            "[2021-03-02 19:12:01,160] [INFO] [logging.py:60:log_dist] [Rank 0] step=4100, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.3086005568504335\n",
            "[2021-03-02 19:12:20,009] [INFO] [timer.py:166:stop] 0/3480, SamplesPerSec=0.4321813026019164\n",
            "training loss: 3.0279424369335173\n",
            "[2021-03-02 19:13:04,527] [INFO] [timer.py:166:stop] 0/3500, SamplesPerSec=0.4322753233959763\n",
            "training loss: 3.0502282977104187\n",
            "[2021-03-02 19:13:47,545] [INFO] [timer.py:166:stop] 0/3520, SamplesPerSec=0.4324481156970918\n",
            "training loss: 3.1314072132110597\n",
            "[2021-03-02 19:14:31,841] [INFO] [timer.py:166:stop] 0/3540, SamplesPerSec=0.4325514582986894\n",
            "training loss: 3.028875917196274\n",
            "[2021-03-02 19:15:19,565] [INFO] [timer.py:166:stop] 0/3560, SamplesPerSec=0.4324734598294938\n",
            "training loss: 3.0545262455940247\n",
            "[2021-03-02 19:16:04,893] [INFO] [timer.py:166:stop] 0/3580, SamplesPerSec=0.43252152967042257\n",
            "training loss: 2.9670458853244783\n",
            "[2021-03-02 19:16:56,528] [INFO] [timer.py:166:stop] 0/3600, SamplesPerSec=0.43224139526728955\n",
            "training loss: 3.1323471307754516\n",
            "[2021-03-02 19:17:43,820] [INFO] [timer.py:166:stop] 0/3620, SamplesPerSec=0.4321887484143233\n",
            "[2021-03-02 19:18:10,223] [INFO] [logging.py:60:log_dist] [Rank 0] step=4120, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1439978420734405\n",
            "[2021-03-02 19:18:28,476] [INFO] [timer.py:166:stop] 0/3640, SamplesPerSec=0.43227210242772696\n",
            "training loss: 3.1030451536178587\n",
            "[2021-03-02 19:19:14,165] [INFO] [timer.py:166:stop] 0/3660, SamplesPerSec=0.43230175624866585\n",
            "training loss: 3.1931304216384886\n",
            "\n",
            " validation loss: 3.1378221809864044 \n",
            "\n",
            "BLEU metric: 0.002299762457516752\n",
            "Valid Structure Metric: 0.9594240837696335\n",
            "[2021-03-02 19:20:56,008] [INFO] [timer.py:166:stop] 0/3680, SamplesPerSec=0.4324314215545889\n",
            "training loss: 3.109145474433899\n",
            "[2021-03-02 19:21:44,999] [INFO] [timer.py:166:stop] 0/3700, SamplesPerSec=0.4322930510560588\n",
            "training loss: 2.89804921746254\n",
            "[2021-03-02 19:22:30,851] [INFO] [timer.py:166:stop] 0/3720, SamplesPerSec=0.4323139059043097\n",
            "training loss: 3.15752494931221\n",
            "[2021-03-02 19:23:15,865] [INFO] [timer.py:166:stop] 0/3740, SamplesPerSec=0.43237647958628583\n",
            "training loss: 3.0333057641983032\n",
            "[2021-03-02 19:23:57,623] [INFO] [timer.py:166:stop] 0/3760, SamplesPerSec=0.43260051172406766\n",
            "training loss: 3.1315294027328493\n",
            "[2021-03-02 19:24:39,590] [INFO] [timer.py:166:stop] 0/3780, SamplesPerSec=0.43281201702043526\n",
            "[2021-03-02 19:25:02,789] [INFO] [logging.py:60:log_dist] [Rank 0] step=4140, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.011946976184845\n",
            "[2021-03-02 19:25:22,260] [INFO] [timer.py:166:stop] 0/3800, SamplesPerSec=0.4329868057088852\n",
            "training loss: 3.1409303069114687\n",
            "[2021-03-02 19:26:08,616] [INFO] [timer.py:166:stop] 0/3820, SamplesPerSec=0.43297877994223466\n",
            "[2021-03-02 19:26:29,160] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "4-33166-3.1378221809864044\n",
            "\n",
            "training loss: 3.187827134132385\n",
            "[2021-03-02 19:27:00,078] [INFO] [timer.py:166:stop] 0/3840, SamplesPerSec=0.4330163263086891\n",
            "training loss: 3.1376815915107725\n",
            "[2021-03-02 19:27:50,269] [INFO] [timer.py:166:stop] 0/3860, SamplesPerSec=0.4328219376653052\n",
            "training loss: 2.9950764775276184\n",
            "[2021-03-02 19:28:37,837] [INFO] [timer.py:166:stop] 0/3880, SamplesPerSec=0.4327564210196546\n",
            "training loss: 3.056116259098053\n",
            "[2021-03-02 19:29:26,660] [INFO] [timer.py:166:stop] 0/3900, SamplesPerSec=0.43263127864938067\n",
            "training loss: 3.218931531906128\n",
            "[2021-03-02 19:30:22,338] [INFO] [timer.py:166:stop] 0/3920, SamplesPerSec=0.43218045335977023\n",
            "training loss: 2.972798788547516\n",
            "[2021-03-02 19:31:10,101] [INFO] [timer.py:166:stop] 0/3940, SamplesPerSec=0.4321101078350839\n",
            "[2021-03-02 19:31:37,255] [INFO] [logging.py:60:log_dist] [Rank 0] step=4160, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.126031529903412\n",
            "[2021-03-02 19:31:59,015] [INFO] [timer.py:166:stop] 0/3960, SamplesPerSec=0.4319862443759365\n",
            "training loss: 3.13606880903244\n",
            "[2021-03-02 19:32:44,393] [INFO] [timer.py:166:stop] 0/3980, SamplesPerSec=0.43202949191163115\n",
            "training loss: 3.243323338031769\n",
            "[2021-03-02 19:33:30,706] [INFO] [timer.py:166:stop] 0/4000, SamplesPerSec=0.43202871368781004\n",
            "training loss: 3.1573132395744326\n",
            "[2021-03-02 19:34:19,563] [INFO] [timer.py:166:stop] 0/4020, SamplesPerSec=0.4319097541135036\n",
            "training loss: 3.1826659440994263\n",
            "[2021-03-02 19:35:09,997] [INFO] [timer.py:166:stop] 0/4040, SamplesPerSec=0.43171927143114697\n",
            "training loss: 2.9964771211147307\n",
            "[2021-03-02 19:35:55,780] [INFO] [timer.py:166:stop] 0/4060, SamplesPerSec=0.4317443180599735\n",
            "training loss: 3.088110589981079\n",
            "\n",
            " validation loss: 3.063492256402969 \n",
            "\n",
            "[2021-03-02 19:36:51,701] [INFO] [timer.py:166:stop] 0/4080, SamplesPerSec=0.4317752075072351\n",
            "training loss: 3.0525690734386446\n",
            "[2021-03-02 19:37:36,351] [INFO] [timer.py:166:stop] 0/4100, SamplesPerSec=0.4318513558117033\n",
            "[2021-03-02 19:38:00,466] [INFO] [logging.py:60:log_dist] [Rank 0] step=4180, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9219256937503815\n",
            "[2021-03-02 19:38:17,365] [INFO] [timer.py:166:stop] 0/4120, SamplesPerSec=0.43209153803157624\n",
            "training loss: 3.068349766731262\n",
            "[2021-03-02 19:39:02,317] [INFO] [timer.py:166:stop] 0/4140, SamplesPerSec=0.43215186505516967\n",
            "training loss: 3.1588125824928284\n",
            "[2021-03-02 19:39:48,042] [INFO] [timer.py:166:stop] 0/4160, SamplesPerSec=0.4321768834570359\n",
            "training loss: 3.09453604221344\n",
            "[2021-03-02 19:40:34,762] [INFO] [timer.py:166:stop] 0/4180, SamplesPerSec=0.43215723049555543\n",
            "training loss: 3.0978089928627015\n",
            "[2021-03-02 19:41:21,467] [INFO] [timer.py:166:stop] 0/4200, SamplesPerSec=0.43213839973203505\n",
            "training loss: 3.186711823940277\n",
            "[2021-03-02 19:42:05,809] [INFO] [timer.py:166:stop] 0/4220, SamplesPerSec=0.4322244099312617\n",
            "training loss: 3.023974907398224\n",
            "[2021-03-02 19:42:55,497] [INFO] [timer.py:166:stop] 0/4240, SamplesPerSec=0.43207398568195116\n",
            "training loss: 3.0815863609313965\n",
            "[2021-03-02 19:43:39,901] [INFO] [timer.py:166:stop] 0/4260, SamplesPerSec=0.4321567410871238\n",
            "[2021-03-02 19:44:07,067] [INFO] [logging.py:60:log_dist] [Rank 0] step=4200, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.962829220294952\n",
            "BLEU metric: 0.007469007870302369\n",
            "Valid Structure Metric: 0.9568062827225131\n",
            "[2021-03-02 19:45:08,182] [INFO] [timer.py:166:stop] 0/4280, SamplesPerSec=0.43224542596159865\n",
            "training loss: 2.962078809738159\n",
            "[2021-03-02 19:45:52,879] [INFO] [timer.py:166:stop] 0/4300, SamplesPerSec=0.43231393213257985\n",
            "training loss: 3.2408711552619933\n",
            "[2021-03-02 19:46:37,266] [INFO] [timer.py:166:stop] 0/4320, SamplesPerSec=0.4323952038374708\n",
            "training loss: 3.1052792191505434\n",
            "[2021-03-02 19:47:24,552] [INFO] [timer.py:166:stop] 0/4340, SamplesPerSec=0.43235086026605796\n",
            "training loss: 2.9671733021736144\n",
            "[2021-03-02 19:48:02,274] [INFO] [timer.py:166:stop] 0/4360, SamplesPerSec=0.43271745107598636\n",
            "training loss: 3.15733562707901\n",
            "[2021-03-02 19:48:49,897] [INFO] [timer.py:166:stop] 0/4380, SamplesPerSec=0.43265750496363725\n",
            "training loss: 3.048941600322723\n",
            "[2021-03-02 19:49:34,272] [INFO] [timer.py:166:stop] 0/4400, SamplesPerSec=0.43273641557953535\n",
            "training loss: 3.0537473022937776\n",
            "[2021-03-02 19:50:16,478] [INFO] [timer.py:166:stop] 0/4420, SamplesPerSec=0.4329066567635324\n",
            "[2021-03-02 19:50:45,050] [INFO] [logging.py:60:log_dist] [Rank 0] step=4220, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.056118404865265\n",
            "[2021-03-02 19:51:05,080] [INFO] [timer.py:166:stop] 0/4440, SamplesPerSec=0.4328052954953871\n",
            "training loss: 3.3411218762397765\n",
            "[2021-03-02 19:51:55,253] [INFO] [timer.py:166:stop] 0/4460, SamplesPerSec=0.4326389805547063\n",
            "training loss: 3.18056560754776\n",
            "\n",
            " validation loss: 2.9665692329406737 \n",
            "\n",
            "[2021-03-02 19:52:57,346] [INFO] [timer.py:166:stop] 0/4480, SamplesPerSec=0.4323641788720804\n",
            "training loss: 3.0387985467910767\n",
            "[2021-03-02 19:53:45,272] [INFO] [timer.py:166:stop] 0/4500, SamplesPerSec=0.4322949483563309\n",
            "training loss: 2.9516788959503173\n",
            "[2021-03-02 19:54:25,105] [INFO] [timer.py:166:stop] 0/4520, SamplesPerSec=0.4325612459463279\n",
            "training loss: 3.127674436569214\n",
            "[2021-03-02 19:55:14,514] [INFO] [timer.py:166:stop] 0/4540, SamplesPerSec=0.4324305432803421\n",
            "training loss: 3.1109453678131103\n",
            "[2021-03-02 19:55:58,552] [INFO] [timer.py:166:stop] 0/4560, SamplesPerSec=0.4325214458770437\n",
            "training loss: 3.1189953207969667\n",
            "[2021-03-02 19:56:46,652] [INFO] [timer.py:166:stop] 0/4580, SamplesPerSec=0.4324455724366115\n",
            "[2021-03-02 19:57:15,621] [INFO] [logging.py:60:log_dist] [Rank 0] step=4240, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.2853904128074647\n",
            "[2021-03-02 19:57:33,811] [INFO] [timer.py:166:stop] 0/4600, SamplesPerSec=0.4324086446345436\n",
            "training loss: 2.995719087123871\n",
            "[2021-03-02 19:58:20,622] [INFO] [timer.py:166:stop] 0/4620, SamplesPerSec=0.432386142394495\n",
            "training loss: 3.233778178691864\n",
            "[2021-03-02 19:59:02,211] [INFO] [timer.py:166:stop] 0/4640, SamplesPerSec=0.4325743990172108\n",
            "training loss: 2.955208587646484\n",
            "[2021-03-02 19:59:48,612] [INFO] [timer.py:166:stop] 0/4660, SamplesPerSec=0.43256780631705877\n",
            "training loss: 3.2180223643779753\n",
            "[2021-03-02 20:00:35,519] [INFO] [timer.py:166:stop] 0/4680, SamplesPerSec=0.4325410545217713\n",
            "training loss: 2.8457199811935423\n",
            "[2021-03-02 20:01:17,825] [INFO] [timer.py:166:stop] 0/4700, SamplesPerSec=0.4326978254869241\n",
            "training loss: 3.0470454573631285\n",
            "[2021-03-02 20:02:05,144] [INFO] [timer.py:166:stop] 0/4720, SamplesPerSec=0.43265436712974586\n",
            "training loss: 3.1972153544425965\n",
            "[2021-03-02 20:02:46,353] [INFO] [timer.py:166:stop] 0/4740, SamplesPerSec=0.43285277323114885\n",
            "[2021-03-02 20:03:16,671] [INFO] [logging.py:60:log_dist] [Rank 0] step=4260, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9063554644584655\n",
            "[2021-03-02 20:03:35,011] [INFO] [timer.py:166:stop] 0/4760, SamplesPerSec=0.432756320044314\n",
            "training loss: 3.2192798495292663\n",
            "[2021-03-02 20:04:17,228] [INFO] [timer.py:166:stop] 0/4780, SamplesPerSec=0.4329131577983543\n",
            "training loss: 3.1238920629024505\n",
            "[2021-03-02 20:05:02,412] [INFO] [timer.py:166:stop] 0/4800, SamplesPerSec=0.43295290332990716\n",
            "training loss: 3.1324017703533173\n",
            "[2021-03-02 20:05:53,504] [INFO] [timer.py:166:stop] 0/4820, SamplesPerSec=0.4327625632355759\n",
            "training loss: 3.097517764568329\n",
            "[2021-03-02 20:06:41,212] [INFO] [timer.py:166:stop] 0/4840, SamplesPerSec=0.43270483362392254\n",
            "training loss: 3.1682561576366424\n",
            "[2021-03-02 20:07:32,153] [INFO] [timer.py:166:stop] 0/4860, SamplesPerSec=0.43252307084791247\n",
            "training loss: 3.3112948417663572\n",
            "\n",
            " validation loss: 3.1107269555330275 \n",
            "\n",
            "BLEU metric: 0.0002768255303592458\n",
            "Valid Structure Metric: 0.9744245524296675\n",
            "[2021-03-02 20:08:48,513] [INFO] [timer.py:166:stop] 0/4880, SamplesPerSec=0.43240145590851764\n",
            "training loss: 2.790723294019699\n",
            "[2021-03-02 20:09:30,124] [INFO] [timer.py:166:stop] 0/4900, SamplesPerSec=0.432578796935768\n",
            "[2021-03-02 20:09:56,856] [INFO] [logging.py:60:log_dist] [Rank 0] step=4280, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.177407944202423\n",
            "[2021-03-02 20:10:13,774] [INFO] [timer.py:166:stop] 0/4920, SamplesPerSec=0.43267725144590435\n",
            "training loss: 3.1063801288604735\n",
            "[2021-03-02 20:10:58,962] [INFO] [timer.py:166:stop] 0/4940, SamplesPerSec=0.4327166395468114\n",
            "training loss: 3.2126890897750853\n",
            "[2021-03-02 20:11:53,159] [INFO] [timer.py:166:stop] 0/4960, SamplesPerSec=0.43241566686711563\n",
            "training loss: 3.039928925037384\n",
            "[2021-03-02 20:12:40,182] [INFO] [timer.py:166:stop] 0/4980, SamplesPerSec=0.4323867741506187\n",
            "training loss: 3.082905101776123\n",
            "[2021-03-02 20:13:24,912] [INFO] [timer.py:166:stop] 0/5000, SamplesPerSec=0.43244391540840693\n",
            "training loss: 3.2419912338256838\n",
            "[2021-03-02 20:14:10,179] [INFO] [timer.py:166:stop] 0/5020, SamplesPerSec=0.43248059811074063\n",
            "training loss: 3.126907563209534\n",
            "[2021-03-02 20:14:54,002] [INFO] [timer.py:166:stop] 0/5040, SamplesPerSec=0.43257061722445256\n",
            "training loss: 2.9200508415699007\n",
            "[2021-03-02 20:15:38,125] [INFO] [timer.py:166:stop] 0/5060, SamplesPerSec=0.4326488559452032\n",
            "[2021-03-02 20:16:07,904] [INFO] [logging.py:60:log_dist] [Rank 0] step=4300, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.060380482673645\n",
            "[2021-03-02 20:16:28,617] [INFO] [timer.py:166:stop] 0/5080, SamplesPerSec=0.4324918034707171\n",
            "training loss: 3.200394606590271\n",
            "[2021-03-02 20:17:18,491] [INFO] [timer.py:166:stop] 0/5100, SamplesPerSec=0.43235870561284534\n",
            "[2021-03-02 20:17:26,731] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "4-34441-3.1107269555330275\n",
            "\n",
            "training loss: 3.1201112866401672\n",
            "[2021-03-02 20:18:08,043] [INFO] [timer.py:166:stop] 0/5120, SamplesPerSec=0.4324757552619429\n",
            "training loss: 3.153612321615219\n",
            "[2021-03-02 20:18:51,537] [INFO] [timer.py:166:stop] 0/5140, SamplesPerSec=0.43257602474934154\n",
            "training loss: 3.2036136865615843\n",
            "[2021-03-02 20:19:39,279] [INFO] [timer.py:166:stop] 0/5160, SamplesPerSec=0.432521467969125\n",
            "training loss: 3.031613880395889\n",
            "[2021-03-02 20:20:31,296] [INFO] [timer.py:166:stop] 0/5180, SamplesPerSec=0.43231295875497755\n",
            "training loss: 2.9848665833473205\n",
            "[2021-03-02 20:21:12,452] [INFO] [timer.py:166:stop] 0/5200, SamplesPerSec=0.4324967230450307\n",
            "training loss: 3.0832948684692383\n",
            "[2021-03-02 20:21:59,200] [INFO] [timer.py:166:stop] 0/5220, SamplesPerSec=0.43247871247408215\n",
            "[2021-03-02 20:22:30,069] [INFO] [logging.py:60:log_dist] [Rank 0] step=4320, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.3506561279296876\n",
            "[2021-03-02 20:22:49,865] [INFO] [timer.py:166:stop] 0/5240, SamplesPerSec=0.4323210099240081\n",
            "training loss: 3.0669644355773924\n",
            "[2021-03-02 20:23:30,975] [INFO] [timer.py:166:stop] 0/5260, SamplesPerSec=0.4325043096122725\n",
            "training loss: 2.9442811250686645\n",
            "\n",
            " validation loss: 3.084047907590866 \n",
            "\n",
            "[2021-03-02 20:24:22,651] [INFO] [timer.py:166:stop] 0/5280, SamplesPerSec=0.432689207621611\n",
            "training loss: 3.0931081056594847\n",
            "[2021-03-02 20:25:08,562] [INFO] [timer.py:166:stop] 0/5300, SamplesPerSec=0.4327003127643529\n",
            "training loss: 2.8754855036735534\n",
            "[2021-03-02 20:25:59,079] [INFO] [timer.py:166:stop] 0/5320, SamplesPerSec=0.43254921492500437\n",
            "training loss: 3.2227865636348723\n",
            "[2021-03-02 20:26:43,406] [INFO] [timer.py:166:stop] 0/5340, SamplesPerSec=0.43261627271153386\n",
            "training loss: 2.849480628967285\n",
            "[2021-03-02 20:27:31,068] [INFO] [timer.py:166:stop] 0/5360, SamplesPerSec=0.43256637142650844\n",
            "training loss: 3.0264776229858397\n",
            "[2021-03-02 20:28:13,755] [INFO] [timer.py:166:stop] 0/5380, SamplesPerSec=0.4326899550583129\n",
            "[2021-03-02 20:28:42,317] [INFO] [logging.py:60:log_dist] [Rank 0] step=4340, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0966699838638307\n",
            "[2021-03-02 20:29:02,207] [INFO] [timer.py:166:stop] 0/5400, SamplesPerSec=0.43261272426196573\n",
            "training loss: 3.169732451438904\n",
            "[2021-03-02 20:29:45,889] [INFO] [timer.py:166:stop] 0/5420, SamplesPerSec=0.43270084991978\n",
            "training loss: 3.1198088765144347\n",
            "[2021-03-02 20:30:34,210] [INFO] [timer.py:166:stop] 0/5440, SamplesPerSec=0.4326286548079652\n",
            "training loss: 3.1570728361606597\n",
            "[2021-03-02 20:31:21,317] [INFO] [timer.py:166:stop] 0/5460, SamplesPerSec=0.43259861520015436\n",
            "training loss: 3.020450460910797\n",
            "BLEU metric: 0.045718401759074746\n",
            "Valid Structure Metric: 0.9620418848167539\n",
            "[2021-03-02 20:32:54,540] [INFO] [timer.py:166:stop] 0/5480, SamplesPerSec=0.4323211777314877\n",
            "training loss: 3.021684300899506\n",
            "[2021-03-02 20:33:37,996] [INFO] [timer.py:166:stop] 0/5500, SamplesPerSec=0.4324166597337089\n",
            "training loss: 3.145906162261963\n",
            "[2021-03-02 20:34:30,109] [INFO] [timer.py:166:stop] 0/5520, SamplesPerSec=0.4322182240309858\n",
            "training loss: 3.025480878353119\n",
            "[2021-03-02 20:35:16,045] [INFO] [timer.py:166:stop] 0/5540, SamplesPerSec=0.43222965668671676\n",
            "[2021-03-02 20:35:43,764] [INFO] [logging.py:60:log_dist] [Rank 0] step=4360, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9548404037952425\n",
            "[2021-03-02 20:36:02,719] [INFO] [timer.py:166:stop] 0/5560, SamplesPerSec=0.43221622799185494\n",
            "training loss: 3.05000034570694\n",
            "[2021-03-02 20:36:49,882] [INFO] [timer.py:166:stop] 0/5580, SamplesPerSec=0.4321865037217062\n",
            "training loss: 3.096936857700348\n",
            "[2021-03-02 20:37:35,339] [INFO] [timer.py:166:stop] 0/5600, SamplesPerSec=0.4322139354775465\n",
            "training loss: 2.9510005712509155\n",
            "[2021-03-02 20:38:17,148] [INFO] [timer.py:166:stop] 0/5620, SamplesPerSec=0.43236250623069555\n",
            "training loss: 2.9535395860672\n",
            "[2021-03-02 20:39:01,876] [INFO] [timer.py:166:stop] 0/5640, SamplesPerSec=0.43241333462620246\n",
            "training loss: 2.933795487880707\n",
            "[2021-03-02 20:39:47,774] [INFO] [timer.py:166:stop] 0/5660, SamplesPerSec=0.43242512178993975\n",
            "training loss: 3.003265392780304\n",
            "\n",
            " validation loss: 3.0600733280181887 \n",
            "\n",
            "[2021-03-02 20:40:43,160] [INFO] [timer.py:166:stop] 0/5680, SamplesPerSec=0.4324292698830201\n",
            "training loss: 3.051728165149689\n",
            "[2021-03-02 20:41:25,421] [INFO] [timer.py:166:stop] 0/5700, SamplesPerSec=0.43256033666407007\n",
            "[2021-03-02 20:41:50,441] [INFO] [logging.py:60:log_dist] [Rank 0] step=4380, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0871385455131533\n",
            "[2021-03-02 20:42:10,443] [INFO] [timer.py:166:stop] 0/5720, SamplesPerSec=0.43260013958263926\n",
            "training loss: 3.1972069561481478\n",
            "[2021-03-02 20:42:54,277] [INFO] [timer.py:166:stop] 0/5740, SamplesPerSec=0.4326784505814337\n",
            "training loss: 2.8672349512577058\n",
            "[2021-03-02 20:43:40,483] [INFO] [timer.py:166:stop] 0/5760, SamplesPerSec=0.4326791049413731\n",
            "training loss: 3.0102585673332216\n",
            "[2021-03-02 20:44:24,501] [INFO] [timer.py:166:stop] 0/5780, SamplesPerSec=0.4327506786775012\n",
            "training loss: 3.009060961008072\n",
            "[2021-03-02 20:45:08,984] [INFO] [timer.py:166:stop] 0/5800, SamplesPerSec=0.4328067195553\n",
            "training loss: 3.0237893044948576\n",
            "[2021-03-02 20:45:55,289] [INFO] [timer.py:166:stop] 0/5820, SamplesPerSec=0.43280374353646955\n",
            "training loss: 3.0467440664768217\n",
            "[2021-03-02 20:46:40,624] [INFO] [timer.py:166:stop] 0/5840, SamplesPerSec=0.4328319219278334\n",
            "training loss: 3.1346717834472657\n",
            "[2021-03-02 20:47:31,027] [INFO] [timer.py:166:stop] 0/5860, SamplesPerSec=0.43269786769461177\n",
            "[2021-03-02 20:47:59,630] [INFO] [logging.py:60:log_dist] [Rank 0] step=4400, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0717391550540922\n",
            "[2021-03-02 20:48:18,736] [INFO] [timer.py:166:stop] 0/5880, SamplesPerSec=0.43265073969228496\n",
            "training loss: 3.065466594696045\n",
            "[2021-03-02 20:49:04,501] [INFO] [timer.py:166:stop] 0/5900, SamplesPerSec=0.43266545531112927\n",
            "training loss: 2.835696643590927\n",
            "[2021-03-02 20:49:54,152] [INFO] [timer.py:166:stop] 0/5920, SamplesPerSec=0.43255721729391416\n",
            "training loss: 3.170390796661377\n",
            "[2021-03-02 20:50:37,661] [INFO] [timer.py:166:stop] 0/5940, SamplesPerSec=0.4326432678704287\n",
            "training loss: 3.1337774097919464\n",
            "[2021-03-02 20:51:20,617] [INFO] [timer.py:166:stop] 0/5960, SamplesPerSec=0.43274616715630065\n",
            "training loss: 3.0154205560684204\n",
            "[2021-03-02 20:52:09,353] [INFO] [timer.py:166:stop] 0/5980, SamplesPerSec=0.43266733867741863\n",
            "training loss: 3.247098278999329\n",
            "[2021-03-02 20:52:52,578] [INFO] [timer.py:166:stop] 0/6000, SamplesPerSec=0.43276107245723083\n",
            "training loss: 2.8600633442401886\n",
            "[2021-03-02 20:53:36,020] [INFO] [timer.py:166:stop] 0/6020, SamplesPerSec=0.4328474561022684\n",
            "[2021-03-02 20:54:06,811] [INFO] [logging.py:60:log_dist] [Rank 0] step=4420, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9814568042755125\n",
            "[2021-03-02 20:54:28,746] [INFO] [timer.py:166:stop] 0/6040, SamplesPerSec=0.432645284508925\n",
            "training loss: 3.2988054871559145\n",
            "[2021-03-02 20:55:11,965] [INFO] [timer.py:166:stop] 0/6060, SamplesPerSec=0.4327383626869033\n",
            "training loss: 2.986215901374817\n",
            "\n",
            " validation loss: 3.0184823632240296 \n",
            "\n",
            "BLEU metric: 0.05318750117157521\n",
            "Valid Structure Metric: 0.9720744680851063\n",
            "[2021-03-02 20:56:57,083] [INFO] [timer.py:166:stop] 0/6080, SamplesPerSec=0.43264832478110754\n",
            "training loss: 3.232779586315155\n",
            "[2021-03-02 20:57:46,782] [INFO] [timer.py:166:stop] 0/6100, SamplesPerSec=0.43254181736273917\n",
            "training loss: 2.9449933886528017\n",
            "[2021-03-02 20:58:28,536] [INFO] [timer.py:166:stop] 0/6120, SamplesPerSec=0.43267907382814985\n",
            "training loss: 3.0911354422569275\n",
            "[2021-03-02 20:59:09,725] [INFO] [timer.py:166:stop] 0/6140, SamplesPerSec=0.4328327655994804\n",
            "training loss: 3.1114245295524596\n",
            "[2021-03-02 20:59:56,461] [INFO] [timer.py:166:stop] 0/6160, SamplesPerSec=0.43281675143877846\n",
            "training loss: 2.848986917734146\n",
            "[2021-03-02 21:00:44,235] [INFO] [timer.py:166:stop] 0/6180, SamplesPerSec=0.4327693912715172\n",
            "[2021-03-02 21:01:09,104] [INFO] [logging.py:60:log_dist] [Rank 0] step=4440, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.980264389514923\n",
            "[2021-03-02 21:01:30,698] [INFO] [timer.py:166:stop] 0/6200, SamplesPerSec=0.4327619282253189\n",
            "training loss: 2.9226504445075987\n",
            "[2021-03-02 21:02:18,100] [INFO] [timer.py:166:stop] 0/6220, SamplesPerSec=0.43272623934103527\n",
            "training loss: 3.0532066762447356\n",
            "[2021-03-02 21:03:03,018] [INFO] [timer.py:166:stop] 0/6240, SamplesPerSec=0.43276536778407143\n",
            "training loss: 2.86343652009964\n",
            "[2021-03-02 21:03:50,656] [INFO] [timer.py:166:stop] 0/6260, SamplesPerSec=0.4327228413386594\n",
            "training loss: 3.101565605401993\n",
            "[2021-03-02 21:04:35,262] [INFO] [timer.py:166:stop] 0/6280, SamplesPerSec=0.4327710187888978\n",
            "training loss: 3.123283576965332\n",
            "[2021-03-02 21:05:19,753] [INFO] [timer.py:166:stop] 0/6300, SamplesPerSec=0.43282232125177417\n",
            "training loss: 3.0106393337249755\n",
            "[2021-03-02 21:06:09,227] [INFO] [timer.py:166:stop] 0/6320, SamplesPerSec=0.43272559317519643\n",
            "training loss: 2.9949411273002626\n",
            "[2021-03-02 21:06:52,133] [INFO] [timer.py:166:stop] 0/6340, SamplesPerSec=0.43282355084935636\n",
            "[2021-03-02 21:07:19,190] [INFO] [logging.py:60:log_dist] [Rank 0] step=4460, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.06544628739357\n",
            "[2021-03-02 21:07:34,723] [INFO] [timer.py:166:stop] 0/6360, SamplesPerSec=0.43293026495555514\n",
            "training loss: 3.083100414276123\n",
            "[2021-03-02 21:08:24,960] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "4-35716-3.0184823632240296\n",
            "\n",
            "[2021-03-02 21:08:33,083] [INFO] [timer.py:166:stop] 0/6380, SamplesPerSec=0.4327434096965269\n",
            "training loss: 2.9948853135108946\n",
            "[2021-03-02 21:09:18,973] [INFO] [timer.py:166:stop] 0/6400, SamplesPerSec=0.43275304085430943\n",
            "training loss: 2.868348151445389\n",
            "[2021-03-02 21:10:04,333] [INFO] [timer.py:166:stop] 0/6420, SamplesPerSec=0.43277808675419593\n",
            "training loss: 3.0966511607170104\n",
            "[2021-03-02 21:10:50,013] [INFO] [timer.py:166:stop] 0/6440, SamplesPerSec=0.4327936535153696\n",
            "training loss: 3.1877675652503967\n",
            "[2021-03-02 21:11:36,623] [INFO] [timer.py:166:stop] 0/6460, SamplesPerSec=0.432782180978447\n",
            "training loss: 3.0226487517356873\n",
            "\n",
            " validation loss: 3.059604933857918 \n",
            "\n",
            "[2021-03-02 21:12:29,793] [INFO] [timer.py:166:stop] 0/6480, SamplesPerSec=0.4329060136564975\n",
            "training loss: 2.8873116970062256\n",
            "[2021-03-02 21:13:19,819] [INFO] [timer.py:166:stop] 0/6500, SamplesPerSec=0.4327957297497801\n",
            "[2021-03-02 21:13:46,704] [INFO] [logging.py:60:log_dist] [Rank 0] step=4480, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.168802261352539\n",
            "[2021-03-02 21:14:06,208] [INFO] [timer.py:166:stop] 0/6520, SamplesPerSec=0.4327906922612508\n",
            "training loss: 3.046912145614624\n",
            "[2021-03-02 21:14:50,875] [INFO] [timer.py:166:stop] 0/6540, SamplesPerSec=0.4328350342320483\n",
            "training loss: 2.9593383610248565\n",
            "[2021-03-02 21:15:31,152] [INFO] [timer.py:166:stop] 0/6560, SamplesPerSec=0.4330045766863728\n",
            "training loss: 3.1484166979789734\n",
            "[2021-03-02 21:16:24,104] [INFO] [timer.py:166:stop] 0/6580, SamplesPerSec=0.43281194526784567\n",
            "training loss: 2.9562800765037536\n",
            "[2021-03-02 21:17:08,272] [INFO] [timer.py:166:stop] 0/6600, SamplesPerSec=0.4328699906646757\n",
            "training loss: 3.1336975038051604\n",
            "[2021-03-02 21:17:50,797] [INFO] [timer.py:166:stop] 0/6620, SamplesPerSec=0.43297421078873677\n",
            "training loss: 2.850839674472809\n",
            "[2021-03-02 21:18:40,593] [INFO] [timer.py:166:stop] 0/6640, SamplesPerSec=0.4328725304286708\n",
            "training loss: 3.236583983898163\n",
            "[2021-03-02 21:19:25,037] [INFO] [timer.py:166:stop] 0/6660, SamplesPerSec=0.43292212372498945\n",
            "[2021-03-02 21:19:49,713] [INFO] [logging.py:60:log_dist] [Rank 0] step=4500, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1976372838020324\n",
            "BLEU metric: 0.01833554476992309\n",
            "Valid Structure Metric: 0.9868131868131869\n",
            "[2021-03-02 21:20:25,775] [INFO] [timer.py:166:stop] 0/6680, SamplesPerSec=0.43305155746521806\n",
            "training loss: 3.0721337676048277\n",
            "[2021-03-02 21:21:12,588] [INFO] [timer.py:166:stop] 0/6700, SamplesPerSec=0.4330340019838525\n",
            "training loss: 3.098287618160248\n",
            "[2021-03-02 21:21:59,338] [INFO] [timer.py:166:stop] 0/6720, SamplesPerSec=0.433018330039071\n",
            "training loss: 3.043509691953659\n",
            "[2021-03-02 21:22:41,663] [INFO] [timer.py:166:stop] 0/6740, SamplesPerSec=0.4331259040028906\n",
            "training loss: 3.109050118923187\n",
            "[2021-03-02 21:23:29,036] [INFO] [timer.py:166:stop] 0/6760, SamplesPerSec=0.43309273587744107\n",
            "training loss: 3.1026395082473757\n",
            "[2021-03-02 21:24:20,191] [INFO] [timer.py:166:stop] 0/6780, SamplesPerSec=0.4329551561099881\n",
            "training loss: 3.081212431192398\n",
            "[2021-03-02 21:25:08,791] [INFO] [timer.py:166:stop] 0/6800, SamplesPerSec=0.4328888847660044\n",
            "training loss: 3.207361972332001\n",
            "[2021-03-02 21:25:50,870] [INFO] [timer.py:166:stop] 0/6820, SamplesPerSec=0.4330022969201119\n",
            "[2021-03-02 21:26:19,154] [INFO] [logging.py:60:log_dist] [Rank 0] step=4520, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1016381680965424\n",
            "[2021-03-02 21:26:40,534] [INFO] [timer.py:166:stop] 0/6840, SamplesPerSec=0.43290710953430556\n",
            "training loss: 3.1472720921039583\n",
            "[2021-03-02 21:27:27,521] [INFO] [timer.py:166:stop] 0/6860, SamplesPerSec=0.4328856377158559\n",
            "training loss: 3.1864153325557707\n",
            "\n",
            " validation loss: 3.117848312854767 \n",
            "\n",
            "[2021-03-02 21:28:27,407] [INFO] [timer.py:166:stop] 0/6880, SamplesPerSec=0.43277031922918435\n",
            "training loss: 2.9739309549331665\n",
            "[2021-03-02 21:29:16,637] [INFO] [timer.py:166:stop] 0/6900, SamplesPerSec=0.43268851247960405\n",
            "training loss: 3.2057540893554686\n",
            "[2021-03-02 21:30:02,299] [INFO] [timer.py:166:stop] 0/6920, SamplesPerSec=0.43270373585408844\n",
            "training loss: 2.8893404245376586\n",
            "[2021-03-02 21:30:44,163] [INFO] [timer.py:166:stop] 0/6940, SamplesPerSec=0.4328214122400747\n",
            "training loss: 3.097096872329712\n",
            "[2021-03-02 21:31:32,657] [INFO] [timer.py:166:stop] 0/6960, SamplesPerSec=0.43275996637406033\n",
            "training loss: 2.976125794649124\n",
            "[2021-03-02 21:32:14,401] [INFO] [timer.py:166:stop] 0/6980, SamplesPerSec=0.4328800626856624\n",
            "[2021-03-02 21:32:45,923] [INFO] [logging.py:60:log_dist] [Rank 0] step=4540, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.970434957742691\n",
            "[2021-03-02 21:33:02,886] [INFO] [timer.py:166:stop] 0/7000, SamplesPerSec=0.4328190152272591\n",
            "training loss: 2.793335723876953\n",
            "[2021-03-02 21:33:50,194] [INFO] [timer.py:166:stop] 0/7020, SamplesPerSec=0.43278976065285696\n",
            "training loss: 3.3267305374145506\n",
            "[2021-03-02 21:34:41,440] [INFO] [timer.py:166:stop] 0/7040, SamplesPerSec=0.43265588139115085\n",
            "training loss: 3.282615768909454\n",
            "[2021-03-02 21:35:22,240] [INFO] [timer.py:166:stop] 0/7060, SamplesPerSec=0.43279990515900885\n",
            "training loss: 3.0896252572536467\n",
            "[2021-03-02 21:36:11,088] [INFO] [timer.py:166:stop] 0/7080, SamplesPerSec=0.432730182122486\n",
            "training loss: 3.12762610912323\n",
            "[2021-03-02 21:36:55,309] [INFO] [timer.py:166:stop] 0/7100, SamplesPerSec=0.43278294689390584\n",
            "training loss: 2.9906306505203246\n",
            "[2021-03-02 21:37:36,758] [INFO] [timer.py:166:stop] 0/7120, SamplesPerSec=0.4329084223379825\n",
            "training loss: 3.1631707072257997\n",
            "[2021-03-02 21:38:24,034] [INFO] [timer.py:166:stop] 0/7140, SamplesPerSec=0.4328802172963972\n",
            "[2021-03-02 21:38:56,300] [INFO] [logging.py:60:log_dist] [Rank 0] step=4560, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0751854479312897\n",
            "[2021-03-02 21:39:15,726] [INFO] [timer.py:166:stop] 0/7160, SamplesPerSec=0.43273661189460466\n",
            "training loss: 3.3005683720111847\n",
            "[2021-03-02 21:40:00,929] [INFO] [timer.py:166:stop] 0/7180, SamplesPerSec=0.4327631469990311\n",
            "training loss: 3.1483771800994873\n",
            "[2021-03-02 21:40:51,941] [INFO] [timer.py:166:stop] 0/7200, SamplesPerSec=0.4326384129395708\n",
            "training loss: 3.1515575110912324\n",
            "[2021-03-02 21:41:34,240] [INFO] [timer.py:166:stop] 0/7220, SamplesPerSec=0.4327403831063305\n",
            "training loss: 3.143180525302887\n",
            "[2021-03-02 21:42:22,859] [INFO] [timer.py:166:stop] 0/7240, SamplesPerSec=0.43267831599906964\n",
            "training loss: 2.9333156287670135\n",
            "[2021-03-02 21:43:09,959] [INFO] [timer.py:166:stop] 0/7260, SamplesPerSec=0.4326558010732497\n",
            "training loss: 3.0250679910182954\n",
            "\n",
            " validation loss: 3.189957541227341 \n",
            "\n",
            "BLEU metric: 0.023255044550636712\n",
            "Valid Structure Metric: 0.9698952879581152\n",
            "[2021-03-02 21:45:03,796] [INFO] [timer.py:166:stop] 0/7280, SamplesPerSec=0.4326806969440596\n",
            "training loss: 3.155038321018219\n",
            "[2021-03-02 21:45:52,345] [INFO] [timer.py:166:stop] 0/7300, SamplesPerSec=0.43262113176229455\n",
            "[2021-03-02 21:46:21,428] [INFO] [logging.py:60:log_dist] [Rank 0] step=4580, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.163454067707062\n",
            "[2021-03-02 21:46:41,218] [INFO] [timer.py:166:stop] 0/7320, SamplesPerSec=0.4325536086938707\n",
            "training loss: 2.9371832489967344\n",
            "[2021-03-02 21:47:25,779] [INFO] [timer.py:166:stop] 0/7340, SamplesPerSec=0.4325964011195753\n",
            "training loss: 3.122846132516861\n",
            "[2021-03-02 21:48:10,535] [INFO] [timer.py:166:stop] 0/7360, SamplesPerSec=0.4326340803693941\n",
            "training loss: 2.9893109261989594\n",
            "[2021-03-02 21:48:53,575] [INFO] [timer.py:166:stop] 0/7380, SamplesPerSec=0.43271503871867123\n",
            "training loss: 3.086405897140503\n",
            "[2021-03-02 21:49:31,602] [INFO] [timer.py:166:stop] 0/7400, SamplesPerSec=0.4329225738823307\n",
            "training loss: 2.9750354409217836\n",
            "[2021-03-02 21:50:12,779] [INFO] [timer.py:166:stop] 0/7420, SamplesPerSec=0.4330495180554926\n",
            "training loss: 3.0375079095363615\n",
            "[2021-03-02 21:51:06,742] [INFO] [timer.py:166:stop] 0/7440, SamplesPerSec=0.4328535482933506\n",
            "training loss: 2.934294855594635\n",
            "[2021-03-02 21:51:53,874] [INFO] [timer.py:166:stop] 0/7460, SamplesPerSec=0.4328303393956559\n",
            "[2021-03-02 21:52:23,954] [INFO] [logging.py:60:log_dist] [Rank 0] step=4600, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.25552761554718\n",
            "[2021-03-02 21:52:40,836] [INFO] [timer.py:166:stop] 0/7480, SamplesPerSec=0.4328115083048011\n",
            "training loss: 3.127673554420471\n",
            "[2021-03-02 21:53:29,455] [INFO] [timer.py:166:stop] 0/7500, SamplesPerSec=0.43275136531100267\n",
            "training loss: 3.0373039841651917\n",
            "[2021-03-02 21:54:11,983] [INFO] [timer.py:166:stop] 0/7520, SamplesPerSec=0.4328433337621743\n",
            "training loss: 3.0318293392658235\n",
            "[2021-03-02 21:54:57,918] [INFO] [timer.py:166:stop] 0/7540, SamplesPerSec=0.43285011984601945\n",
            "training loss: 3.1030915856361387\n",
            "[2021-03-02 21:55:47,142] [INFO] [timer.py:166:stop] 0/7560, SamplesPerSec=0.43277538623161194\n",
            "training loss: 3.100077223777771\n",
            "[2021-03-02 21:56:29,105] [INFO] [timer.py:166:stop] 0/7580, SamplesPerSec=0.43288053830033485\n",
            "training loss: 3.1490641951560976\n",
            "[2021-03-02 21:57:12,790] [INFO] [timer.py:166:stop] 0/7600, SamplesPerSec=0.4329426921131716\n",
            "training loss: 2.9754212617874147\n",
            "[2021-03-02 21:58:00,351] [INFO] [timer.py:166:stop] 0/7620, SamplesPerSec=0.4329091780493927\n",
            "[2021-03-02 21:58:36,307] [INFO] [logging.py:60:log_dist] [Rank 0] step=4620, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0839084148406983\n",
            "[2021-03-02 21:58:53,410] [INFO] [timer.py:166:stop] 0/7640, SamplesPerSec=0.4327409717150082\n",
            "[2021-03-02 21:59:21,899] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "4-36991-3.189957541227341\n",
            "\n",
            "training loss: 3.0251691699028016\n",
            "[2021-03-02 21:59:44,122] [INFO] [timer.py:166:stop] 0/7660, SamplesPerSec=0.4327810779100726\n",
            "training loss: 3.128680145740509\n",
            "\n",
            " validation loss: 3.13589124083519 \n",
            "\n",
            "[2021-03-02 22:00:46,028] [INFO] [timer.py:166:stop] 0/7680, SamplesPerSec=0.4326541619086501\n",
            "training loss: 2.8924836993217466\n",
            "[2021-03-02 22:01:29,295] [INFO] [timer.py:166:stop] 0/7700, SamplesPerSec=0.4327261997110895\n",
            "training loss: 3.013088619709015\n",
            "[2021-03-02 22:02:16,094] [INFO] [timer.py:166:stop] 0/7720, SamplesPerSec=0.43271216418864805\n",
            "training loss: 2.9320698261260985\n",
            "[2021-03-02 22:03:00,478] [INFO] [timer.py:166:stop] 0/7740, SamplesPerSec=0.43275666791622974\n",
            "training loss: 2.9881380438804626\n",
            "[2021-03-02 22:03:43,084] [INFO] [timer.py:166:stop] 0/7760, SamplesPerSec=0.43284388292280307\n",
            "training loss: 2.9871758222579956\n",
            "[2021-03-02 22:04:28,057] [INFO] [timer.py:166:stop] 0/7780, SamplesPerSec=0.432873656524334\n",
            "[2021-03-02 22:04:52,430] [INFO] [logging.py:60:log_dist] [Rank 0] step=4640, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9643654048442842\n",
            "[2021-03-02 22:05:11,430] [INFO] [timer.py:166:stop] 0/7800, SamplesPerSec=0.4329417196325019\n",
            "training loss: 3.0050825119018554\n",
            "[2021-03-02 22:06:03,192] [INFO] [timer.py:166:stop] 0/7820, SamplesPerSec=0.4328083748026825\n",
            "training loss: 3.091751742362976\n",
            "[2021-03-02 22:06:54,428] [INFO] [timer.py:166:stop] 0/7840, SamplesPerSec=0.43268835429639263\n",
            "training loss: 3.2254053354263306\n",
            "[2021-03-02 22:07:46,029] [INFO] [timer.py:166:stop] 0/7860, SamplesPerSec=0.4325603086806385\n",
            "training loss: 3.0144087731838227\n",
            "BLEU metric: 0.00279564139331044\n",
            "Valid Structure Metric: 0.9798994974874372\n",
            "[2021-03-02 22:08:51,977] [INFO] [timer.py:166:stop] 0/7880, SamplesPerSec=0.43257813837105286\n",
            "training loss: 3.032106691598892\n",
            "[2021-03-02 22:09:34,837] [INFO] [timer.py:166:stop] 0/7900, SamplesPerSec=0.43265816711296307\n",
            "training loss: 3.055106836557388\n",
            "[2021-03-02 22:10:20,892] [INFO] [timer.py:166:stop] 0/7920, SamplesPerSec=0.4326622693336388\n",
            "training loss: 3.0164408683776855\n",
            "[2021-03-02 22:11:04,639] [INFO] [timer.py:166:stop] 0/7940, SamplesPerSec=0.43272077514212226\n",
            "[2021-03-02 22:11:36,560] [INFO] [logging.py:60:log_dist] [Rank 0] step=4660, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.022654116153717\n",
            "[2021-03-02 22:11:55,478] [INFO] [timer.py:166:stop] 0/7960, SamplesPerSec=0.43261216982923056\n",
            "training loss: 3.093094062805176\n",
            "[2021-03-02 22:12:43,591] [INFO] [timer.py:166:stop] 0/7980, SamplesPerSec=0.43256809096507437\n",
            "training loss: 3.109945368766785\n",
            "[2021-03-02 22:13:27,632] [INFO] [timer.py:166:stop] 0/8000, SamplesPerSec=0.4326194950027578\n",
            "training loss: 2.880063807964325\n",
            "[2021-03-02 22:14:09,154] [INFO] [timer.py:166:stop] 0/8020, SamplesPerSec=0.4327294798759539\n",
            "training loss: 3.131262445449829\n",
            "[2021-03-02 22:14:51,671] [INFO] [timer.py:166:stop] 0/8040, SamplesPerSec=0.43281577753380895\n",
            "training loss: 2.9800854861736297\n",
            "[2021-03-02 22:15:36,511] [INFO] [timer.py:166:stop] 0/8060, SamplesPerSec=0.4328476715353931\n",
            "training loss: 3.0636556506156922\n",
            "\n",
            " validation loss: 3.010744923353195 \n",
            "\n",
            "[2021-03-02 22:16:30,237] [INFO] [timer.py:166:stop] 0/8080, SamplesPerSec=0.4329138501445711\n",
            "training loss: 3.0005855023860932\n",
            "[2021-03-02 22:17:20,554] [INFO] [timer.py:166:stop] 0/8100, SamplesPerSec=0.4328186237137934\n",
            "[2021-03-02 22:17:46,044] [INFO] [logging.py:60:log_dist] [Rank 0] step=4680, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.155265247821808\n",
            "[2021-03-02 22:18:07,911] [INFO] [timer.py:166:stop] 0/8120, SamplesPerSec=0.4327921872057089\n",
            "training loss: 3.1208969056606293\n",
            "[2021-03-02 22:18:56,501] [INFO] [timer.py:166:stop] 0/8140, SamplesPerSec=0.43273750657677923\n",
            "training loss: 3.033350032567978\n",
            "[2021-03-02 22:19:39,071] [INFO] [timer.py:166:stop] 0/8160, SamplesPerSec=0.4328213019344952\n",
            "training loss: 3.114177918434143\n",
            "[2021-03-02 22:20:25,068] [INFO] [timer.py:166:stop] 0/8180, SamplesPerSec=0.4328262170984986\n",
            "training loss: 3.0387237071990967\n",
            "[2021-03-02 22:21:10,456] [INFO] [timer.py:166:stop] 0/8200, SamplesPerSec=0.43284500090993305\n",
            "training loss: 2.950083816051483\n",
            "[2021-03-02 22:21:58,917] [INFO] [timer.py:166:stop] 0/8220, SamplesPerSec=0.4327936502764438\n",
            "training loss: 2.9959362387657165\n",
            "[2021-03-02 22:22:45,091] [INFO] [timer.py:166:stop] 0/8240, SamplesPerSec=0.4327945719805277\n",
            "training loss: 2.8910029530525208\n",
            "[2021-03-02 22:23:33,577] [INFO] [timer.py:166:stop] 0/8260, SamplesPerSec=0.43274303021392213\n",
            "[2021-03-02 22:23:57,943] [INFO] [logging.py:60:log_dist] [Rank 0] step=4700, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9476622581481933\n",
            "[2021-03-02 22:24:13,025] [INFO] [timer.py:166:stop] 0/8280, SamplesPerSec=0.43289626559645716\n",
            "training loss: 3.082859826087952\n",
            "[2021-03-02 22:25:01,273] [INFO] [timer.py:166:stop] 0/8300, SamplesPerSec=0.43285009038594513\n",
            "training loss: 3.00469069480896\n",
            "[2021-03-02 22:25:45,584] [INFO] [timer.py:166:stop] 0/8320, SamplesPerSec=0.43289282058730416\n",
            "training loss: 2.858954495191574\n",
            "[2021-03-02 22:26:31,070] [INFO] [timer.py:166:stop] 0/8340, SamplesPerSec=0.432908953063976\n",
            "training loss: 2.9777560234069824\n",
            "[2021-03-02 22:27:19,011] [INFO] [timer.py:166:stop] 0/8360, SamplesPerSec=0.4328699549090623\n",
            "training loss: 2.9969370663166046\n",
            "[2021-03-02 22:28:02,160] [INFO] [timer.py:166:stop] 0/8380, SamplesPerSec=0.4329383380548525\n",
            "training loss: 3.0889508724212646\n",
            "[2021-03-02 22:28:49,601] [INFO] [timer.py:166:stop] 0/8400, SamplesPerSec=0.4329106109415722\n",
            "training loss: 2.994530999660492\n",
            "[2021-03-02 22:29:33,027] [INFO] [timer.py:166:stop] 0/8420, SamplesPerSec=0.4329724251104142\n",
            "[2021-03-02 22:30:03,511] [INFO] [logging.py:60:log_dist] [Rank 0] step=4720, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0073133051395415\n",
            "[2021-03-02 22:30:23,245] [INFO] [timer.py:166:stop] 0/8440, SamplesPerSec=0.4328830579105025\n",
            "training loss: 3.152649772167206\n",
            "[2021-03-02 22:31:08,054] [INFO] [timer.py:166:stop] 0/8460, SamplesPerSec=0.43291397576161716\n",
            "training loss: 2.9798593521118164\n",
            "\n",
            " validation loss: 3.0970557570457458 \n",
            "\n",
            "BLEU metric: 0.028008046361102452\n",
            "Valid Structure Metric: 0.9764397905759162\n",
            "[2021-03-02 22:32:46,785] [INFO] [timer.py:166:stop] 0/8480, SamplesPerSec=0.4330175901026024\n",
            "training loss: 3.157702016830444\n",
            "[2021-03-02 22:33:33,321] [INFO] [timer.py:166:stop] 0/8500, SamplesPerSec=0.43300997059077073\n",
            "training loss: 2.9213735818862916\n",
            "[2021-03-02 22:34:21,850] [INFO] [timer.py:166:stop] 0/8520, SamplesPerSec=0.43295849454709867\n",
            "training loss: 3.072204512357712\n",
            "[2021-03-02 22:35:09,732] [INFO] [timer.py:166:stop] 0/8540, SamplesPerSec=0.43292149332672897\n",
            "training loss: 3.1522891521453857\n",
            "[2021-03-02 22:35:54,988] [INFO] [timer.py:166:stop] 0/8560, SamplesPerSec=0.43294219335208095\n",
            "training loss: 3.194379198551178\n",
            "[2021-03-02 22:36:40,681] [INFO] [timer.py:166:stop] 0/8580, SamplesPerSec=0.43295322941008857\n",
            "[2021-03-02 22:37:06,069] [INFO] [logging.py:60:log_dist] [Rank 0] step=4740, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9079702258110047\n",
            "[2021-03-02 22:37:21,528] [INFO] [timer.py:166:stop] 0/8600, SamplesPerSec=0.4330699001750864\n",
            "training loss: 3.0217911541461944\n",
            "[2021-03-02 22:38:07,586] [INFO] [timer.py:166:stop] 0/8620, SamplesPerSec=0.4330726504673845\n",
            "training loss: 2.895702713727951\n",
            "[2021-03-02 22:38:51,133] [INFO] [timer.py:166:stop] 0/8640, SamplesPerSec=0.43312991466906664\n",
            "training loss: 3.138872170448303\n",
            "[2021-03-02 22:39:37,086] [INFO] [timer.py:166:stop] 0/8660, SamplesPerSec=0.4331347806062952\n",
            "training loss: 3.1214084565639495\n",
            "[2021-03-02 22:40:23,890] [INFO] [timer.py:166:stop] 0/8680, SamplesPerSec=0.4331212740742206\n",
            "training loss: 3.0402878403663633\n",
            "[2021-03-02 22:41:04,503] [INFO] [timer.py:166:stop] 0/8700, SamplesPerSec=0.43324133449358637\n",
            "training loss: 3.0873339653015135\n",
            "[2021-03-02 22:41:45,343] [INFO] [timer.py:166:stop] 0/8720, SamplesPerSec=0.43335603851085014\n",
            "training loss: 3.0077155232429504\n",
            "[2021-03-02 22:42:31,474] [INFO] [timer.py:166:stop] 0/8740, SamplesPerSec=0.4333565311834202\n",
            "[2021-03-02 22:43:02,300] [INFO] [logging.py:60:log_dist] [Rank 0] step=4760, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1233789682388307\n",
            "[2021-03-02 22:43:19,179] [INFO] [timer.py:166:stop] 0/8760, SamplesPerSec=0.4333232827800983\n",
            "training loss: 2.8436492562294005\n",
            "[2021-03-02 22:44:04,952] [INFO] [timer.py:166:stop] 0/8780, SamplesPerSec=0.4333314932449807\n",
            "training loss: 3.1892645716667176\n",
            "[2021-03-02 22:44:47,145] [INFO] [timer.py:166:stop] 0/8800, SamplesPerSec=0.43341610461174485\n",
            "training loss: 3.069654643535614\n",
            "[2021-03-02 22:45:33,891] [INFO] [timer.py:166:stop] 0/8820, SamplesPerSec=0.4334033630516511\n",
            "training loss: 2.920142042636871\n",
            "[2021-03-02 22:46:13,113] [INFO] [timer.py:166:stop] 0/8840, SamplesPerSec=0.4335506416505257\n",
            "training loss: 3.143546450138092\n",
            "[2021-03-02 22:47:00,923] [INFO] [timer.py:166:stop] 0/8860, SamplesPerSec=0.4335150536548594\n",
            "training loss: 3.2340063035488127\n",
            "\n",
            " validation loss: 3.0843333303928375 \n",
            "\n",
            "[2021-03-02 22:47:56,439] [INFO] [timer.py:166:stop] 0/8880, SamplesPerSec=0.43353979292048256\n",
            "training loss: 3.1378222584724424\n",
            "[2021-03-02 22:48:40,783] [INFO] [timer.py:166:stop] 0/8900, SamplesPerSec=0.4335776185990936\n",
            "[2021-03-02 22:49:07,680] [INFO] [logging.py:60:log_dist] [Rank 0] step=4780, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0201459646224977\n",
            "[2021-03-02 22:49:24,734] [INFO] [timer.py:166:stop] 0/8920, SamplesPerSec=0.43362356319046347\n",
            "[2021-03-02 22:49:49,138] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "4-38269-3.0843333303928375\n",
            "\n",
            "EPOCH: 5\n",
            "training loss: 3.1316752433776855\n",
            "\n",
            " validation loss: 2.9893207609653474 \n",
            "\n",
            "BLEU metric: 0.01267132438141782\n",
            "Valid Structure Metric: 0.9672131147540983\n",
            "[2021-03-02 22:50:53,655] [INFO] [timer.py:166:stop] 0/8940, SamplesPerSec=0.43374067541897415\n",
            "training loss: 3.1440317630767822\n",
            "[2021-03-02 22:51:42,954] [INFO] [timer.py:166:stop] 0/8960, SamplesPerSec=0.4336737884640484\n",
            "training loss: 3.054826009273529\n",
            "[2021-03-02 22:52:31,868] [INFO] [timer.py:166:stop] 0/8980, SamplesPerSec=0.43361526550677715\n",
            "training loss: 2.9746404886245728\n",
            "[2021-03-02 22:53:14,311] [INFO] [timer.py:166:stop] 0/9000, SamplesPerSec=0.4336922377622977\n",
            "training loss: 3.0376711010932924\n",
            "[2021-03-02 22:54:00,401] [INFO] [timer.py:166:stop] 0/9020, SamplesPerSec=0.4336928213983172\n",
            "training loss: 2.938737565279007\n",
            "[2021-03-02 22:54:44,760] [INFO] [timer.py:166:stop] 0/9040, SamplesPerSec=0.4337294390603162\n",
            "training loss: 3.032269561290741\n",
            "[2021-03-02 22:55:30,494] [INFO] [timer.py:166:stop] 0/9060, SamplesPerSec=0.4337373350387184\n",
            "[2021-03-02 22:55:57,854] [INFO] [logging.py:60:log_dist] [Rank 0] step=4800, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.982201945781708\n",
            "[2021-03-02 22:56:18,044] [INFO] [timer.py:166:stop] 0/9080, SamplesPerSec=0.43370756251688114\n",
            "training loss: 2.953700840473175\n",
            "[2021-03-02 22:57:00,194] [INFO] [timer.py:166:stop] 0/9100, SamplesPerSec=0.4337895895243696\n",
            "training loss: 2.898775464296341\n",
            "[2021-03-02 22:57:41,726] [INFO] [timer.py:166:stop] 0/9120, SamplesPerSec=0.4338840531255749\n",
            "training loss: 3.03301454782486\n",
            "[2021-03-02 22:58:27,363] [INFO] [timer.py:166:stop] 0/9140, SamplesPerSec=0.4338935313043356\n",
            "training loss: 3.1513720631599424\n",
            "[2021-03-02 22:59:05,946] [INFO] [timer.py:166:stop] 0/9160, SamplesPerSec=0.4340480663817343\n",
            "training loss: 2.7872284054756165\n",
            "[2021-03-02 22:59:52,582] [INFO] [timer.py:166:stop] 0/9180, SamplesPerSec=0.43403664601028674\n",
            "training loss: 2.900171864032745\n",
            "[2021-03-02 23:00:34,310] [INFO] [timer.py:166:stop] 0/9200, SamplesPerSec=0.43412583663077875\n",
            "training loss: 3.0985261678695677\n",
            "[2021-03-02 23:01:14,404] [INFO] [timer.py:166:stop] 0/9220, SamplesPerSec=0.4342481042282625\n",
            "[2021-03-02 23:01:42,924] [INFO] [logging.py:60:log_dist] [Rank 0] step=4820, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9052370488643646\n",
            "[2021-03-02 23:02:01,737] [INFO] [timer.py:166:stop] 0/9240, SamplesPerSec=0.4342221010320341\n",
            "training loss: 3.2226546466350556\n",
            "[2021-03-02 23:02:50,895] [INFO] [timer.py:166:stop] 0/9260, SamplesPerSec=0.43415905653490067\n",
            "training loss: 3.095899748802185\n",
            "[2021-03-02 23:03:34,668] [INFO] [timer.py:166:stop] 0/9280, SamplesPerSec=0.43420570274033354\n",
            "training loss: 3.194749677181244\n",
            "[2021-03-02 23:04:25,703] [INFO] [timer.py:166:stop] 0/9300, SamplesPerSec=0.434104923808463\n",
            "training loss: 3.0072721898555757\n",
            "[2021-03-02 23:05:14,858] [INFO] [timer.py:166:stop] 0/9320, SamplesPerSec=0.4340426159203434\n",
            "training loss: 3.0526957333087923\n",
            "\n",
            " validation loss: 3.1038127869367598 \n",
            "\n",
            "[2021-03-02 23:06:11,077] [INFO] [timer.py:166:stop] 0/9340, SamplesPerSec=0.43403876694066407\n",
            "training loss: 3.025531214475632\n",
            "[2021-03-02 23:06:54,387] [INFO] [timer.py:166:stop] 0/9360, SamplesPerSec=0.4340945627726891\n",
            "training loss: 2.7987071752548216\n",
            "[2021-03-02 23:07:41,374] [INFO] [timer.py:166:stop] 0/9380, SamplesPerSec=0.4340762545484836\n",
            "[2021-03-02 23:08:09,063] [INFO] [logging.py:60:log_dist] [Rank 0] step=4840, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.948683297634125\n",
            "[2021-03-02 23:08:28,180] [INFO] [timer.py:166:stop] 0/9400, SamplesPerSec=0.4340616447114839\n",
            "training loss: 3.041826605796814\n",
            "[2021-03-02 23:09:14,559] [INFO] [timer.py:166:stop] 0/9420, SamplesPerSec=0.43405564157985244\n",
            "training loss: 3.1262478291988374\n",
            "[2021-03-02 23:10:02,061] [INFO] [timer.py:166:stop] 0/9440, SamplesPerSec=0.434027245224909\n",
            "training loss: 3.0762091875076294\n",
            "[2021-03-02 23:10:55,129] [INFO] [timer.py:166:stop] 0/9460, SamplesPerSec=0.4338881588218033\n",
            "training loss: 3.1022146880626678\n",
            "[2021-03-02 23:11:38,463] [INFO] [timer.py:166:stop] 0/9480, SamplesPerSec=0.4339430448708003\n",
            "training loss: 3.0258288025856017\n",
            "[2021-03-02 23:12:31,993] [INFO] [timer.py:166:stop] 0/9500, SamplesPerSec=0.43379561588688187\n",
            "training loss: 3.1021990299224855\n",
            "[2021-03-02 23:13:17,477] [INFO] [timer.py:166:stop] 0/9520, SamplesPerSec=0.4338079295687481\n",
            "training loss: 3.0068574786186217\n",
            "BLEU metric: 0.08566130652914375\n",
            "Valid Structure Metric: 0.9829842931937173\n",
            "[2021-03-02 23:14:41,293] [INFO] [timer.py:166:stop] 0/9540, SamplesPerSec=0.433872542359871\n",
            "[2021-03-02 23:15:05,276] [INFO] [logging.py:60:log_dist] [Rank 0] step=4860, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0195731043815615\n",
            "[2021-03-02 23:15:23,440] [INFO] [timer.py:166:stop] 0/9560, SamplesPerSec=0.4339503958121515\n",
            "training loss: 3.030427706241608\n",
            "[2021-03-02 23:16:09,625] [INFO] [timer.py:166:stop] 0/9580, SamplesPerSec=0.4339485378780527\n",
            "training loss: 2.943788695335388\n",
            "[2021-03-02 23:16:54,639] [INFO] [timer.py:166:stop] 0/9600, SamplesPerSec=0.43396966974921247\n",
            "training loss: 2.960616594552994\n",
            "[2021-03-02 23:17:44,754] [INFO] [timer.py:166:stop] 0/9620, SamplesPerSec=0.4338908358208413\n",
            "training loss: 3.0941832661628723\n",
            "[2021-03-02 23:18:31,248] [INFO] [timer.py:166:stop] 0/9640, SamplesPerSec=0.4338830667465194\n",
            "training loss: 3.0192513823509217\n",
            "[2021-03-02 23:19:24,969] [INFO] [timer.py:166:stop] 0/9660, SamplesPerSec=0.43373453518334365\n",
            "training loss: 2.9769651889801025\n",
            "[2021-03-02 23:20:10,829] [INFO] [timer.py:166:stop] 0/9680, SamplesPerSec=0.4337394626545286\n",
            "training loss: 3.0836088836193083\n",
            "[2021-03-02 23:20:53,605] [INFO] [timer.py:166:stop] 0/9700, SamplesPerSec=0.43380420589671054\n",
            "[2021-03-02 23:21:23,667] [INFO] [logging.py:60:log_dist] [Rank 0] step=4880, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.8637387216091157\n",
            "[2021-03-02 23:21:40,594] [INFO] [timer.py:166:stop] 0/9720, SamplesPerSec=0.43378710897817485\n",
            "training loss: 2.7842762768268585\n",
            "\n",
            " validation loss: 3.089766189455986 \n",
            "\n",
            "[2021-03-02 23:22:34,150] [INFO] [timer.py:166:stop] 0/9740, SamplesPerSec=0.43381376388009396\n",
            "training loss: 2.8496204733848574\n",
            "[2021-03-02 23:23:21,751] [INFO] [timer.py:166:stop] 0/9760, SamplesPerSec=0.43378491492139043\n",
            "training loss: 2.9144542634487154\n",
            "[2021-03-02 23:24:09,124] [INFO] [timer.py:166:stop] 0/9780, SamplesPerSec=0.4337605872341871\n",
            "training loss: 2.928214728832245\n",
            "[2021-03-02 23:24:58,522] [INFO] [timer.py:166:stop] 0/9800, SamplesPerSec=0.4336974770572358\n",
            "training loss: 3.222016727924347\n",
            "[2021-03-02 23:25:39,075] [INFO] [timer.py:166:stop] 0/9820, SamplesPerSec=0.4338041032576947\n",
            "training loss: 3.0833453118801115\n",
            "[2021-03-02 23:26:27,865] [INFO] [timer.py:166:stop] 0/9840, SamplesPerSec=0.4337527738755701\n",
            "training loss: 2.8462408900260927\n",
            "[2021-03-02 23:27:17,578] [INFO] [timer.py:166:stop] 0/9860, SamplesPerSec=0.43368405706216684\n",
            "[2021-03-02 23:27:51,894] [INFO] [logging.py:60:log_dist] [Rank 0] step=4900, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0965301036834716\n",
            "[2021-03-02 23:28:11,134] [INFO] [timer.py:166:stop] 0/9880, SamplesPerSec=0.43354248733838047\n",
            "training loss: 2.807827043533325\n",
            "[2021-03-02 23:28:58,306] [INFO] [timer.py:166:stop] 0/9900, SamplesPerSec=0.43352278146186685\n",
            "training loss: 3.1191677510738374\n",
            "[2021-03-02 23:29:46,783] [INFO] [timer.py:166:stop] 0/9920, SamplesPerSec=0.4334784422228611\n",
            "training loss: 3.2244190096855165\n",
            "[2021-03-02 23:30:33,964] [INFO] [timer.py:166:stop] 0/9940, SamplesPerSec=0.4334587615447254\n",
            "training loss: 3.0947511196136475\n",
            "[2021-03-02 23:31:22,876] [INFO] [timer.py:166:stop] 0/9960, SamplesPerSec=0.4334065224866757\n",
            "training loss: 3.027258586883545\n",
            "[2021-03-02 23:32:03,181] [INFO] [timer.py:166:stop] 0/9980, SamplesPerSec=0.43351656611043987\n",
            "training loss: 2.96461620926857\n",
            "[2021-03-02 23:32:42,042] [INFO] [timer.py:166:stop] 0/10000, SamplesPerSec=0.43365336524638903\n",
            "training loss: 2.9376378774642946\n",
            "[2021-03-02 23:33:26,823] [INFO] [timer.py:166:stop] 0/10020, SamplesPerSec=0.4336785473717916\n",
            "[2021-03-02 23:33:49,421] [INFO] [logging.py:60:log_dist] [Rank 0] step=4920, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9238916993141175\n",
            "[2021-03-02 23:34:05,479] [INFO] [timer.py:166:stop] 0/10040, SamplesPerSec=0.4338184305557518\n",
            "training loss: 2.929410493373871\n",
            "[2021-03-02 23:34:55,221] [INFO] [timer.py:166:stop] 0/10060, SamplesPerSec=0.4337503850896767\n",
            "training loss: 3.090408456325531\n",
            "[2021-03-02 23:35:47,189] [INFO] [timer.py:166:stop] 0/10080, SamplesPerSec=0.4336410973702809\n",
            "training loss: 3.1649388790130617\n",
            "[2021-03-02 23:36:33,403] [INFO] [timer.py:166:stop] 0/10100, SamplesPerSec=0.43363940991357647\n",
            "training loss: 3.1176023960113524\n",
            "[2021-03-02 23:37:24,385] [INFO] [timer.py:166:stop] 0/10120, SamplesPerSec=0.4335491436387802\n",
            "training loss: 3.0440121412277223\n",
            "\n",
            " validation loss: 3.082933786511421 \n",
            "\n",
            "BLEU metric: 0.02119397915288601\n",
            "Valid Structure Metric: 0.9777486910994765\n",
            "[2021-03-02 23:39:01,278] [INFO] [timer.py:166:stop] 0/10140, SamplesPerSec=0.43352058887922507\n",
            "training loss: 3.1941794753074646\n",
            "[2021-03-02 23:39:47,619] [INFO] [timer.py:166:stop] 0/10160, SamplesPerSec=0.4335167926143446\n",
            "training loss: 3.150090140104294\n",
            "[2021-03-02 23:40:34,318] [INFO] [timer.py:166:stop] 0/10180, SamplesPerSec=0.43350640561329007\n",
            "[2021-03-02 23:40:57,584] [INFO] [logging.py:60:log_dist] [Rank 0] step=4940, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.04057030081749\n",
            "[2021-03-02 23:41:16,713] [INFO] [timer.py:166:stop] 0/10200, SamplesPerSec=0.4335753897165558\n",
            "[2021-03-02 23:41:33,531] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "5-39545-3.082933786511421\n",
            "\n",
            "training loss: 3.0834620118141176\n",
            "[2021-03-02 23:42:09,963] [INFO] [timer.py:166:stop] 0/10220, SamplesPerSec=0.43356431548320307\n",
            "training loss: 2.9426910519599914\n",
            "[2021-03-02 23:43:01,873] [INFO] [timer.py:166:stop] 0/10240, SamplesPerSec=0.4334582514140756\n",
            "training loss: 3.077085053920746\n",
            "[2021-03-02 23:43:45,223] [INFO] [timer.py:166:stop] 0/10260, SamplesPerSec=0.43350940352548006\n",
            "training loss: 3.038714611530304\n",
            "[2021-03-02 23:44:37,848] [INFO] [timer.py:166:stop] 0/10280, SamplesPerSec=0.4333908201128212\n",
            "training loss: 2.575746178627014\n",
            "[2021-03-02 23:45:21,765] [INFO] [timer.py:166:stop] 0/10300, SamplesPerSec=0.4334315474944681\n",
            "training loss: 3.203459644317627\n",
            "[2021-03-02 23:46:06,294] [INFO] [timer.py:166:stop] 0/10320, SamplesPerSec=0.4334609909203676\n",
            "training loss: 3.0092546343803406\n",
            "[2021-03-02 23:46:55,606] [INFO] [timer.py:166:stop] 0/10340, SamplesPerSec=0.4334033824906527\n",
            "[2021-03-02 23:47:22,301] [INFO] [logging.py:60:log_dist] [Rank 0] step=4960, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.860148197412491\n",
            "[2021-03-02 23:47:42,541] [INFO] [timer.py:166:stop] 0/10360, SamplesPerSec=0.43338912871886737\n",
            "training loss: 2.9784948766231536\n",
            "[2021-03-02 23:48:26,893] [INFO] [timer.py:166:stop] 0/10380, SamplesPerSec=0.43342168323060465\n",
            "training loss: 2.8983051657676695\n",
            "[2021-03-02 23:49:08,784] [INFO] [timer.py:166:stop] 0/10400, SamplesPerSec=0.4334985908043516\n",
            "training loss: 3.019331085681915\n",
            "[2021-03-02 23:49:54,556] [INFO] [timer.py:166:stop] 0/10420, SamplesPerSec=0.4335052204675706\n",
            "training loss: 3.1041398286819457\n",
            "[2021-03-02 23:50:40,359] [INFO] [timer.py:166:stop] 0/10440, SamplesPerSec=0.4335112481764683\n",
            "training loss: 2.976649361848831\n",
            "[2021-03-02 23:51:26,159] [INFO] [timer.py:166:stop] 0/10460, SamplesPerSec=0.43351731323347314\n",
            "training loss: 3.110343539714813\n",
            "[2021-03-02 23:52:14,265] [INFO] [timer.py:166:stop] 0/10480, SamplesPerSec=0.4334819916656747\n",
            "training loss: 2.788752204179764\n",
            "[2021-03-02 23:52:57,580] [INFO] [timer.py:166:stop] 0/10500, SamplesPerSec=0.43353258492407787\n",
            "[2021-03-02 23:53:25,204] [INFO] [logging.py:60:log_dist] [Rank 0] step=4980, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.006439983844757\n",
            "[2021-03-02 23:53:40,047] [INFO] [timer.py:166:stop] 0/10520, SamplesPerSec=0.4335981445679788\n",
            "training loss: 2.9969752848148348\n",
            "\n",
            " validation loss: 3.122009867429733 \n",
            "\n",
            "[2021-03-02 23:54:33,579] [INFO] [timer.py:166:stop] 0/10540, SamplesPerSec=0.4336492214955011\n",
            "training loss: 3.06458203792572\n",
            "[2021-03-02 23:55:16,977] [INFO] [timer.py:166:stop] 0/10560, SamplesPerSec=0.43369777017313477\n",
            "training loss: 3.087831711769104\n",
            "[2021-03-02 23:56:06,459] [INFO] [timer.py:166:stop] 0/10580, SamplesPerSec=0.433637954247786\n",
            "training loss: 3.0484081625938417\n",
            "[2021-03-02 23:56:55,672] [INFO] [timer.py:166:stop] 0/10600, SamplesPerSec=0.4335831553526508\n",
            "training loss: 3.0460466027259825\n",
            "[2021-03-02 23:57:40,549] [INFO] [timer.py:166:stop] 0/10620, SamplesPerSec=0.43360533569058657\n",
            "training loss: 3.0246075630187987\n",
            "[2021-03-02 23:58:30,671] [INFO] [timer.py:166:stop] 0/10640, SamplesPerSec=0.4335347457762568\n",
            "training loss: 2.9998512268066406\n",
            "[2021-03-02 23:59:18,694] [INFO] [timer.py:166:stop] 0/10660, SamplesPerSec=0.43350143750985143\n",
            "[2021-03-02 23:59:47,948] [INFO] [logging.py:60:log_dist] [Rank 0] step=5000, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0311838388442993\n",
            "[2021-03-03 00:00:03,851] [INFO] [timer.py:166:stop] 0/10680, SamplesPerSec=0.43351871496156785\n",
            "training loss: 3.028564524650574\n",
            "[2021-03-03 00:00:52,837] [INFO] [timer.py:166:stop] 0/10700, SamplesPerSec=0.433468657857721\n",
            "training loss: 3.0981148064136503\n",
            "[2021-03-03 00:01:37,691] [INFO] [timer.py:166:stop] 0/10720, SamplesPerSec=0.43349124720748644\n",
            "training loss: 2.976342034339905\n",
            "BLEU metric: 0.027980624894615617\n",
            "Valid Structure Metric: 0.9698952879581152\n",
            "[2021-03-03 00:03:13,447] [INFO] [timer.py:166:stop] 0/10740, SamplesPerSec=0.43350283367300246\n",
            "training loss: 2.955613672733307\n",
            "[2021-03-03 00:04:02,006] [INFO] [timer.py:166:stop] 0/10760, SamplesPerSec=0.4334605525879373\n",
            "training loss: 3.017216944694519\n",
            "[2021-03-03 00:04:43,413] [INFO] [timer.py:166:stop] 0/10780, SamplesPerSec=0.43354312896259145\n",
            "training loss: 3.1976375699043276\n",
            "[2021-03-03 00:05:27,219] [INFO] [timer.py:166:stop] 0/10800, SamplesPerSec=0.43358365766531126\n",
            "training loss: 2.9283878326416017\n",
            "[2021-03-03 00:06:18,969] [INFO] [timer.py:166:stop] 0/10820, SamplesPerSec=0.43348600747687266\n",
            "[2021-03-03 00:06:48,064] [INFO] [logging.py:60:log_dist] [Rank 0] step=5020, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.161176657676697\n",
            "[2021-03-03 00:07:02,749] [INFO] [timer.py:166:stop] 0/10840, SamplesPerSec=0.4335269302019435\n",
            "training loss: 2.903620159626007\n",
            "[2021-03-03 00:07:48,602] [INFO] [timer.py:166:stop] 0/10860, SamplesPerSec=0.43353182813377006\n",
            "training loss: 3.072840714454651\n",
            "[2021-03-03 00:08:34,001] [INFO] [timer.py:166:stop] 0/10880, SamplesPerSec=0.4335445457895186\n",
            "training loss: 3.0530423283576966\n",
            "[2021-03-03 00:09:16,129] [INFO] [timer.py:166:stop] 0/10900, SamplesPerSec=0.4336136325001034\n",
            "training loss: 2.9270287454128265\n",
            "[2021-03-03 00:10:06,608] [INFO] [timer.py:166:stop] 0/10920, SamplesPerSec=0.4335386938164357\n",
            "training loss: 3.005010986328125\n",
            "\n",
            " validation loss: 2.933946266770363 \n",
            "\n",
            "[2021-03-03 00:11:11,203] [INFO] [timer.py:166:stop] 0/10940, SamplesPerSec=0.4334012731109477\n",
            "training loss: 3.0954599142074586\n",
            "[2021-03-03 00:12:02,416] [INFO] [timer.py:166:stop] 0/10960, SamplesPerSec=0.43331449138665923\n",
            "training loss: 3.0717406153678892\n",
            "[2021-03-03 00:12:42,742] [INFO] [timer.py:166:stop] 0/10980, SamplesPerSec=0.4334142652224919\n",
            "[2021-03-03 00:13:12,471] [INFO] [logging.py:60:log_dist] [Rank 0] step=5040, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.990491843223572\n",
            "[2021-03-03 00:13:29,226] [INFO] [timer.py:166:stop] 0/11000, SamplesPerSec=0.4334085088066854\n",
            "training loss: 2.8545522809028627\n",
            "[2021-03-03 00:14:13,057] [INFO] [timer.py:166:stop] 0/11020, SamplesPerSec=0.43344802921175907\n",
            "training loss: 3.039732015132904\n",
            "[2021-03-03 00:15:02,359] [INFO] [timer.py:166:stop] 0/11040, SamplesPerSec=0.4333942787980054\n",
            "training loss: 2.887445294857025\n",
            "[2021-03-03 00:15:46,629] [INFO] [timer.py:166:stop] 0/11060, SamplesPerSec=0.4334262064266212\n",
            "training loss: 2.7054622769355774\n",
            "[2021-03-03 00:16:32,050] [INFO] [timer.py:166:stop] 0/11080, SamplesPerSec=0.4334385143453693\n",
            "training loss: 2.9493386924266813\n",
            "[2021-03-03 00:17:24,399] [INFO] [timer.py:166:stop] 0/11100, SamplesPerSec=0.43333351771980333\n",
            "training loss: 3.02954763174057\n",
            "[2021-03-03 00:18:15,658] [INFO] [timer.py:166:stop] 0/11120, SamplesPerSec=0.4332473432082134\n",
            "training loss: 3.240044867992401\n",
            "[2021-03-03 00:19:07,265] [INFO] [timer.py:166:stop] 0/11140, SamplesPerSec=0.43315566771491615\n",
            "[2021-03-03 00:19:32,974] [INFO] [logging.py:60:log_dist] [Rank 0] step=5060, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9846269845962525\n",
            "[2021-03-03 00:19:49,125] [INFO] [timer.py:166:stop] 0/11160, SamplesPerSec=0.43322824568308993\n",
            "training loss: 2.9726253747940063\n",
            "[2021-03-03 00:20:31,718] [INFO] [timer.py:166:stop] 0/11180, SamplesPerSec=0.4332882662719104\n",
            "training loss: 3.0265803217887877\n",
            "[2021-03-03 00:21:17,485] [INFO] [timer.py:166:stop] 0/11200, SamplesPerSec=0.4332948776031476\n",
            "training loss: 2.930367237329483\n",
            "[2021-03-03 00:22:05,930] [INFO] [timer.py:166:stop] 0/11220, SamplesPerSec=0.4332566438852879\n",
            "training loss: 2.9723685145378114\n",
            "[2021-03-03 00:22:51,116] [INFO] [timer.py:166:stop] 0/11240, SamplesPerSec=0.43327298624218663\n",
            "training loss: 2.8848145604133606\n",
            "[2021-03-03 00:23:41,925] [INFO] [timer.py:166:stop] 0/11260, SamplesPerSec=0.4331955237376353\n",
            "training loss: 3.0363463521003724\n",
            "[2021-03-03 00:24:29,988] [INFO] [timer.py:166:stop] 0/11280, SamplesPerSec=0.43316404986355667\n",
            "training loss: 3.050132077932358\n",
            "[2021-03-03 00:25:13,847] [INFO] [timer.py:166:stop] 0/11300, SamplesPerSec=0.43320250727257553\n",
            "[2021-03-03 00:25:41,950] [INFO] [logging.py:60:log_dist] [Rank 0] step=5080, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.024688667058945\n",
            "[2021-03-03 00:26:02,527] [INFO] [timer.py:166:stop] 0/11320, SamplesPerSec=0.43316088695089\n",
            "training loss: 3.119846534729004\n",
            "\n",
            " validation loss: 3.0546083033084868 \n",
            "\n",
            "BLEU metric: 0.08193958857993693\n",
            "Valid Structure Metric: 0.9869109947643979\n",
            "[2021-03-03 00:27:34,580] [INFO] [timer.py:166:stop] 0/11340, SamplesPerSec=0.4332261260013291\n",
            "training loss: 3.0323856294155123\n",
            "[2021-03-03 00:28:22,977] [INFO] [timer.py:166:stop] 0/11360, SamplesPerSec=0.43318929375900306\n",
            "training loss: 2.7821558713912964\n",
            "[2021-03-03 00:29:07,868] [INFO] [timer.py:166:stop] 0/11380, SamplesPerSec=0.43321041412594696\n",
            "training loss: 3.0629172384738923\n",
            "[2021-03-03 00:29:53,711] [INFO] [timer.py:166:stop] 0/11400, SamplesPerSec=0.43321578944174083\n",
            "training loss: 2.9665233552455903\n",
            "[2021-03-03 00:30:33,938] [INFO] [timer.py:166:stop] 0/11420, SamplesPerSec=0.4333134752940154\n",
            "training loss: 3.050054907798767\n",
            "[2021-03-03 00:31:16,269] [INFO] [timer.py:166:stop] 0/11440, SamplesPerSec=0.43337631856982706\n",
            "training loss: 2.9540663957595825\n",
            "[2021-03-03 00:32:00,958] [INFO] [timer.py:166:stop] 0/11460, SamplesPerSec=0.43340028062228564\n",
            "[2021-03-03 00:32:27,952] [INFO] [logging.py:60:log_dist] [Rank 0] step=5100, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0917059898376467\n",
            "[2021-03-03 00:32:44,191] [INFO] [timer.py:166:stop] 0/11480, SamplesPerSec=0.433448016085814\n",
            "[2021-03-03 00:32:53,439] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "5-40820-3.0546083033084868\n",
            "\n",
            "training loss: 3.0776856660842897\n",
            "[2021-03-03 00:33:39,449] [INFO] [timer.py:166:stop] 0/11500, SamplesPerSec=0.4333914432793167\n",
            "training loss: 3.0798037946224213\n",
            "[2021-03-03 00:34:29,760] [INFO] [timer.py:166:stop] 0/11520, SamplesPerSec=0.4333236090362718\n",
            "training loss: 2.9075864970684053\n",
            "[2021-03-03 00:35:15,510] [INFO] [timer.py:166:stop] 0/11540, SamplesPerSec=0.4333302377481504\n",
            "training loss: 2.9819331228733064\n",
            "[2021-03-03 00:36:05,515] [INFO] [timer.py:166:stop] 0/11560, SamplesPerSec=0.43326771158125377\n",
            "training loss: 3.1343591153621673\n",
            "[2021-03-03 00:37:00,004] [INFO] [timer.py:166:stop] 0/11580, SamplesPerSec=0.4331327642690884\n",
            "training loss: 2.8591345727443693\n",
            "[2021-03-03 00:37:46,546] [INFO] [timer.py:166:stop] 0/11600, SamplesPerSec=0.433126874880621\n",
            "training loss: 3.04276442527771\n",
            "[2021-03-03 00:38:35,750] [INFO] [timer.py:166:stop] 0/11620, SamplesPerSec=0.43307803028460773\n",
            "[2021-03-03 00:39:04,909] [INFO] [logging.py:60:log_dist] [Rank 0] step=5120, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0067487478256227\n",
            "[2021-03-03 00:39:20,867] [INFO] [timer.py:166:stop] 0/11640, SamplesPerSec=0.4330952049786892\n",
            "training loss: 3.137892317771912\n",
            "[2021-03-03 00:40:09,361] [INFO] [timer.py:166:stop] 0/11660, SamplesPerSec=0.43305800800185357\n",
            "training loss: 3.0506691873073577\n",
            "[2021-03-03 00:40:56,778] [INFO] [timer.py:166:stop] 0/11680, SamplesPerSec=0.433038237773879\n",
            "training loss: 3.090646171569824\n",
            "[2021-03-03 00:41:47,739] [INFO] [timer.py:166:stop] 0/11700, SamplesPerSec=0.4329617259048675\n",
            "training loss: 2.8698046505451202\n",
            "[2021-03-03 00:42:33,158] [INFO] [timer.py:166:stop] 0/11720, SamplesPerSec=0.4329741653769501\n",
            "training loss: 3.0313025057315826\n",
            "\n",
            " validation loss: 3.110784187912941 \n",
            "\n",
            "[2021-03-03 00:43:29,131] [INFO] [timer.py:166:stop] 0/11740, SamplesPerSec=0.43297796714589903\n",
            "training loss: 2.981641787290573\n",
            "[2021-03-03 00:44:12,844] [INFO] [timer.py:166:stop] 0/11760, SamplesPerSec=0.43301753598714343\n",
            "training loss: 2.8515892744064333\n",
            "[2021-03-03 00:44:54,840] [INFO] [timer.py:166:stop] 0/11780, SamplesPerSec=0.4330843067192837\n",
            "[2021-03-03 00:45:21,875] [INFO] [logging.py:60:log_dist] [Rank 0] step=5140, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.988296961784363\n",
            "[2021-03-03 00:45:37,698] [INFO] [timer.py:166:stop] 0/11800, SamplesPerSec=0.4331371662505627\n",
            "training loss: 3.0840559720993044\n",
            "[2021-03-03 00:46:25,744] [INFO] [timer.py:166:stop] 0/11820, SamplesPerSec=0.4331075095420337\n",
            "training loss: 3.0097129344940186\n",
            "[2021-03-03 00:47:14,720] [INFO] [timer.py:166:stop] 0/11840, SamplesPerSec=0.43306322382556545\n",
            "training loss: 3.0109061360359193\n",
            "[2021-03-03 00:47:58,332] [INFO] [timer.py:166:stop] 0/11860, SamplesPerSec=0.4331039092866085\n",
            "training loss: 3.089920234680176\n",
            "[2021-03-03 00:48:41,126] [INFO] [timer.py:166:stop] 0/11880, SamplesPerSec=0.43315741263381\n",
            "training loss: 2.9066870331764223\n",
            "[2021-03-03 00:49:34,601] [INFO] [timer.py:166:stop] 0/11900, SamplesPerSec=0.433042321138841\n",
            "training loss: 2.9832295060157774\n",
            "[2021-03-03 00:50:16,475] [INFO] [timer.py:166:stop] 0/11920, SamplesPerSec=0.43311020470935074\n",
            "training loss: 2.8912242531776426\n",
            "BLEU metric: 0.015937797272987938\n",
            "Valid Structure Metric: 0.9764397905759162\n",
            "[2021-03-03 00:51:54,282] [INFO] [timer.py:166:stop] 0/11940, SamplesPerSec=0.43311836589793523\n",
            "[2021-03-03 00:52:21,757] [INFO] [logging.py:60:log_dist] [Rank 0] step=5160, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.8568285048007964\n",
            "[2021-03-03 00:52:36,934] [INFO] [timer.py:166:stop] 0/11960, SamplesPerSec=0.43317369924264937\n",
            "training loss: 3.177103352546692\n",
            "[2021-03-03 00:53:22,951] [INFO] [timer.py:166:stop] 0/11980, SamplesPerSec=0.43317613918704373\n",
            "training loss: 2.9981289863586427\n",
            "[2021-03-03 00:54:06,930] [INFO] [timer.py:166:stop] 0/12000, SamplesPerSec=0.43321045977878875\n",
            "training loss: 2.8892282247543335\n",
            "[2021-03-03 00:54:46,975] [INFO] [timer.py:166:stop] 0/12020, SamplesPerSec=0.43330612759058235\n",
            "training loss: 3.09351726770401\n",
            "[2021-03-03 00:55:34,271] [INFO] [timer.py:166:stop] 0/12040, SamplesPerSec=0.43328839643006933\n",
            "training loss: 2.948554575443268\n",
            "[2021-03-03 00:56:18,665] [INFO] [timer.py:166:stop] 0/12060, SamplesPerSec=0.43331591069875297\n",
            "training loss: 3.0008683800697327\n",
            "[2021-03-03 00:57:03,924] [INFO] [timer.py:166:stop] 0/12080, SamplesPerSec=0.4333298804258199\n",
            "training loss: 2.9562829971313476\n",
            "[2021-03-03 00:57:51,408] [INFO] [timer.py:166:stop] 0/12100, SamplesPerSec=0.43330928120168105\n",
            "[2021-03-03 00:58:22,305] [INFO] [logging.py:60:log_dist] [Rank 0] step=5180, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.265891993045807\n",
            "[2021-03-03 00:58:43,785] [INFO] [timer.py:166:stop] 0/12120, SamplesPerSec=0.4332129514476252\n",
            "training loss: 3.115006613731384\n",
            "\n",
            " validation loss: 2.9791254967451097 \n",
            "\n",
            "[2021-03-03 00:59:43,125] [INFO] [timer.py:166:stop] 0/12140, SamplesPerSec=0.4331655246465733\n",
            "training loss: 2.970337224006653\n",
            "[2021-03-03 01:00:29,935] [INFO] [timer.py:166:stop] 0/12160, SamplesPerSec=0.4331557068474619\n",
            "training loss: 2.8894800305366517\n",
            "[2021-03-03 01:01:14,965] [INFO] [timer.py:166:stop] 0/12180, SamplesPerSec=0.43317335571191534\n",
            "training loss: 3.068498933315277\n",
            "[2021-03-03 01:01:59,355] [INFO] [timer.py:166:stop] 0/12200, SamplesPerSec=0.43320079240255555\n",
            "training loss: 3.000315475463867\n",
            "[2021-03-03 01:02:45,099] [INFO] [timer.py:166:stop] 0/12220, SamplesPerSec=0.4332073432528693\n",
            "training loss: 3.052086514234543\n",
            "[2021-03-03 01:03:34,330] [INFO] [timer.py:166:stop] 0/12240, SamplesPerSec=0.43316040076919593\n",
            "training loss: 3.1973002433776854\n",
            "[2021-03-03 01:04:19,992] [INFO] [timer.py:166:stop] 0/12260, SamplesPerSec=0.4331682445606298\n",
            "[2021-03-03 01:04:47,576] [INFO] [logging.py:60:log_dist] [Rank 0] step=5200, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.8898454546928405\n",
            "[2021-03-03 01:05:05,406] [INFO] [timer.py:166:stop] 0/12280, SamplesPerSec=0.4331798680406562\n",
            "training loss: 3.149027144908905\n",
            "[2021-03-03 01:05:47,895] [INFO] [timer.py:166:stop] 0/12300, SamplesPerSec=0.4332360816854283\n",
            "training loss: 2.884781873226166\n",
            "[2021-03-03 01:06:34,246] [INFO] [timer.py:166:stop] 0/12320, SamplesPerSec=0.4332332647359095\n",
            "training loss: 3.137633830308914\n",
            "[2021-03-03 01:07:18,509] [INFO] [timer.py:166:stop] 0/12340, SamplesPerSec=0.4332622265542527\n",
            "training loss: 2.731943225860596\n",
            "[2021-03-03 01:08:02,593] [INFO] [timer.py:166:stop] 0/12360, SamplesPerSec=0.4332938210464298\n",
            "training loss: 2.978144085407257\n",
            "[2021-03-03 01:08:50,201] [INFO] [timer.py:166:stop] 0/12380, SamplesPerSec=0.4332718671389759\n",
            "training loss: 3.126494812965393\n",
            "[2021-03-03 01:09:32,692] [INFO] [timer.py:166:stop] 0/12400, SamplesPerSec=0.43332747645493247\n",
            "training loss: 2.822411459684372\n",
            "[2021-03-03 01:10:19,911] [INFO] [timer.py:166:stop] 0/12420, SamplesPerSec=0.433311410732424\n",
            "[2021-03-03 01:10:45,393] [INFO] [logging.py:60:log_dist] [Rank 0] step=5220, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1357712507247926\n",
            "[2021-03-03 01:11:02,625] [INFO] [timer.py:166:stop] 0/12440, SamplesPerSec=0.4333634162248455\n",
            "training loss: 3.044649463891983\n",
            "[2021-03-03 01:11:48,715] [INFO] [timer.py:166:stop] 0/12460, SamplesPerSec=0.43336436513485893\n",
            "training loss: 3.0150916457176207\n",
            "[2021-03-03 01:12:38,699] [INFO] [timer.py:166:stop] 0/12480, SamplesPerSec=0.433306713881871\n",
            "training loss: 2.994546103477478\n",
            "[2021-03-03 01:13:25,978] [INFO] [timer.py:166:stop] 0/12500, SamplesPerSec=0.43328988698510124\n",
            "training loss: 3.0883105516433718\n",
            "[2021-03-03 01:14:18,235] [INFO] [timer.py:166:stop] 0/12520, SamplesPerSec=0.4331985004297883\n",
            "training loss: 3.2434805631637573\n",
            "\n",
            " validation loss: 2.972603076696396 \n",
            "\n",
            "BLEU metric: 0.012753796380576214\n",
            "Valid Structure Metric: 0.9685863874345549\n",
            "[2021-03-03 01:16:04,880] [INFO] [timer.py:166:stop] 0/12540, SamplesPerSec=0.4331423864623662\n",
            "training loss: 2.7202250123023988\n",
            "[2021-03-03 01:16:43,895] [INFO] [timer.py:166:stop] 0/12560, SamplesPerSec=0.43324939828656933\n",
            "training loss: 3.0765070021152496\n",
            "[2021-03-03 01:17:31,404] [INFO] [timer.py:166:stop] 0/12580, SamplesPerSec=0.43322934926762\n",
            "[2021-03-03 01:17:56,667] [INFO] [logging.py:60:log_dist] [Rank 0] step=5240, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0138206601142885\n",
            "[2021-03-03 01:18:16,749] [INFO] [timer.py:166:stop] 0/12600, SamplesPerSec=0.43324159429072523\n",
            "training loss: 3.157718241214752\n",
            "[2021-03-03 01:19:08,517] [INFO] [timer.py:166:stop] 0/12620, SamplesPerSec=0.43315827988010386\n",
            "training loss: 2.986010253429413\n",
            "[2021-03-03 01:19:53,665] [INFO] [timer.py:166:stop] 0/12640, SamplesPerSec=0.43317353087852123\n",
            "training loss: 2.9979811310768127\n",
            "[2021-03-03 01:20:38,780] [INFO] [timer.py:166:stop] 0/12660, SamplesPerSec=0.4331892193479182\n",
            "training loss: 3.1601230263710023\n",
            "[2021-03-03 01:21:23,012] [INFO] [timer.py:166:stop] 0/12680, SamplesPerSec=0.43321791668229104\n",
            "training loss: 3.0127403140068054\n",
            "[2021-03-03 01:22:07,667] [INFO] [timer.py:166:stop] 0/12700, SamplesPerSec=0.43324030244077705\n",
            "training loss: 2.824039614200592\n",
            "[2021-03-03 01:22:55,471] [INFO] [timer.py:166:stop] 0/12720, SamplesPerSec=0.43321612988025404\n",
            "training loss: 2.9733998477458954\n",
            "[2021-03-03 01:23:44,423] [INFO] [timer.py:166:stop] 0/12740, SamplesPerSec=0.43317511357441385\n",
            "[2021-03-03 01:24:15,007] [INFO] [logging.py:60:log_dist] [Rank 0] step=5260, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1355797290802\n",
            "[2021-03-03 01:24:28,062] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "5-42095-2.972603076696396\n",
            "\n",
            "[2021-03-03 01:24:37,897] [INFO] [timer.py:166:stop] 0/12760, SamplesPerSec=0.433151527324043\n",
            "training loss: 3.055258536338806\n",
            "[2021-03-03 01:25:21,592] [INFO] [timer.py:166:stop] 0/12780, SamplesPerSec=0.4331879619818191\n",
            "training loss: 3.0817207634449004\n",
            "[2021-03-03 01:26:06,058] [INFO] [timer.py:166:stop] 0/12800, SamplesPerSec=0.4332129832831964\n",
            "training loss: 3.090810775756836\n",
            "[2021-03-03 01:26:55,222] [INFO] [timer.py:166:stop] 0/12820, SamplesPerSec=0.4331691377944016\n",
            "training loss: 2.9441884636878966\n",
            "[2021-03-03 01:27:45,155] [INFO] [timer.py:166:stop] 0/12840, SamplesPerSec=0.4331141978659544\n",
            "training loss: 2.887483114004135\n",
            "[2021-03-03 01:28:27,514] [INFO] [timer.py:166:stop] 0/12860, SamplesPerSec=0.43316994049342056\n",
            "training loss: 2.991689646244049\n",
            "[2021-03-03 01:29:14,933] [INFO] [timer.py:166:stop] 0/12880, SamplesPerSec=0.43315179805001386\n",
            "training loss: 3.24906530380249\n",
            "[2021-03-03 01:30:05,612] [INFO] [timer.py:166:stop] 0/12900, SamplesPerSec=0.4330863023279433\n",
            "[2021-03-03 01:30:27,995] [INFO] [logging.py:60:log_dist] [Rank 0] step=5280, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.999190831184387\n",
            "[2021-03-03 01:30:44,514] [INFO] [timer.py:166:stop] 0/12920, SamplesPerSec=0.4331920404555484\n",
            "training loss: 2.8782902121543885\n",
            "\n",
            " validation loss: 3.061182290315628 \n",
            "\n",
            "[2021-03-03 01:31:35,129] [INFO] [timer.py:166:stop] 0/12940, SamplesPerSec=0.43327345770131\n",
            "training loss: 3.029399108886719\n",
            "[2021-03-03 01:32:22,356] [INFO] [timer.py:166:stop] 0/12960, SamplesPerSec=0.43325804162427234\n",
            "training loss: 2.819199651479721\n",
            "[2021-03-03 01:33:12,710] [INFO] [timer.py:166:stop] 0/12980, SamplesPerSec=0.43319744793424964\n",
            "training loss: 3.1729623079299927\n",
            "[2021-03-03 01:33:56,542] [INFO] [timer.py:166:stop] 0/13000, SamplesPerSec=0.433231227092203\n",
            "training loss: 2.8227179765701296\n",
            "[2021-03-03 01:34:44,500] [INFO] [timer.py:166:stop] 0/13020, SamplesPerSec=0.4332054014802696\n",
            "training loss: 2.9517659902572633\n",
            "[2021-03-03 01:35:29,839] [INFO] [timer.py:166:stop] 0/13040, SamplesPerSec=0.4332173706470068\n",
            "training loss: 3.044439125061035\n",
            "[2021-03-03 01:36:13,712] [INFO] [timer.py:166:stop] 0/13060, SamplesPerSec=0.4332503590832084\n",
            "[2021-03-03 01:36:41,128] [INFO] [logging.py:60:log_dist] [Rank 0] step=5300, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0861317992210386\n",
            "[2021-03-03 01:36:59,271] [INFO] [timer.py:166:stop] 0/13080, SamplesPerSec=0.43325905277679677\n",
            "training loss: 3.0284499168395995\n",
            "[2021-03-03 01:37:48,229] [INFO] [timer.py:166:stop] 0/13100, SamplesPerSec=0.4332190204196266\n",
            "training loss: 3.119162082672119\n",
            "[2021-03-03 01:38:40,040] [INFO] [timer.py:166:stop] 0/13120, SamplesPerSec=0.43313832111915224\n",
            "training loss: 2.9056451976299287\n",
            "BLEU metric: 0.027866283812914103\n",
            "Valid Structure Metric: 0.9777486910994765\n",
            "[2021-03-03 01:40:10,904] [INFO] [timer.py:166:stop] 0/13140, SamplesPerSec=0.4330973985382352\n",
            "training loss: 2.931837660074234\n",
            "[2021-03-03 01:40:55,719] [INFO] [timer.py:166:stop] 0/13160, SamplesPerSec=0.43311688037159435\n",
            "training loss: 3.0389050006866456\n",
            "[2021-03-03 01:41:47,038] [INFO] [timer.py:166:stop] 0/13180, SamplesPerSec=0.433043732947047\n",
            "training loss: 2.9512078762054443\n",
            "[2021-03-03 01:42:33,101] [INFO] [timer.py:166:stop] 0/13200, SamplesPerSec=0.4330454918856573\n",
            "training loss: 2.877565693855286\n",
            "[2021-03-03 01:43:18,087] [INFO] [timer.py:166:stop] 0/13220, SamplesPerSec=0.4330625246376803\n",
            "[2021-03-03 01:43:46,846] [INFO] [logging.py:60:log_dist] [Rank 0] step=5320, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9994378328323363\n",
            "[2021-03-03 01:44:06,101] [INFO] [timer.py:166:stop] 0/13240, SamplesPerSec=0.4330366249551483\n",
            "training loss: 3.0531922221183776\n",
            "[2021-03-03 01:44:49,834] [INFO] [timer.py:166:stop] 0/13260, SamplesPerSec=0.43307134940454767\n",
            "training loss: 2.869581162929535\n",
            "[2021-03-03 01:45:31,652] [INFO] [timer.py:166:stop] 0/13280, SamplesPerSec=0.43313302522232405\n",
            "training loss: 2.8327760100364685\n",
            "[2021-03-03 01:46:19,160] [INFO] [timer.py:166:stop] 0/13300, SamplesPerSec=0.43311426383107415\n",
            "training loss: 2.8411763668060304\n",
            "[2021-03-03 01:47:04,570] [INFO] [timer.py:166:stop] 0/13320, SamplesPerSec=0.4331251064151592\n",
            "training loss: 2.9420066833496095\n",
            "\n",
            " validation loss: 3.0590015649795532 \n",
            "\n",
            "[2021-03-03 01:47:58,118] [INFO] [timer.py:166:stop] 0/13340, SamplesPerSec=0.4331663561516817\n",
            "training loss: 2.970024585723877\n",
            "[2021-03-03 01:48:40,913] [INFO] [timer.py:166:stop] 0/13360, SamplesPerSec=0.43321383537206704\n",
            "training loss: 2.9831092834472654\n",
            "[2021-03-03 01:49:25,468] [INFO] [timer.py:166:stop] 0/13380, SamplesPerSec=0.43323646805924954\n",
            "[2021-03-03 01:49:51,445] [INFO] [logging.py:60:log_dist] [Rank 0] step=5340, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.1070562601089478\n",
            "[2021-03-03 01:50:14,205] [INFO] [timer.py:166:stop] 0/13400, SamplesPerSec=0.43320046501924364\n",
            "training loss: 2.7813392519950866\n",
            "[2021-03-03 01:50:57,871] [INFO] [timer.py:166:stop] 0/13420, SamplesPerSec=0.43323550198349897\n",
            "training loss: 2.912951636314392\n",
            "[2021-03-03 01:51:41,725] [INFO] [timer.py:166:stop] 0/13440, SamplesPerSec=0.43326779592120274\n",
            "training loss: 2.885996717214584\n",
            "[2021-03-03 01:52:24,099] [INFO] [timer.py:166:stop] 0/13460, SamplesPerSec=0.43332066556615484\n",
            "training loss: 2.916969883441925\n",
            "[2021-03-03 01:53:13,842] [INFO] [timer.py:166:stop] 0/13480, SamplesPerSec=0.43327071518039073\n",
            "training loss: 2.9762036740779876\n",
            "[2021-03-03 01:54:00,879] [INFO] [timer.py:166:stop] 0/13500, SamplesPerSec=0.4332585640606743\n",
            "training loss: 3.0500648021698\n",
            "[2021-03-03 01:54:49,469] [INFO] [timer.py:166:stop] 0/13520, SamplesPerSec=0.4332248791947994\n",
            "training loss: 3.00742444396019\n",
            "[2021-03-03 01:55:34,702] [INFO] [timer.py:166:stop] 0/13540, SamplesPerSec=0.43323785123249564\n",
            "[2021-03-03 01:56:01,482] [INFO] [logging.py:60:log_dist] [Rank 0] step=5360, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9893004179000853\n",
            "[2021-03-03 01:56:24,092] [INFO] [timer.py:166:stop] 0/13560, SamplesPerSec=0.4331932272716124\n",
            "training loss: 2.7586300373077393\n",
            "[2021-03-03 01:57:10,474] [INFO] [timer.py:166:stop] 0/13580, SamplesPerSec=0.4331903076925519\n",
            "training loss: 3.0777324378490447\n",
            "[2021-03-03 01:57:52,669] [INFO] [timer.py:166:stop] 0/13600, SamplesPerSec=0.4332452029362641\n",
            "training loss: 3.0406127333641053\n",
            "[2021-03-03 01:58:38,326] [INFO] [timer.py:166:stop] 0/13620, SamplesPerSec=0.43325220310085344\n",
            "training loss: 2.939715141057968\n",
            "[2021-03-03 01:59:24,793] [INFO] [timer.py:166:stop] 0/13640, SamplesPerSec=0.4332480466349306\n",
            "training loss: 3.180708384513855\n",
            "[2021-03-03 02:00:07,421] [INFO] [timer.py:166:stop] 0/13660, SamplesPerSec=0.4332966638828953\n",
            "training loss: 2.7756078004837037\n",
            "[2021-03-03 02:00:54,109] [INFO] [timer.py:166:stop] 0/13680, SamplesPerSec=0.4332894264801029\n",
            "training loss: 2.90426185131073\n",
            "[2021-03-03 02:01:43,927] [INFO] [timer.py:166:stop] 0/13700, SamplesPerSec=0.4332393059343795\n",
            "[2021-03-03 02:02:10,496] [INFO] [logging.py:60:log_dist] [Rank 0] step=5380, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.204413104057312\n",
            "[2021-03-03 02:02:31,357] [INFO] [timer.py:166:stop] 0/13720, SamplesPerSec=0.433222015280273\n",
            "training loss: 2.8888836026191713\n",
            "\n",
            " validation loss: 3.0107494562864305 \n",
            "\n",
            "BLEU metric: 0.05298815094341968\n",
            "Valid Structure Metric: 0.9712041884816754\n",
            "[2021-03-03 02:04:21,778] [INFO] [timer.py:166:stop] 0/13740, SamplesPerSec=0.4332129122792756\n",
            "training loss: 3.133008688688278\n",
            "[2021-03-03 02:05:07,550] [INFO] [timer.py:166:stop] 0/13760, SamplesPerSec=0.43321833432689794\n",
            "training loss: 2.8503546118736267\n",
            "[2021-03-03 02:05:50,253] [INFO] [timer.py:166:stop] 0/13780, SamplesPerSec=0.433265552277532\n",
            "training loss: 3.0047802567481994\n",
            "[2021-03-03 02:06:33,710] [INFO] [timer.py:166:stop] 0/13800, SamplesPerSec=0.4333023725025923\n",
            "training loss: 3.017164385318756\n",
            "[2021-03-03 02:07:21,253] [INFO] [timer.py:166:stop] 0/13820, SamplesPerSec=0.4332835825499968\n",
            "training loss: 2.787059634923935\n",
            "[2021-03-03 02:08:06,205] [INFO] [timer.py:166:stop] 0/13840, SamplesPerSec=0.4332999965772462\n",
            "training loss: 2.898306357860565\n",
            "[2021-03-03 02:08:54,705] [INFO] [timer.py:166:stop] 0/13860, SamplesPerSec=0.4332682886666864\n",
            "[2021-03-03 02:09:24,540] [INFO] [logging.py:60:log_dist] [Rank 0] step=5400, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.845831698179245\n",
            "[2021-03-03 02:09:41,331] [INFO] [timer.py:166:stop] 0/13880, SamplesPerSec=0.4332620284819826\n",
            "training loss: 2.980862098932266\n",
            "[2021-03-03 02:10:24,388] [INFO] [timer.py:166:stop] 0/13900, SamplesPerSec=0.43330399500450156\n",
            "training loss: 2.7794608175754547\n",
            "[2021-03-03 02:11:13,163] [INFO] [timer.py:166:stop] 0/13920, SamplesPerSec=0.4332687252772336\n",
            "training loss: 3.051682150363922\n",
            "[2021-03-03 02:12:02,037] [INFO] [timer.py:166:stop] 0/13940, SamplesPerSec=0.43323220442937355\n",
            "training loss: 3.0272844195365907\n",
            "[2021-03-03 02:12:46,197] [INFO] [timer.py:166:stop] 0/13960, SamplesPerSec=0.43325920046984184\n",
            "training loss: 2.9636702656745912\n",
            "[2021-03-03 02:13:32,062] [INFO] [timer.py:166:stop] 0/13980, SamplesPerSec=0.4332632214151658\n",
            "training loss: 2.9156704425811766\n",
            "[2021-03-03 02:14:16,127] [INFO] [timer.py:166:stop] 0/14000, SamplesPerSec=0.4332913725373334\n",
            "training loss: 3.0090149521827696\n",
            "[2021-03-03 02:15:01,862] [INFO] [timer.py:166:stop] 0/14020, SamplesPerSec=0.43329706222803255\n",
            "[2021-03-03 02:15:32,416] [INFO] [logging.py:60:log_dist] [Rank 0] step=5420, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.9964797616004946\n",
            "[2021-03-03 02:15:34,799] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/vanilla_performer/chords/latest_ckpt/mp_rank_00_model_states.pt\n",
            "\n",
            "5-43370-3.0107494562864305\n",
            "\n",
            "[2021-03-03 02:15:59,004] [INFO] [timer.py:166:stop] 0/14040, SamplesPerSec=0.4332450210039767\n",
            "training loss: 2.926615279912949\n",
            "[2021-03-03 02:16:43,462] [INFO] [timer.py:166:stop] 0/14060, SamplesPerSec=0.4332678260724902\n",
            "training loss: 2.754979646205902\n",
            "[2021-03-03 02:17:28,761] [INFO] [timer.py:166:stop] 0/14080, SamplesPerSec=0.43327935177500737\n",
            "training loss: 3.033014398813248\n",
            "[2021-03-03 02:18:14,668] [INFO] [timer.py:166:stop] 0/14100, SamplesPerSec=0.43328274572051245\n",
            "training loss: 3.099722421169281\n",
            "[2021-03-03 02:19:00,290] [INFO] [timer.py:166:stop] 0/14120, SamplesPerSec=0.43328992690812684\n",
            "training loss: 2.931558871269226\n",
            "\n",
            " validation loss: 2.9560177087783814 \n",
            "\n",
            "[2021-03-03 02:19:55,762] [INFO] [timer.py:166:stop] 0/14140, SamplesPerSec=0.4333033776547651\n",
            "training loss: 2.812914949655533\n",
            "[2021-03-03 02:20:43,068] [INFO] [timer.py:166:stop] 0/14160, SamplesPerSec=0.4332881876574545\n",
            "training loss: 3.0763571858406067\n",
            "[2021-03-03 02:21:28,930] [INFO] [timer.py:166:stop] 0/14180, SamplesPerSec=0.4332921543628111\n",
            "[2021-03-03 02:21:55,894] [INFO] [logging.py:60:log_dist] [Rank 0] step=5440, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.001433515548706\n",
            "[2021-03-03 02:22:12,855] [INFO] [timer.py:166:stop] 0/14200, SamplesPerSec=0.4333217111409972\n",
            "training loss: 2.9047843873500825\n",
            "[2021-03-03 02:22:57,430] [INFO] [timer.py:166:stop] 0/14220, SamplesPerSec=0.4333426209194782\n",
            "training loss: 3.061545544862747\n",
            "[2021-03-03 02:23:46,491] [INFO] [timer.py:166:stop] 0/14240, SamplesPerSec=0.4333042968397239\n",
            "training loss: 2.894186627864838\n",
            "[2021-03-03 02:24:32,298] [INFO] [timer.py:166:stop] 0/14260, SamplesPerSec=0.4333089447834614\n",
            "training loss: 3.044828701019287\n",
            "[2021-03-03 02:25:16,271] [INFO] [timer.py:166:stop] 0/14280, SamplesPerSec=0.43333768863876737\n",
            "training loss: 2.7610636234283445\n",
            "[2021-03-03 02:26:02,675] [INFO] [timer.py:166:stop] 0/14300, SamplesPerSec=0.4333344295923557\n",
            "training loss: 3.135227286815643\n",
            "[2021-03-03 02:26:50,077] [INFO] [timer.py:166:stop] 0/14320, SamplesPerSec=0.4333180970097571\n",
            "training loss: 3.092854160070419\n",
            "BLEU metric: 0.02188604240069889\n",
            "Valid Structure Metric: 0.9842931937172775\n",
            "[2021-03-03 02:28:16,831] [INFO] [timer.py:166:stop] 0/14340, SamplesPerSec=0.4333652271800549\n",
            "[2021-03-03 02:28:44,304] [INFO] [logging.py:60:log_dist] [Rank 0] step=5460, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.008747696876526\n",
            "[2021-03-03 02:29:02,021] [INFO] [timer.py:166:stop] 0/14360, SamplesPerSec=0.43337782467670705\n",
            "training loss: 3.0232859790325164\n",
            "[2021-03-03 02:29:45,963] [INFO] [timer.py:166:stop] 0/14380, SamplesPerSec=0.4334066988126527\n",
            "training loss: 2.9648165702819824\n",
            "[2021-03-03 02:30:31,721] [INFO] [timer.py:166:stop] 0/14400, SamplesPerSec=0.4334117933133814\n",
            "training loss: 3.0339123904705048\n",
            "[2021-03-03 02:31:19,579] [INFO] [timer.py:166:stop] 0/14420, SamplesPerSec=0.4333895137110231\n",
            "training loss: 3.009841352701187\n",
            "[2021-03-03 02:32:13,944] [INFO] [timer.py:166:stop] 0/14440, SamplesPerSec=0.43328266519726527\n",
            "training loss: 3.0100389659404754\n",
            "[2021-03-03 02:32:58,661] [INFO] [timer.py:166:stop] 0/14460, SamplesPerSec=0.43330143562484635\n",
            "training loss: 3.146567225456238\n",
            "[2021-03-03 02:33:40,609] [INFO] [timer.py:166:stop] 0/14480, SamplesPerSec=0.43335606014966477\n",
            "training loss: 3.0234509825706484\n",
            "[2021-03-03 02:34:31,841] [INFO] [timer.py:166:stop] 0/14500, SamplesPerSec=0.4332902835764261\n",
            "[2021-03-03 02:34:58,590] [INFO] [logging.py:60:log_dist] [Rank 0] step=5480, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.0493893921375275\n",
            "[2021-03-03 02:35:18,114] [INFO] [timer.py:166:stop] 0/14520, SamplesPerSec=0.4332888377035863\n",
            "training loss: 3.1275611102581022\n",
            "\n",
            " validation loss: 2.9836942493915557 \n",
            "\n",
            "[2021-03-03 02:36:19,644] [INFO] [timer.py:166:stop] 0/14540, SamplesPerSec=0.43322899025450257\n",
            "training loss: 2.911677724123001\n",
            "[2021-03-03 02:37:08,500] [INFO] [timer.py:166:stop] 0/14560, SamplesPerSec=0.4331943299887898\n",
            "training loss: 3.0939343094825746\n",
            "[2021-03-03 02:37:54,356] [INFO] [timer.py:166:stop] 0/14580, SamplesPerSec=0.43319839545886957\n",
            "training loss: 2.791706258058548\n",
            "[2021-03-03 02:38:36,762] [INFO] [timer.py:166:stop] 0/14600, SamplesPerSec=0.4332467950343966\n",
            "training loss: 3.0315256118774414\n",
            "[2021-03-03 02:39:23,127] [INFO] [timer.py:166:stop] 0/14620, SamplesPerSec=0.43324423163896686\n",
            "training loss: 2.8888624131679537\n",
            "[2021-03-03 02:40:09,953] [INFO] [timer.py:166:stop] 0/14640, SamplesPerSec=0.4332357645486864\n",
            "training loss: 2.873074918985367\n",
            "[2021-03-03 02:40:55,318] [INFO] [timer.py:166:stop] 0/14660, SamplesPerSec=0.4332460347879699\n",
            "[2021-03-03 02:41:21,191] [INFO] [logging.py:60:log_dist] [Rank 0] step=5500, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 2.687964141368866\n",
            "[2021-03-03 02:41:43,195] [INFO] [timer.py:166:stop] 0/14680, SamplesPerSec=0.43322414365229334\n",
            "training loss: 3.237293708324432\n",
            "[2021-03-03 02:42:31,962] [INFO] [timer.py:166:stop] 0/14700, SamplesPerSec=0.43319095477495706\n",
            "training loss: 3.1916815400123597\n",
            "[2021-03-03 02:43:15,669] [INFO] [timer.py:166:stop] 0/14720, SamplesPerSec=0.4332223809731634\n",
            "training loss: 2.994767314195633\n",
            "[2021-03-03 02:44:03,617] [INFO] [timer.py:166:stop] 0/14740, SamplesPerSec=0.4331997184846505\n",
            "training loss: 3.0536633253097536\n",
            "[2021-03-03 02:44:46,822] [INFO] [timer.py:166:stop] 0/14760, SamplesPerSec=0.4332374334359814\n",
            "training loss: 2.9162844896316527\n",
            "[2021-03-03 02:45:29,995] [INFO] [timer.py:166:stop] 0/14780, SamplesPerSec=0.43327545924654337\n",
            "training loss: 3.0733149528503416\n",
            "[2021-03-03 02:46:23,738] [INFO] [timer.py:166:stop] 0/14800, SamplesPerSec=0.433179315646319\n",
            "training loss: 2.9718204975128173\n",
            "[2021-03-03 02:47:09,585] [INFO] [timer.py:166:stop] 0/14820, SamplesPerSec=0.4331834489362623\n",
            "[2021-03-03 02:47:36,425] [INFO] [logging.py:60:log_dist] [Rank 0] step=5520, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 3.2287234127521516\n",
            "[2021-03-03 02:47:56,773] [INFO] [timer.py:166:stop] 0/14840, SamplesPerSec=0.43317059979559225\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}