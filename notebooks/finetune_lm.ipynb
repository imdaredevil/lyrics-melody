{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_lm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXtfSfm0Ga7e",
        "outputId": "4025a30d-07fd-4377-bff3-684b1f843cd8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install performer-pytorch --upgrade\n",
        "!pip install deepspeed\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.3.0\n",
            "Collecting performer-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/46/ab/a248fc127146097e889eb68edb5e82c1bb6d73ae432194cecb2e28f6cd2b/performer_pytorch-0.15.0-py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/86/f1df73868c1c433a9184d94e86cdd970951ecf14d8b556b41302febb9a12/local_attention-1.2.2-py3-none-any.whl\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (1.19.5)\n",
            "Building wheels for collected packages: axial-positional-embedding\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp36-none-any.whl size=2904 sha256=ae878934b587682878ae1a6f729437af40607c4ebe3061dac4536bcefebbc8dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built axial-positional-embedding\n",
            "Installing collected packages: local-attention, einops, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 local-attention-1.2.2 performer-pytorch-0.15.0\n",
            "Collecting deepspeed\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bd/b2b544ca1286252e9a559b1508e64d0d61af7a73b6bf6737568858128e11/deepspeed-0.3.10.tar.gz (281kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (0.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deepspeed) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 14.4MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.4.0->deepspeed) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed) (51.3.3)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.10-cp36-none-any.whl size=272627 sha256=ab86745955339a2ea834b2dd6ee9cf166b6d9521d76ed8d0447d84ee0ba21f63\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/3c/9c/39a16330874a2c55f61fe2c501e120258975d509177ffdcda7\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.10 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=1077d2762eb8e82cc0408c5e807509a2ac8affe22ac8dbc7282553d5ffe2b199\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqSleDzsG7BI",
        "outputId": "2efe5ac7-006a-4974-ca4f-d2f8726baef1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jan 30 01:12:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cmLf2tXHJR6",
        "outputId": "8dcb43bb-3fa5-4152-fb6f-334ffc70aa0f"
      },
      "source": [
        "%%writefile ds_config.json\n",
        "\n",
        "{\n",
        "  \"train_batch_size\": 64,\n",
        "  \"gradient_accumulation_steps\": 8,\n",
        "  \"steps_per_print\": 40,\n",
        "  \"gradient_clipping\": 0.5,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"Adam\",\n",
        "    \"params\": {\n",
        "      \"lr\": 0.001,\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.98\n",
        "      ],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\" : 0.1\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 0.001,\n",
        "      \"warmup_num_steps\": 100\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ds_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zKioANHSzw",
        "outputId": "a8428816-d7f2-437d-cc2e-8bf6deb5ee94"
      },
      "source": [
        "%%writefile finetune_lm.py\n",
        "\n",
        "import deepspeed\n",
        "from performer_pytorch import PerformerLM, AutoregressiveWrapper\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import re\n",
        "from itertools import cycle\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import AutoTokenizer\n",
        "from statistics import mean\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Finetune LM on lyrics')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--pretrained-model', '-pm', type=str,\n",
        "                        help='Pretrained huggingface model to load')\n",
        "\n",
        "    parser.add_argument('--tokenizer', '-tok', type=str,\n",
        "                        help='Hugginface tokenizer to use')\n",
        "\n",
        "    parser.add_argument('--max-seq-len', '-msl', type=int, default=1024,\n",
        "                        help='Max sequence length')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, required=True,\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--epochs', '-e', type=int, default=20,\n",
        "                        help='Number of epochs')\n",
        "    \n",
        "    parser.add_argument('--validate-every', '-ve', type=int, default=200,\n",
        "                        help='Validate every n batches')\n",
        "    \n",
        "    parser.add_argument('--generate-every', '-ge', type=int, default=400,\n",
        "                        help='Generate every n batches')\n",
        "\n",
        "    parser.add_argument('--print-training-loss-every', '-ptle', type=int, default=20,\n",
        "                        help='It will average training loss and print it every n steps')\n",
        "\n",
        "    parser.add_argument('--validate-size', '-vs', type=int, default=40,\n",
        "                        help='Will calculate average of validation loss for n batches')\n",
        "\n",
        "    parser.add_argument('--validate-batch-size', '-vss', type=int, default=1,\n",
        "                        help='Batch size for validation dataset')\n",
        "\n",
        "    parser.add_argument('--checkpoints-per-epoch', '-cpp', type=int, default=3,\n",
        "                        help='How many checkpoints to keep per epoch')\n",
        "    \n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank passed from distributed launcher')\n",
        "    \n",
        "    parser = deepspeed.add_config_arguments(parser)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, dataset_file, tokenizer, max_length=1024):\n",
        "        super().__init__()\n",
        "        brackets = re.compile(r'\\[.*?\\]|\\(.*?\\)')\n",
        "        clean = re.compile(r\"-|[^A-Za-z \\n']+\")\n",
        "        space = re.compile(r\"\\s+\")\n",
        "        \n",
        "        df = pd.read_parquet(dataset_file)\n",
        "        lyrics = [space.sub(\" \", clean.sub(lambda match: \" \" if match.group(0) == \"-\" else \"\", brackets.sub(\"\", df['Lyric'][idx]))).lstrip() \\\n",
        "                                                                     for idx in range(len(df)) if df['Idiom'][idx] == 'ENGLISH']\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, use_fast=True)\n",
        "\n",
        "        encoded_lyrics = self.batch_encode(lyrics)\n",
        "\n",
        "        self.max_seq_len = max([len(l) for l in encoded_lyrics])\n",
        "        self.mean_seq_len = mean([len(l) for l in encoded_lyrics])\n",
        "\n",
        "        self.lyrics = [torch.tensor([self.tokenizer.bos_token_id] + l[:max_length - 2] + [self.tokenizer.eos_token_id]) for l in encoded_lyrics]\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.lyrics[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lyrics)\n",
        "\n",
        "    def batch_encode(self, sequences):\n",
        "        return self.tokenizer.batch_encode_plus(sequences)['input_ids']\n",
        "\n",
        "    def batch_decode(self, sequences, masks=None):\n",
        "        if masks is None:\n",
        "            return self.tokenizer.batch_decode(sequences)\n",
        "\n",
        "        batch = []\n",
        "        for sequence, mask in zip(sequences, masks):\n",
        "            size = len(sequence)\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "            batch.append(self.tokenizer.decode(sequence[:true_size]))\n",
        "        return batch\n",
        "\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        data = batch[0].view(1, -1)\n",
        "        masks = torch.ones_like(data).bool()\n",
        "        return (data, masks)\n",
        "\n",
        "    lengths = [seq.size(0) for seq in batch]\n",
        "    max_length = max(lengths)\n",
        "    masks = torch.arange(max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(lengths).view(-1, 1)\n",
        "    padded_data = torch.zeros(batch_size, max_length)\n",
        "    for i, l in enumerate(lengths):\n",
        "        padded_data[i, :l] = batch[i]\n",
        "\n",
        "    return (padded_data.long(), masks)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = LyricsDataset(dataset_file=args.dataset_file,\n",
        "                            tokenizer=args.tokenizer,\n",
        "                            max_length=args.max_seq_len)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_log_dir = os.path.join(args.save_dir, 'train')\n",
        "    val_log_dir = os.path.join(args.save_dir, 'val')\n",
        "    Path(train_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(val_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    writer_train = SummaryWriter(log_dir=train_log_dir)\n",
        "    writer_val = SummaryWriter(log_dir=val_log_dir)\n",
        "    \n",
        "    model = PerformerLM(\n",
        "            dim = 768,\n",
        "            heads = 12,\n",
        "            depth = 6,\n",
        "            num_tokens = len(dataset.tokenizer),\n",
        "            max_seq_len = args.max_seq_len,\n",
        "            emb_dropout = 0.1,\n",
        "            ff_dropout = 0.1,\n",
        "            attn_dropout = 0.1,\n",
        "            tie_embed = True,\n",
        "            reversible = True,\n",
        "            causal = True\n",
        "    )\n",
        "\n",
        "    if args.pretrained_model:\n",
        "        model.load_state_dict(torch.load(args.pretrained_model), strict=False)\n",
        "        print(\"Loaded pretrained model: {}\".format(args.pretrained_model))\n",
        "\n",
        "    model = AutoregressiveWrapper(model).to(device)\n",
        "\n",
        "    model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=model, model_parameters=model.parameters(),  training_data=train_dataset, collate_fn=collate_fn_zero_pad)\n",
        "    device = model_engine.local_rank\n",
        "\n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=args.validate_batch_size, shuffle=True, collate_fn=collate_fn_zero_pad)\n",
        "    val_loader = cycle(val_loader_)\n",
        "\n",
        "    num_batches = (len(train_dataset) + trainloader.batch_size - 1) // trainloader.batch_size\n",
        "\n",
        "    save_every = num_batches // args.checkpoints_per_epoch\n",
        "    save_at = 0\n",
        "    saving_steps = []\n",
        "    for _ in range(args.checkpoints_per_epoch - 1):\n",
        "        save_at += save_every\n",
        "        saving_steps.append(save_at)\n",
        "    saving_steps.append(num_batches - 1)\n",
        "\n",
        "    print(\"\\n\", \"Dataset maximum sequence length: {} Dataset mean sequence length: {}\".format(dataset.max_seq_len, dataset.mean_seq_len, \"\\n\"))\n",
        "    print(\"\\n\", \"Train Dataset - size: {}, batches: {}\".format(len(train_dataset), num_batches), \"\\n\")\n",
        "    print(\"\\n\", \"Validate Dataset - size: {}, batches: {}\".format(len(val_dataset), len(val_loader_)), \"\\n\")\n",
        "\n",
        "    checkpoint_name, client_state = model_engine.load_checkpoint(args.save_dir, load_module_strict=False)\n",
        "    # checkpoint_name = None\n",
        "\n",
        "    if checkpoint_name is not None:\n",
        "        print(\"\\nLoaded checkpoint: {}\\n\".format(checkpoint_name))        \n",
        "        i = client_state['i']\n",
        "        i += 1\n",
        "        epoch, step = divmod(i, num_batches)\n",
        "        print(\"Epoch: {}, step: {}, i: {}\".format(epoch, step, i))\n",
        "        if step == 0:\n",
        "            print(\"Starting next epoch...\")\n",
        "            rng = torch.get_rng_state()\n",
        "            trainloader = iter(trainloader)\n",
        "        else:\n",
        "            rng = torch.load(os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "            torch.set_rng_state(rng)\n",
        "            trainloader = iter(trainloader)\n",
        "            print(\"Advancing dataloader...\")\n",
        "            for _ in range(step):\n",
        "                next(trainloader)\n",
        "    else:\n",
        "        print(\"\\nNo checkpoint found, training from scratch\\n\")\n",
        "        i = 0\n",
        "        step = 0\n",
        "        epoch = 0\n",
        "        rng = torch.get_rng_state()\n",
        "        trainloader = iter(trainloader)\n",
        "\n",
        "\n",
        "    for e in range(args.epochs - epoch):\n",
        "        running_loss = 0\n",
        "        running_loss_steps = 0\n",
        "        print(\"EPOCH: {}\".format(e + epoch))\n",
        "        while True:\n",
        "            try:\n",
        "                data = next(trainloader)\n",
        "            except StopIteration:\n",
        "                step = 0\n",
        "                rng = torch.get_rng_state()\n",
        "                trainloader = iter(trainloader)\n",
        "                break\n",
        "\n",
        "            model_engine.train()\n",
        "            lyrics, mask = data\n",
        "            loss = model_engine(lyrics.to(device), mask=mask.to(device), return_loss=True)\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            running_loss_steps += 1\n",
        "            if running_loss_steps == args.print_training_loss_every or step == 0:\n",
        "                avg_loss = running_loss / running_loss_steps\n",
        "                print(\"training loss: {}\".format(avg_loss))\n",
        "                writer_train.add_scalar(\"Loss\", avg_loss, i)\n",
        "                writer_train.flush()\n",
        "                running_loss = 0\n",
        "                running_loss_steps = 0\n",
        "\n",
        "            if step % args.validate_every == 0:\n",
        "                model_engine.eval()\n",
        "                with torch.no_grad():\n",
        "                    running_eval_loss = 0\n",
        "                    for _ in range(args.validate_size):\n",
        "                        lyrics, mask = next(val_loader)\n",
        "                        loss = model_engine(lyrics.to(device), mask=mask.to(device), return_loss=True)\n",
        "                        running_eval_loss += loss.item()\n",
        "                    avg_eval_loss = running_eval_loss / args.validate_size\n",
        "                    print('\\n', f'validation loss: {avg_eval_loss}', '\\n')\n",
        "                    writer_val.add_scalar(\"Loss\", avg_eval_loss, i)\n",
        "                    writer_val.flush()\n",
        "                    running_eval_loss = 0\n",
        "\n",
        "            if step % args.generate_every == 0:\n",
        "                # <bos> token\n",
        "                initial = torch.full((1,1), dataset.tokenizer.bos_token_id).long()\n",
        "\n",
        "                outs = [model_engine.module.generate(initial.to(device), seq_len=args.max_seq_len//2, eos_token=dataset.tokenizer.eos_token_id)[0] for _ in range(4)]\n",
        "                decoded_outs = '\\n'.join(dataset.batch_decode(outs))\n",
        "                print(decoded_outs)\n",
        "\n",
        "                with open(os.path.join(args.save_dir, 'outputs.txt'), 'a') as f:\n",
        "                    f.write(decoded_outs + '\\n\\n')\n",
        "\n",
        "            if step in saving_steps:\n",
        "                loss_to_ckpt = avg_eval_loss if avg_eval_loss is not None else loss.item()\n",
        "                ckpt_id = \"{}-{}-{}\".format(e + epoch, i, loss_to_ckpt)\n",
        "                model_engine.save_checkpoint(args.save_dir, tag=ckpt_id, client_state = {'i': i, 'step': step, 'epoch': e + epoch})\n",
        "                torch.save(rng, os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "\n",
        "            i += 1\n",
        "            step += 1\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting finetune_lm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FhiaZbvAhMU",
        "outputId": "8183426c-82f4-4c17-a98e-e6748149864b"
      },
      "source": [
        "%env TOKENIZERS_PARALLELISM=false"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: TOKENIZERS_PARALLELISM=false\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u9ajI_PHZeS",
        "outputId": "48fe3dd4-3162-4105-aa58-06f76eb87576"
      },
      "source": [
        "!deepspeed finetune_lm.py -df drive/MyDrive/language_model/onlylyrics.parquet -tok distilgpt2 -sd drive/MyDrive/language_model/pretrained -ve 400 -ge 800 -cpp 6 --deepspeed --deepspeed_config ds_config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-01-30 01:33:23,550] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-01-30 01:33:23,583] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 finetune_lm.py -df drive/MyDrive/language_model/onlylyrics.parquet -tok distilgpt2 -sd drive/MyDrive/language_model/pretrained -ve 400 -ge 800 -cpp 6 --deepspeed --deepspeed_config ds_config.json\n",
            "[2021-01-30 01:33:24,396] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.8.3\n",
            "[2021-01-30 01:33:24,397] [INFO] [launch.py:78:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-01-30 01:33:24,397] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-01-30 01:33:24,397] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-01-30 01:33:24,397] [INFO] [launch.py:100:main] dist_world_size=1\n",
            "[2021-01-30 01:33:24,397] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 1024). Running this sequence through the model will result in indexing errors\n",
            "2021-01-30 01:34:51.461349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "[2021-01-30 01:34:57,326] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.10, git-hash=unknown, git-branch=unknown\n",
            "[2021-01-30 01:34:57,326] [INFO] [distributed.py:40:init_distributed] Initializing torch distributed with backend: nccl\n",
            "[2021-01-30 01:34:57,391] [INFO] [engine.py:72:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.5751082897186279 seconds\n",
            "[2021-01-30 01:34:59,258] [INFO] [engine.py:518:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2021-01-30 01:34:59,258] [INFO] [engine.py:521:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.9, 0.98]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "[2021-01-30 01:34:59,258] [INFO] [engine.py:551:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.9, 0.98]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "[2021-01-30 01:34:59,258] [INFO] [engine.py:382:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-01-30 01:34:59,258] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fdc51b3ffd0>\n",
            "[2021-01-30 01:34:59,259] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:705:print] DeepSpeedEngine configuration:\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7fdc51b3e4e0>\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   allreduce_always_fp32 ........ False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   amp_enabled .................. False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   amp_params ................... False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   disable_allgather ............ False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   dump_state ................... False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   dynamic_loss_scale_args ...... None\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   elasticity_enabled ........... False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   fp16_enabled ................. False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   global_rank .................. 0\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   gradient_accumulation_steps .. 8\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   gradient_clipping ............ 0.5\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   loss_scale ................... 0\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   memory_breakdown ............. False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   optimizer_name ............... adam\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.98], 'eps': 1e-08, 'weight_decay': 0.1, 'adam_w_mode': True}\n",
            "[2021-01-30 01:34:59,259] [INFO] [config.py:709:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   pld_enabled .................. False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   pld_params ................... False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   prescale_gradients ........... False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   scheduler_name ............... WarmupLR\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 100}\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   sparse_attention ............. None\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   sparse_gradients_enabled ..... False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   steps_per_print .............. 40\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   tensorboard_enabled .......... False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   tensorboard_output_path ...... \n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   train_batch_size ............. 64\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   train_micro_batch_size_per_gpu  8\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   wall_clock_breakdown ......... False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   world_size ................... 1\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   zero_allow_untested_optimizer  False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   zero_config .................. {\n",
            "    \"allgather_bucket_size\": 500000000,\n",
            "    \"allgather_partitions\": true,\n",
            "    \"contiguous_gradients\": false,\n",
            "    \"cpu_offload\": false,\n",
            "    \"elastic_checkpoint\": true,\n",
            "    \"load_from_fp32_weights\": true,\n",
            "    \"overlap_comm\": false,\n",
            "    \"reduce_bucket_size\": 500000000,\n",
            "    \"reduce_scatter\": true,\n",
            "    \"stage\": 0\n",
            "}\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   zero_enabled ................. False\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:709:print]   zero_optimization_stage ...... 0\n",
            "[2021-01-30 01:34:59,260] [INFO] [config.py:716:print]   json = {\n",
            "    \"gradient_accumulation_steps\":8,\n",
            "    \"gradient_clipping\":0.5,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"adam_w_mode\":true,\n",
            "            \"betas\":[\n",
            "                0.9,\n",
            "                0.98\n",
            "            ],\n",
            "            \"eps\":1e-08,\n",
            "            \"lr\":0.001,\n",
            "            \"weight_decay\":0.1\n",
            "        },\n",
            "        \"type\":\"Adam\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":0.001,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":100\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"steps_per_print\":40,\n",
            "    \"train_batch_size\":64\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.5811176300048828 seconds\n",
            "\n",
            " Dataset maximum sequence length: 3531 Dataset mean sequence length: 311.65233649747654\n",
            "\n",
            " Train Dataset - size: 103250, batches: 12907 \n",
            "\n",
            "\n",
            " Validate Dataset - size: 11473, batches: 11473 \n",
            "\n",
            "[2021-01-30 01:34:59,845] [INFO] [engine.py:1284:_load_checkpoint] rank: 0 loading checkpoint: drive/MyDrive/language_model/pretrained/0-12906-4.439273464679718/mp_rank_00_model_states.pt\n",
            "\n",
            "Loaded checkpoint: drive/MyDrive/language_model/pretrained/0-12906-4.439273464679718/mp_rank_00_model_states.pt\n",
            "\n",
            "Epoch: 1, step: 0, i: 12907\n",
            "Starting next epoch...\n",
            "EPOCH: 1\n",
            "training loss: 4.637027263641357\n",
            "\n",
            " validation loss: 4.55166996717453 \n",
            "\n",
            "There's like a long time It's having a long time that's such a long way Ive been doing hell A long day And when the only a long time we save all took half longer in a long long time When I paid long since a long long way we have long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long T long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long long the long long long long long long long long long long long long long long long long long long long\n",
            "She wrote to ring the Rodeo she wrote and hairy guts from hollow the flower wide Well called New York fields of the names spark the flowers of grass The fabricale of fruits to the field of humour for The fascist green grass was swallowed a sack She spoke They filled with green roses Sweet colorsflowers they smiled and nesthered Her bubblecorn and day Well prancin' woody flower well she garden peardy green grass rosemary She gave more than sleepy milk field telephone wire c clown But them bendle pie and a gold And sheling the barn and rock and hair she wraps us for things And just seen that egg field Today was the bloom bloom and wingbred Step home while she stumbled Un greener bloom as bright above the flowery carrots andarrow And looking to rock bloom and stre stands lovely flower garden she that green grass grass grow green laid to win and just rose rose flower I swear that feather bird feathers lays down the flower breeze and spice rose grassflower flower rose green flower grows green green grow green and to me flower birds and bright So the flower Farm greener grass green rose green grass and greener green flowers grow elseoved leaf around yellow brown hair growth and flower That boy my flower flower grass grow grow a lady lovely flower bloom bloom bloom may<|endoftext|>\n",
            "Rake Lady for all over Ivegas manizer Full of desem for Demian Know Who have mansions within And Im naw Mother Nature is full of Wyclefkers barkiniti and Tonto whats slingerie poser of me clique mhairs im Lambo Talenling so I I'm fucking I swines Ben Mack krimpin for fun Pussy girls ladies in a J Zwisprich Gangstgrickets's jaw a hundred thousand miles near death Anyth soundin shit to fight Aint no hymnys president through the floor Slips Keats and shook meek smokin I'm a grill Middle east coastbodies gonna beat grew up hills I wanted to the coat full of water in a hundredres niggas swans are lips sugar done nias So I want me with mac Busser me Sneaking crack I'm an inch my chains Are you Rappin I bowing water Wanna see fannie mil below me tell me from Jones cracked deuce me I can see me jumped freaked up in the front these pages I can ugly tired of Compton Lord I watched you laugh The streets of yia Im say Is at the west street shres every inch inch of leather coat and the m These waltzer than a prolly mixtapes and gin Spoe Young Money cruise ry and progress knowledge like a movie rims and chains throw rocks And the guns Thre fresh ounce bubble the rhymes wit a niggaz up smoke it's Texas niggaz mixing Indian leather but its heat I stay them empty cabls too money every bitch I'm with these cases hoes Ones out to keep grinding word as diamonds that's run in roks in to raise Hoes all yerje Doin Hoes wanna live in here Money walking the highway when I got it Grind nights I'ma get the porches up before the door Im in your bare and bones sick of my strap watch me like choppers self reach I'm shaw noun' me The grease my rims Turn around Beat you Bentleys I start fuckin' hood fuck all of souls And if its thrills I invite me half fucked every breath I can ride to breathe this without a million thousand orange rib getting to live crime I runnin out faces and you say that they pull the city You in this don't gettin got bitch Y you I treatin' money and honeys bitches Y'Macein me that liquor somanisies I eat hard\n",
            "Calling Jason in flames I need you I don't need somebody I'm a trouble Can I don't believe it some secrets calling from somewhere can't mean them And I'm cold as bad and you cheat it in the martyr In this town declare Fuck what they don't be back He don't freak Who ain't mind bitch How many people tell the fuck I can't stay 'em what they say you should be I can't matter 'em <|endoftext|>\n",
            "training loss: 4.585254287719726\n",
            "[2021-01-30 01:37:19,461] [INFO] [timer.py:166:stop] 0/40, SamplesPerSec=3.2992482932862868\n",
            "training loss: 4.526574993133545\n",
            "training loss: 4.585954189300537\n",
            "[2021-01-30 01:38:55,951] [INFO] [timer.py:166:stop] 0/80, SamplesPerSec=3.308115217047626\n",
            "training loss: 4.479627776145935\n",
            "training loss: 4.5940508365631105\n",
            "[2021-01-30 01:40:40,389] [INFO] [timer.py:166:stop] 0/120, SamplesPerSec=3.2211967515913766\n",
            "training loss: 4.727700543403626\n",
            "training loss: 4.5929865598678585\n",
            "[2021-01-30 01:42:27,332] [INFO] [timer.py:166:stop] 0/160, SamplesPerSec=3.160036250387236\n",
            "training loss: 4.572310376167297\n",
            "training loss: 4.4765393257141115\n",
            "[2021-01-30 01:44:07,817] [INFO] [timer.py:166:stop] 0/200, SamplesPerSec=3.16499868945659\n",
            "training loss: 4.633006858825683\n",
            "[2021-01-30 01:44:50,283] [INFO] [logging.py:60:log_dist] [Rank 0] step=1640, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.701248550415039\n",
            "[2021-01-30 01:45:50,614] [INFO] [timer.py:166:stop] 0/240, SamplesPerSec=3.15615626228816\n",
            "training loss: 4.477984154224396\n",
            "training loss: 4.6484497547149655\n",
            "[2021-01-30 01:47:36,490] [INFO] [timer.py:166:stop] 0/280, SamplesPerSec=3.1362163247682404\n",
            "training loss: 4.495615077018738\n",
            "training loss: 4.565322375297546\n",
            "[2021-01-30 01:49:13,377] [INFO] [timer.py:166:stop] 0/320, SamplesPerSec=3.1562638030495602\n",
            "training loss: 4.539012241363525\n",
            "training loss: 4.523388123512268\n",
            "[2021-01-30 01:50:48,201] [INFO] [timer.py:166:stop] 0/360, SamplesPerSec=3.1792720604730045\n",
            "training loss: 4.523407447338104\n",
            "training loss: 4.5610299706459045\n",
            "[2021-01-30 01:52:29,199] [INFO] [timer.py:166:stop] 0/400, SamplesPerSec=3.178193387947553\n",
            "training loss: 4.553712236881256\n",
            "\n",
            " validation loss: 4.663652247190475 \n",
            "\n",
            "training loss: 4.520313787460327\n",
            "[2021-01-30 01:54:05,220] [INFO] [timer.py:166:stop] 0/440, SamplesPerSec=3.2015411988942226\n",
            "training loss: 4.536036944389343\n",
            "training loss: 4.5232309103012085\n",
            "[2021-01-30 01:55:35,869] [INFO] [timer.py:166:stop] 0/480, SamplesPerSec=3.22668524809267\n",
            "training loss: 4.525576543807984\n",
            "training loss: 4.67751944065094\n",
            "[2021-01-30 01:57:10,523] [INFO] [timer.py:166:stop] 0/520, SamplesPerSec=3.2380962062813174\n",
            "training loss: 4.5753450155258175\n",
            "[2021-01-30 01:57:59,420] [INFO] [logging.py:60:log_dist] [Rank 0] step=1680, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.485203540325164\n",
            "[2021-01-30 01:58:56,192] [INFO] [timer.py:166:stop] 0/560, SamplesPerSec=3.2221081062216093\n",
            "training loss: 4.528047788143158\n",
            "training loss: 4.5494094610214235\n",
            "[2021-01-30 02:00:39,586] [INFO] [timer.py:166:stop] 0/600, SamplesPerSec=3.2132898347888807\n",
            "training loss: 4.642648410797119\n",
            "training loss: 4.489765787124634\n",
            "[2021-01-30 02:02:18,608] [INFO] [timer.py:166:stop] 0/640, SamplesPerSec=3.214443352352316\n",
            "training loss: 4.497786414623261\n",
            "training loss: 4.520899057388306\n",
            "[2021-01-30 02:04:00,165] [INFO] [timer.py:166:stop] 0/680, SamplesPerSec=3.2106371013339796\n",
            "training loss: 4.530337595939637\n",
            "training loss: 4.557914066314697\n",
            "[2021-01-30 02:05:41,659] [INFO] [timer.py:166:stop] 0/720, SamplesPerSec=3.2073736214722546\n",
            "training loss: 4.5309882164001465\n",
            "training loss: 4.545700097084046\n",
            "[2021-01-30 02:07:26,375] [INFO] [timer.py:166:stop] 0/760, SamplesPerSec=3.199014529842561\n",
            "training loss: 4.642463374137878\n",
            "training loss: 4.5448445796966555\n",
            "[2021-01-30 02:09:02,740] [INFO] [timer.py:166:stop] 0/800, SamplesPerSec=3.204909872098963\n",
            "training loss: 4.537889552116394\n",
            "\n",
            " validation loss: 4.714199358224869 \n",
            "\n",
            "Put your clothes your clothes You need me together You Lovin about your rub me Gornin like some ex kissins really so lavish Make niggas let the crib Girl genie that crazy baby Make them bitches and fly so just on the way you drive you give me as well So I don't ask about to the crowd parties just like you did the other chick the crib bought a kick like the club Ay That my NikeOOK How dare never let 'bout the job Like them clothes Like a thing in the party like me freak so likeair Max's all night I taught me I don't do it seems like a chick Don't us about my crib used to clutch Push a summer flight to stay all these clubs a while Wayne's That's my chick yeah let life But girls you ny booty girls say I'm creep under my rounds Girl you like Know exactly high' you ha Now like I chose the fuck My game is money bouncin' like what I know we gon' crazy Everything's to get on my shit make you hoes like having fun I done told them other chick like she ripped like her and they sexy like Gin like Ferrari and I ain't barely drop her hot like dubs I get it tastes dons chips different in the crib for a polisk you know there's shoes Do your girlfriend baby Girl I get his girl Like we don't never text on my hair like Elvis what come models like old she like Obama high school matters like coats That's club So take it CHORUS I like that Ass like a cable Tell the club It's he sexy She like my crew she told her She told how she Best I don't my jeans my party she was like it's How she name Come on the club like a bottle She like you holler When she gon' them shoes and backseat meals but oh club We ain't grab the first as we just to fore was let her I was rims like And that like the right kids Oww but I'm Ferrari let her or bad bitches like Kobe Like time I just noticed lil' like Girl is nasty Got body like me She's like the car she come on the dudes like Ay ay like that stage show my tank top heels she bangimes I got no more about how bottles like All these bottles right Oww I like a dark Trick Who's pretend straight to my head like me like the Aww I was ever was poppin' pretend you a famous<|endoftext|>\n",
            "Wise the barrio out of fear that trouble I'll have to match This love and waiting Under the photograph it off my eyes This is crazy Is that just might be happening on This fucking old tattooed by the dark in the sun This passion has gone away This faded We call out to love At times Hello hello This evening Gives the sun hangs strange This house of our worn Taste this thing And let's nothing runs This thing I left love turned a stain change This hate on this days since I guess I will hide the door The wind blows This thing tore me You give me here I thought of gasoline<|endoftext|>\n",
            "You can see me no emotion No one all your joy puts us all my darling please Emotions that you found joy hello It only someone I see you on the crowd It's leaving our joy for long time we close your heart sometime will see it bothered loving yes it won't see my freedom of joy all over joy Dim joy since joy Unconditional love I hear ev' touch aside<|endoftext|>\n",
            "Trying times before you can't get me away I see in my head As I never let you make you push myself I push you I never gonna make you a sad I need you a fool of you go on and let's ego's gonna do not gonna let's calm This fight and never gonna get for real or never let others and on the power I'm gonna let you in you I like an animal live and beat down You wanna lose control is lost your time has I'm gonna run and cold Let's gonna let you're gonna let's gonna make you know nobody gonna fight this cage no no intention you you gonna let you try to hell I'm gonna let you to lose your body down and let but I'm gonna let you don't let all go I don't let you down I don't ya gonna let nobody gonna let you gonna let your gonna let you cause you down no plan to let it like I wanna rush you wanna let the light gonna let me tonight I already held you wanna let you I know I guarantee you down and I won't let you and let nobody gonna let you say Cause you gonna let you when you out wrong I wanna let you gonna I'm gonna let you don't let you out of nothing gonna let you gonna let me let me down gonna let you am gonna let you gonna let you gonna let you gonna let me you right but here I wanna let you gonna let I gonna let me I I I gonna let you gonna let you gonna let you gonna let you gonna let nobody nobody gonna let you gonna let you gonna let you gonna let you gonna let nobody gonna let nobody gonna let you gonna ever let you gonna let burn<|endoftext|>\n",
            "training loss: 4.534624838829041\n",
            "[2021-01-30 02:11:12,879] [INFO] [timer.py:166:stop] 0/840, SamplesPerSec=3.206281137216157\n",
            "training loss: 4.622838497161865\n",
            "[2021-01-30 02:11:47,655] [INFO] [logging.py:60:log_dist] [Rank 0] step=1720, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.604942965507507\n",
            "[2021-01-30 02:12:51,379] [INFO] [timer.py:166:stop] 0/880, SamplesPerSec=3.2081988777742536\n",
            "training loss: 4.614282703399658\n",
            "training loss: 4.611268448829651\n",
            "[2021-01-30 02:14:27,141] [INFO] [timer.py:166:stop] 0/920, SamplesPerSec=3.2137986362544626\n",
            "training loss: 4.5843600034713745\n",
            "training loss: 4.466068720817566\n",
            "[2021-01-30 02:16:08,673] [INFO] [timer.py:166:stop] 0/960, SamplesPerSec=3.2111648294654267\n",
            "training loss: 4.577068161964417\n",
            "training loss: 4.560292148590088\n",
            "[2021-01-30 02:17:45,981] [INFO] [timer.py:166:stop] 0/1000, SamplesPerSec=3.2142016369838835\n",
            "training loss: 4.586128211021423\n",
            "training loss: 4.550553202629089\n",
            "[2021-01-30 02:19:25,012] [INFO] [timer.py:166:stop] 0/1040, SamplesPerSec=3.214865026933223\n",
            "training loss: 4.615422606468201\n",
            "training loss: 4.599624300003052\n",
            "[2021-01-30 02:21:06,706] [INFO] [timer.py:166:stop] 0/1080, SamplesPerSec=3.2122897034376448\n",
            "training loss: 4.460858845710755\n",
            "training loss: 4.613502287864685\n",
            "[2021-01-30 02:22:47,067] [INFO] [timer.py:166:stop] 0/1120, SamplesPerSec=3.2114386211895822\n",
            "training loss: 4.638203287124634\n",
            "training loss: 4.477600121498108\n",
            "[2021-01-30 02:24:31,364] [INFO] [timer.py:166:stop] 0/1160, SamplesPerSec=3.2062728732595698\n",
            "training loss: 4.538062191009521\n",
            "[2021-01-30 02:25:14,308] [INFO] [logging.py:60:log_dist] [Rank 0] step=1760, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.475897407531738\n",
            "[2021-01-30 02:26:13,250] [INFO] [timer.py:166:stop] 0/1200, SamplesPerSec=3.2040467524123133\n",
            "training loss: 4.457944309711456\n",
            "\n",
            " validation loss: 4.287151455879211 \n",
            "\n",
            "training loss: 4.61493182182312\n",
            "[2021-01-30 02:28:03,944] [INFO] [timer.py:166:stop] 0/1240, SamplesPerSec=3.197045017862515\n",
            "training loss: 4.466604840755463\n",
            "training loss: 4.4757705450057985\n",
            "[2021-01-30 02:29:46,708] [INFO] [timer.py:166:stop] 0/1280, SamplesPerSec=3.194381158498764\n",
            "training loss: 4.565140092372895\n",
            "training loss: 4.483073687553405\n",
            "[2021-01-30 02:31:23,234] [INFO] [timer.py:166:stop] 0/1320, SamplesPerSec=3.1979223585573435\n",
            "training loss: 4.522888898849487\n",
            "training loss: 4.4377766132354735\n",
            "[2021-01-30 02:32:57,302] [INFO] [timer.py:166:stop] 0/1360, SamplesPerSec=3.20358211365254\n",
            "training loss: 4.53577070236206\n",
            "training loss: 4.457735514640808\n",
            "[2021-01-30 02:34:30,929] [INFO] [timer.py:166:stop] 0/1400, SamplesPerSec=3.2093430631640483\n",
            "training loss: 4.411122417449951\n",
            "training loss: 4.442772126197815\n",
            "[2021-01-30 02:36:05,375] [INFO] [timer.py:166:stop] 0/1440, SamplesPerSec=3.2140667751888516\n",
            "training loss: 4.4969822525978085\n",
            "training loss: 4.501057660579681\n",
            "[2021-01-30 02:37:41,358] [INFO] [timer.py:166:stop] 0/1480, SamplesPerSec=3.2172007913889673\n",
            "training loss: 4.571244263648987\n",
            "[2021-01-30 02:38:24,337] [INFO] [logging.py:60:log_dist] [Rank 0] step=1800, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.7594804763793945\n",
            "[2021-01-30 02:39:21,879] [INFO] [timer.py:166:stop] 0/1520, SamplesPerSec=3.216305958483105\n",
            "training loss: 4.514097023010254\n",
            "training loss: 4.570903420448303\n",
            "[2021-01-30 02:41:00,410] [INFO] [timer.py:166:stop] 0/1560, SamplesPerSec=3.217109393314751\n",
            "training loss: 4.567099797725677\n",
            "training loss: 4.7239431738853455\n",
            "[2021-01-30 02:42:42,931] [INFO] [timer.py:166:stop] 0/1600, SamplesPerSec=3.2146437051311425\n",
            "training loss: 4.520523476600647\n",
            "\n",
            " validation loss: 4.447001022100449 \n",
            "\n",
            "What dimes went as much did it went When I took my sweet and I went to rapping I really learned my ribs of trouble mug and then you to be stopped a city let me pop Oh that's so even wonder where does do is all your bunny izzing party lily Go all I got the music go away balling your crew she is so much Go Barbie door down go Go tell me After Gates a hillabbin Ain't no more You don't got no worry baby go go to tell me away go When she cant believe are going down When they threw away Go hard tell you oh Diamonds when she had long go go go Go This far away Go go go go Go with no And I hope life was your hair Go go go go go go go go go go go go running go go go go go go go go go go down go go go go go go go go go go go go go go go go go Go go go go go go go go go go go go go go go go go no me go go go o caside go go go go go go go go go go go go go go go go go go go go go go go go go go good go stop go go go go go go go go go go go go go go go go go go go go<|endoftext|>\n",
            "I tried to pray That I prayed out And I beg to pray I can't see you Oh oh you pray the hopes that it is way back Let me Now I knew that I needed a hero I tried I cry Nothing's not weak If I realized how I suffered but a day When I can fall apart Every day Back to think of a way I knew how it ends No way Words were true Yes I saw the living And you were broken heart So I could never break I never know that I guess you were chasing the one I dreamed For the day For days When no one Ever heard you're hid from youre chasing the way Was the way No way Cause the way back of you meant to say Fancy lives Cause I realized Its I dreamed it's the day Now I guess I knew That you knew I knew Was the way of one two I believed that saves me OOh<|endoftext|>\n",
            "Stand over Please don't go out where we go how freaks start Just wind stops running cold wind We're putting up force we're near Just pull off I'm dancing with her Sleepless night I don't figure the weather but wait till the sky But I don't need is so hard Falling in the storm Make up high does it starts to make a stormtroopers don't stop falling apart Singing up then I'm falling apart Wondering that caring the rush right so high Just let this storming is so miraculous don't stop falling starlight Just when all along So why Do youre cool because love is to pieces can't want it up We're supernatural automatic with you're both know why but you're falling Just as you in and I will not coming right Doing this right in love along just feels so hard to pieces Falling in love comes shining Knowing you're falling in her Just like the storming so hard enough I'm falling Thunder start falling falling in love ever wanted to find you wanna get better than the storm That you're falling falling falling falling falling And you're falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling I just falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling I'm falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling falling falling it falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling Falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling falling\n",
            "I want you to play the night and me No I want more crying I want it I wanna play the floor warm up on roll And everybody's been getting hot damn well I want mama boogie In a n' I want to playin' bandana watching in a player love you All night I never know that's wild in slippin' Christian Sweatly And I wanna go slippers Rocking ball<|endoftext|>\n",
            "training loss: 4.407784652709961\n",
            "[2021-01-30 02:44:53,476] [INFO] [timer.py:166:stop] 0/1640, SamplesPerSec=3.2137106152893624\n",
            "training loss: 4.5405372142791744\n",
            "training loss: 4.532022833824158\n",
            "[2021-01-30 02:46:37,630] [INFO] [timer.py:166:stop] 0/1680, SamplesPerSec=3.210194638679479\n",
            "training loss: 4.478462505340576\n",
            "training loss: 4.588691687583923\n",
            "[2021-01-30 02:48:19,823] [INFO] [timer.py:166:stop] 0/1720, SamplesPerSec=3.208317771527139\n",
            "training loss: 4.533878898620605\n",
            "training loss: 4.466267871856689\n",
            "[2021-01-30 02:50:03,654] [INFO] [timer.py:166:stop] 0/1760, SamplesPerSec=3.205331239870917\n",
            "training loss: 4.537819194793701\n",
            "training loss: 4.545209836959839\n",
            "[2021-01-30 02:51:49,213] [INFO] [timer.py:166:stop] 0/1800, SamplesPerSec=3.2012503874033573\n",
            "training loss: 4.544811701774597\n",
            "[2021-01-30 02:52:27,703] [INFO] [logging.py:60:log_dist] [Rank 0] step=1840, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.545383024215698\n",
            "[2021-01-30 02:53:30,484] [INFO] [timer.py:166:stop] 0/1840, SamplesPerSec=3.2003414924402414\n",
            "training loss: 4.510943424701691\n",
            "training loss: 4.624821162223816\n",
            "[2021-01-30 02:55:13,767] [INFO] [timer.py:166:stop] 0/1880, SamplesPerSec=3.1981020871418555\n",
            "training loss: 4.47139995098114\n",
            "training loss: 4.483117866516113\n",
            "[2021-01-30 02:56:45,971] [INFO] [timer.py:166:stop] 0/1920, SamplesPerSec=3.203350078009747\n",
            "training loss: 4.477286946773529\n",
            "training loss: 4.5656573414802555\n",
            "[2021-01-30 02:58:24,184] [INFO] [timer.py:166:stop] 0/1960, SamplesPerSec=3.204456299259776\n",
            "training loss: 4.470229971408844\n",
            "training loss: 4.495231413841248\n",
            "[2021-01-30 03:00:14,636] [INFO] [timer.py:166:stop] 0/2000, SamplesPerSec=3.197670729719958\n",
            "training loss: 4.528428912162781\n",
            "\n",
            " validation loss: 4.463841569423676 \n",
            "\n",
            "training loss: 4.549604058265686\n",
            "[2021-01-30 03:02:11,705] [INFO] [timer.py:166:stop] 0/2040, SamplesPerSec=3.1893395834018796\n",
            "training loss: 4.537729787826538\n",
            "training loss: 4.432738983631134\n",
            "[2021-01-30 03:03:53,144] [INFO] [timer.py:166:stop] 0/2080, SamplesPerSec=3.1886669204399043\n",
            "training loss: 4.531532144546508\n",
            "training loss: 4.56296489238739\n",
            "[2021-01-30 03:05:42,947] [INFO] [timer.py:166:stop] 0/2120, SamplesPerSec=3.1830115937084273\n",
            "training loss: 4.501964628696442\n",
            "[2021-01-30 03:06:25,065] [INFO] [logging.py:60:log_dist] [Rank 0] step=1880, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.42125324010849\n",
            "[2021-01-30 03:07:06,631] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/language_model/pretrained/1-15058-4.463841569423676/mp_rank_00_model_states.pt\n",
            "[2021-01-30 03:07:33,007] [INFO] [timer.py:166:stop] 0/2160, SamplesPerSec=3.1806899905370036\n",
            "training loss: 4.543485689163208\n",
            "training loss: 4.458857870101928\n",
            "[2021-01-30 03:09:11,608] [INFO] [timer.py:166:stop] 0/2200, SamplesPerSec=3.1818482906770127\n",
            "training loss: 4.458145427703857\n",
            "training loss: 4.504807353019714\n",
            "[2021-01-30 03:10:57,857] [INFO] [timer.py:166:stop] 0/2240, SamplesPerSec=3.1786435113388576\n",
            "training loss: 4.52933531999588\n",
            "training loss: 4.530369710922241\n",
            "[2021-01-30 03:12:38,833] [INFO] [timer.py:166:stop] 0/2280, SamplesPerSec=3.178478196712566\n",
            "training loss: 4.565752148628235\n",
            "training loss: 4.5460110068321224\n",
            "[2021-01-30 03:14:20,173] [INFO] [timer.py:166:stop] 0/2320, SamplesPerSec=3.178120107344616\n",
            "training loss: 4.58105502128601\n",
            "training loss: 4.6007853031158445\n",
            "[2021-01-30 03:16:04,266] [INFO] [timer.py:166:stop] 0/2360, SamplesPerSec=3.176301799590147\n",
            "training loss: 4.528485429286957\n",
            "training loss: 4.5928754806518555\n",
            "[2021-01-30 03:17:44,668] [INFO] [timer.py:166:stop] 0/2400, SamplesPerSec=3.1764857532543296\n",
            "training loss: 4.5747178316116335\n",
            "\n",
            " validation loss: 4.6630729854106905 \n",
            "\n",
            "Dayshhhhh What are we've lost in the moon Shine another polar Christmas day I am falling all the very soul Love is nothing that you do Is that we started sing Sing it clear in harmony Sing it all For all we do a cloud Throw us through the little understanding And have lost in harmony Singing in each 'Cause we stand here Comes far S are singing Singing God on us Singing about We're singing Singing Singing all so much love's a melody Keep singing Singing Singing Singing too much Oh Singing all along All I know Sing Singing Singing theres a rocking in Singing song Singing Singing It's singing Singing Singing the song Singing Singing Singing Singing Singing Singing Singing Singing Singing Singing Because Singing Singing Singinging Singinging Singing I'm singinginging Singing Singinginging Singing Singing Singing Singinging Singinginginginginginginginginginginginginginginginginginging Singinginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginging\n",
            "screen yea shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiuuiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii bugiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Well Iiiiiii late Iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Damniiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii raised Daiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiihiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
            "Shake Shake shake shake A few big fat muscle Tightutter from sleeping pretty lit up My seat Leaugush freeze in the channel is dropping Shake Shake half a trunk tooth Shake Im fist on the room and shake shake Shake it and sh shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake little shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake Hey shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake L shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake Shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake little shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake yeah shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake Our DJ shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake yeah shake shake shake shake shake shake shake shake shake shake a little and shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake shake Take shake shake shake shake shake shake shake shake shake\n",
            "I'm a talking to chill But I am the way things must you once as you do this meaning And all the facts's probably should talk to prove And I'm finally understand The only you feel And that you little help me So wipe the very small memories fade intoience And it's a curse all It was none It sends you are you feel like a little bit of thick You feel plain to in it So dear We don't care It's no you walk like me It's alright When you It's nothing And all melt And it's a little bit<|endoftext|>\n",
            "training loss: 4.546815276145935\n",
            "[2021-01-30 03:20:19,405] [INFO] [timer.py:166:stop] 0/2440, SamplesPerSec=3.1774094688610237\n",
            "training loss: 4.541053295135498\n",
            "[2021-01-30 03:20:59,303] [INFO] [logging.py:60:log_dist] [Rank 0] step=1920, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.551008486747742\n",
            "[2021-01-30 03:22:00,560] [INFO] [timer.py:166:stop] 0/2480, SamplesPerSec=3.1771859240136155\n",
            "training loss: 4.465237164497376\n",
            "training loss: 4.526547694206238\n",
            "[2021-01-30 03:23:38,138] [INFO] [timer.py:166:stop] 0/2520, SamplesPerSec=3.1787633869468634\n",
            "training loss: 4.427645421028137\n",
            "training loss: 4.473430871963501\n",
            "[2021-01-30 03:25:15,108] [INFO] [timer.py:166:stop] 0/2560, SamplesPerSec=3.1805935360582334\n",
            "training loss: 4.44727144241333\n",
            "training loss: 4.479414868354797\n",
            "[2021-01-30 03:26:52,718] [INFO] [timer.py:166:stop] 0/2600, SamplesPerSec=3.1820569499735147\n",
            "training loss: 4.41865303516388\n",
            "training loss: 4.479401659965515\n",
            "[2021-01-30 03:28:28,288] [INFO] [timer.py:166:stop] 0/2640, SamplesPerSec=3.184457888787592\n",
            "training loss: 4.485105621814728\n",
            "training loss: 4.5420340418815615\n",
            "[2021-01-30 03:30:07,348] [INFO] [timer.py:166:stop] 0/2680, SamplesPerSec=3.1851363175993046\n",
            "training loss: 4.701667428016663\n",
            "training loss: 4.445887613296509\n",
            "[2021-01-30 03:31:44,455] [INFO] [timer.py:166:stop] 0/2720, SamplesPerSec=3.1867074673139264\n",
            "training loss: 4.58968026638031\n",
            "training loss: 4.3451675295829775\n",
            "[2021-01-30 03:33:17,682] [INFO] [timer.py:166:stop] 0/2760, SamplesPerSec=3.1900227936295353\n",
            "training loss: 4.5219938397407535\n",
            "[2021-01-30 03:33:56,054] [INFO] [logging.py:60:log_dist] [Rank 0] step=1960, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.503554213047027\n",
            "[2021-01-30 03:34:56,329] [INFO] [timer.py:166:stop] 0/2800, SamplesPerSec=3.1907832400647997\n",
            "training loss: 4.460953342914581\n",
            "\n",
            " validation loss: 4.623272669315338 \n",
            "\n",
            "training loss: 4.364733076095581\n",
            "[2021-01-30 03:36:36,378] [INFO] [timer.py:166:stop] 0/2840, SamplesPerSec=3.192457709077706\n",
            "training loss: 4.551544284820556\n",
            "training loss: 4.5448713302612305\n",
            "[2021-01-30 03:38:15,137] [INFO] [timer.py:166:stop] 0/2880, SamplesPerSec=3.193114471272957\n",
            "training loss: 4.509058976173401\n",
            "training loss: 4.52087037563324\n",
            "[2021-01-30 03:39:48,735] [INFO] [timer.py:166:stop] 0/2920, SamplesPerSec=3.1960098986741596\n",
            "training loss: 4.450099754333496\n",
            "training loss: 4.523803973197937\n",
            "[2021-01-30 03:41:35,498] [INFO] [timer.py:166:stop] 0/2960, SamplesPerSec=3.1931493149467127\n",
            "training loss: 4.551946687698364\n",
            "training loss: 4.433792364597321\n",
            "[2021-01-30 03:43:07,672] [INFO] [timer.py:166:stop] 0/3000, SamplesPerSec=3.1965737127323526\n",
            "training loss: 4.447313284873962\n",
            "training loss: 4.432108449935913\n",
            "[2021-01-30 03:44:40,690] [INFO] [timer.py:166:stop] 0/3040, SamplesPerSec=3.199559446903232\n",
            "training loss: 4.467482125759124\n",
            "training loss: 4.540319609642029\n",
            "[2021-01-30 03:46:23,850] [INFO] [timer.py:166:stop] 0/3080, SamplesPerSec=3.1982546690119547\n",
            "training loss: 4.39710111618042\n",
            "[2021-01-30 03:47:05,720] [INFO] [logging.py:60:log_dist] [Rank 0] step=2000, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.459477257728577\n",
            "[2021-01-30 03:48:04,338] [INFO] [timer.py:166:stop] 0/3120, SamplesPerSec=3.1980796442515746\n",
            "training loss: 4.4758503317832945\n",
            "training loss: 4.627939701080322\n",
            "[2021-01-30 03:49:46,611] [INFO] [timer.py:166:stop] 0/3160, SamplesPerSec=3.1971861449720422\n",
            "training loss: 4.51372606754303\n",
            "training loss: 4.570939993858337\n",
            "[2021-01-30 03:51:28,198] [INFO] [timer.py:166:stop] 0/3200, SamplesPerSec=3.196590380760907\n",
            "training loss: 4.614869570732116\n",
            "\n",
            " validation loss: 4.594017577171326 \n",
            "\n",
            "I'll never know that I always think I don't know that I know that I know how to say History is yours the things that it's Love is looking at the energy that and the one else You don't know I don't know I know that you know I know I know I know that I know that I will Know there is that I Know that I know they know you will know that I know it's know know that it will you know that I know that I know know know know that I know I know know that I made that is that how that that I know know that the S I know I know know that that love that Oh you know know know know know know I know know know that know know know know know know I know know that already know Know that I know know Hustling know know know know know know know know know know know know know know know know know know know know know know know know know Know know know know tw know know know know know know know know know know know know Know know know know know know know know know know know know know know know know know Know know know know you know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know by<|endoftext|>\n",
            "Oh ba ba ba Ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba back now Now gettin down ba ba ba Ba ba ba ba ba ba ba ba ba ba ba ba ba Ba ba ba boy ba ba ba ba ba ba ba ba ba Ba ba ba baah ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba Ba ba ba ba ba ba ba ba ba ba ba ba ba ba da ba ba ba ba ba ba ba ba ba ba ba ba ba Ba ba ba's causing little places ba ba ba Ba ba ba ba ba babe ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba ba My ba ba ba ba ba ba ba forget her in ba ba ba dof Ba ba ba ba ba ba ba ba ba ba ba ba ba ba Ba ba ba ba ba giveout ba ba ba ba ba ba ba ba ba ba Ba that ba ba ba ba ba ba ba ba ba ba ba ba Ba ba ba ba ba ba ba ba ba ba ba ba ba Ba ba Ba Ba ba ba ba yeah ba ba ba ba ba ba ba ba ba ba ba ba Going in<|endoftext|>\n",
            "Lookin' hard The ink's up things you and I need to sayin out of my flames Can I feel alright Pick me what you cut Now I I need Now I set I'm second I'm caught Jealousy All the wait for my problem Can't do is something wrong What you Are I'm sure them Look I have a waste I need What I I Got me down I don't forget How can I forget So Have I'm forget you get you here Something about me tonight Catch you gonna bring it up out What you to put my hate Tell Da question asked for Whatchu This much my self make things to tell me What is I missed Do you ever doubt have you They put bare so fast and tell That direction My name is you out Cuz'T ALL KNOW I'm in your wounds Baby Angelina Who can't get me on my feelings Have a taste the hell And let me now Did I You are at my shadows haunt you're trying What are you Tell me in these facts Cuz I forget What you forgot when I forget And get the moment I get it will Tell me tell me where I one forlter<|endoftext|>\n",
            "Soulja Boy not your lady Even though we doing my song for war That's my season Masterbal Onhet trails on the world come on fire burn this mama run on your motherfright Let it up Let's run this motherfucker here with your motherfuckin' When this Run through a motherfuckin' beat jump out Run away runnin' Runnin' out on the house like to the pressure Rollin' runnin' a pleasure I I die For a moms' with me by Joe I slay this motherfuckers It's just a motherfucker runnin' That's it is born mother fuckin' niggas gon' runnin' with the heat in the re running through Man what I ain't got cats tryna runnin' rich niggas walkin' through man Shit we done been aqua Damn I got me and ran into niggas runnin' to fuck with this fucked with Every motherfuckin' motherfuckin' on this motherfucker us all y' Motherfucker Runnin' fittin' motherfucker motherfuckin' runnin' with it You fuckin' with me Runnin' on the flowin' mother game repeat C and blocks Let's up nigga this motherfucker you motherfucker motherfucker And here If I die run here we sayin' runnin' man to Bad motherfuckin' crazy it again and some Big you like it's gang shit on these motherfuckin' on father and mill' Runnin' wet Fuck with my motherfucker Com' catch me runnin' runnin' Stupid You don't gotta In this motherfuckin' at the motherfucker Cause I was a mother fuckin' some fun Run 'Pac runnin' in my motherfuckin' people tell each other to the motherfuckuh' me Ran this motherfucker on motherfuckin' motherfucker You niggas wanna runnin' at the cool goin' motherfuckerww Yeah Motherfucker runnin' motherfucker filthy motherfucker I runnin' your motherfucker know by fast is the motherfucking Gun runnin' motherfucker motherfucker Two punch Your mother fuckin' motherfucker y' mother fuckin' best fuckin' motherfucker If you motherfucker motherfucker wanted to church Go motherfucker wanna fuck with the motherfucker Motherfucker motherfucker motherfuckin' motherfucker That nigga Runb Okay I runnin'\n",
            "training loss: 4.575645554065704\n",
            "[2021-01-30 03:53:50,879] [INFO] [timer.py:166:stop] 0/3240, SamplesPerSec=3.193540876102137\n",
            "training loss: 4.593151330947876\n",
            "training loss: 4.5019532442092896\n",
            "[2021-01-30 03:55:30,637] [INFO] [timer.py:166:stop] 0/3280, SamplesPerSec=3.1937160500004245\n",
            "training loss: 4.525376772880554\n",
            "training loss: 4.344745564460754\n",
            "[2021-01-30 03:57:14,820] [INFO] [timer.py:166:stop] 0/3320, SamplesPerSec=3.192186882147631\n",
            "training loss: 4.430475044250488\n",
            "training loss: 4.523804402351379\n",
            "[2021-01-30 03:59:00,920] [INFO] [timer.py:166:stop] 0/3360, SamplesPerSec=3.189969555844147\n",
            "training loss: 4.487639200687409\n",
            "training loss: 4.522344076633454\n",
            "[2021-01-30 04:00:35,615] [INFO] [timer.py:166:stop] 0/3400, SamplesPerSec=3.1920768037416436\n",
            "training loss: 4.610137164592743\n",
            "[2021-01-30 04:01:19,801] [INFO] [logging.py:60:log_dist] [Rank 0] step=2040, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.46916230916977\n",
            "[2021-01-30 04:02:24,606] [INFO] [timer.py:166:stop] 0/3440, SamplesPerSec=3.188843125915637\n",
            "training loss: 4.524498915672302\n",
            "training loss: 4.524666595458984\n",
            "[2021-01-30 04:04:09,474] [INFO] [timer.py:166:stop] 0/3480, SamplesPerSec=3.187194781030289\n",
            "training loss: 4.586573958396912\n",
            "training loss: 4.434407711029053\n",
            "[2021-01-30 04:05:41,446] [INFO] [timer.py:166:stop] 0/3520, SamplesPerSec=3.19024249371017\n",
            "training loss: 4.30775146484375\n",
            "training loss: 4.439148807525635\n",
            "[2021-01-30 04:07:18,044] [INFO] [timer.py:166:stop] 0/3560, SamplesPerSec=3.1915709523267743\n",
            "training loss: 4.564222526550293\n",
            "training loss: 4.466406071186066\n",
            "[2021-01-30 04:08:58,946] [INFO] [timer.py:166:stop] 0/3600, SamplesPerSec=3.1913472615831764\n",
            "training loss: 4.548581624031067\n",
            "\n",
            " validation loss: 4.672711026668549 \n",
            "\n",
            "training loss: 4.516268718242645\n",
            "[2021-01-30 04:10:45,774] [INFO] [timer.py:166:stop] 0/3640, SamplesPerSec=3.190335866768264\n",
            "training loss: 4.363587403297425\n",
            "training loss: 4.473506486415863\n",
            "[2021-01-30 04:12:25,038] [INFO] [timer.py:166:stop] 0/3680, SamplesPerSec=3.190697338740383\n",
            "training loss: 4.572744393348694\n",
            "training loss: 4.422087442874909\n",
            "[2021-01-30 04:13:57,258] [INFO] [timer.py:166:stop] 0/3720, SamplesPerSec=3.1934642394227843\n",
            "training loss: 4.495772027969361\n",
            "[2021-01-30 04:14:38,194] [INFO] [logging.py:60:log_dist] [Rank 0] step=2080, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.519384753704071\n",
            "[2021-01-30 04:15:37,209] [INFO] [timer.py:166:stop] 0/3760, SamplesPerSec=3.193552171193278\n",
            "training loss: 4.455252516269684\n",
            "training loss: 4.562090110778809\n",
            "[2021-01-30 04:17:17,176] [INFO] [timer.py:166:stop] 0/3800, SamplesPerSec=3.1936329263666248\n",
            "training loss: 4.52581387758255\n",
            "training loss: 4.433165109157562\n",
            "[2021-01-30 04:18:50,966] [INFO] [timer.py:166:stop] 0/3840, SamplesPerSec=3.195765387770687\n",
            "training loss: 4.573241233825684\n",
            "training loss: 4.5215914964675905\n",
            "[2021-01-30 04:20:22,380] [INFO] [timer.py:166:stop] 0/3880, SamplesPerSec=3.1986399588277643\n",
            "training loss: 4.511582016944885\n",
            "training loss: 4.562317061424255\n",
            "[2021-01-30 04:22:03,770] [INFO] [timer.py:166:stop] 0/3920, SamplesPerSec=3.1982022393008736\n",
            "training loss: 4.440901696681976\n",
            "training loss: 4.4586750864982605\n",
            "[2021-01-30 04:23:39,776] [INFO] [timer.py:166:stop] 0/3960, SamplesPerSec=3.199512741714214\n",
            "training loss: 4.387905740737915\n",
            "training loss: 4.518926000595092\n",
            "[2021-01-30 04:25:14,899] [INFO] [timer.py:166:stop] 0/4000, SamplesPerSec=3.20108114365166\n",
            "training loss: 4.562970376014709\n",
            "\n",
            " validation loss: 4.684265381097793 \n",
            "\n",
            "You're fool enough of your life of the sun How could change the mystery of it is heaven Every single life How can happen How can You can this magic is a full of a twist of confusion And when you believe it is another million different life But it mayned How can look When it live it How can I fix this How it rotting your heart When anyone who just relief It's right When it feels right How can Change it When it Feel it feel is it feels right How to me How high How could it die How can you believe it feels yeah it first it How it up comes right How a light it How it How can it human and it Oh how to the power When nobody talk it feels so safe How they end You feel It feels to it feel when it How it How it I feel it it up when it It could it it to something Oh it it when it How it it feel it it it me when it How could it feel it it it it it it it it it<|endoftext|>\n",
            "Turn up up the corner jar up YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAEAH YEAHEA yEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAH YEAEAH YEAH NEAH YEAH TEAH YEAH YEAH YEAH For YEAH YEAH YEAH YEAH YEAH YEAH YEAEA SoEAH YEAH YEAH HE YEAH YEAEAH YEAH YEAH YEAH YEAH YEA YEAH YEAH YEAH YEAH YEAH YEAEAH YEAH YEAH RoyEAH YEAH LA IEAHEAH YEA PABEAH YEAH YeEAH YahEA YEAH YEAH YEAEAH ExcEAH YEAHEAEAH YEAEAH YEAH YEAEA HEAH YEAH YEAEAH YEAHEAEAH YEAEAH YEAH Yeu YEAEAH YEAH YEAH It YeahEAH NevEA Yeah GEA YEAH YEAH YEAH'EAH YEAH YEAH YEAH YEAH YEAH YEAEAEAHEAH YEAH YEAHEA IEAFEAH YEAH YEAH YEAH YEAH YEAHEAHEAH YEAHEAH YEAEAhe YEAH YEAEAH YEAH YEAH NowEAHEAH HEAH YEA\n",
            "We're the same things we're never outside I'll never afraid It hurts from this for living No nothing we are the same old woods Don't want to stress No I'm never find something that No I wouldn't understand what more no time you could learn to regret Never never see It hurts Telling me and blow your heart Nothing is the tide Telling me Crying I see I see Lying on 'Til I see the tide to learn It's killing of me I see something It's killing time to me I see you like paper I justify me Liars of grass I see You see you see the tide Holding me Crushing black sea Killing me bending out killing me Watching me No no times Burning the tide Burning you 'Tilating Fears Seeing the ocean Telling me on tv Blood stains Selling me Locked inside While I see you Watching over Holding me Seeing yourself killing me Telling me towards me Watching me tonight Images killing me Telling me Spitting with me outside Killing me Changing me You see the sun Watching me Breaking me over your new day Watching me Watching me Watching me to see your boat killing me Watching me Watching me Watching me Watching you Watching me Watching me Watching the tide Catching me Watching me Watching me Watching me Watching Watching me Watching me Watching your face Watching me Watching me Watching it Watching me Watching me Watching me Watching Watching us Watching me Watching me Watching my time Watching Watching me Watching Watching the end Watching me Watching me Watching Watching Watching Watching you Watching me Watching Watching Watching Watching Watching Watching Watching Watching me Watching Watching Watching Watching Watching Watching Watching watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching Watching\n",
            "What kills you shouldn't kill within your uptight Kicking up heads Drinks out your minds on the pills Snatched out I'm get out of their stuff Emotion whos are caught that I Need I'd break these drugs who just like drugs stinks out Killing all the cuffs incisions Sticking out from the ready for a new team Staring out Stick out Today Doctor me Count out Who out of our covers are we don't do Round their asses Pulling out of a while and a dead or two Transcicate Running out the weekend Come out Waring at all the morning Waring out like junkies stay out in dreams come outshine Round and through the mountains And we do some loud So don't say That's 'n't agree Let me out all alright to do Baby Outarm Put out knowing So come out And we sing out of lies out They don't do we drop top 'Cause we do you know you out Put 'bout out out I don't know that out out That's what about how we out Just out out<|endoftext|>\n",
            "training loss: 4.442857015132904\n",
            "[2021-01-30 04:27:39,146] [INFO] [timer.py:166:stop] 0/4040, SamplesPerSec=3.20125388895197\n",
            "training loss: 4.364115285873413\n",
            "[2021-01-30 04:28:22,008] [INFO] [logging.py:60:log_dist] [Rank 0] step=2120, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.487859010696411\n",
            "[2021-01-30 04:29:25,683] [INFO] [timer.py:166:stop] 0/4080, SamplesPerSec=3.199191627557064\n",
            "training loss: 4.506779563426972\n",
            "training loss: 4.3376168489456175\n",
            "[2021-01-30 04:31:02,539] [INFO] [timer.py:166:stop] 0/4120, SamplesPerSec=3.2001782236148\n",
            "training loss: 4.526573443412781\n",
            "training loss: 4.564758682250977\n",
            "[2021-01-30 04:32:43,210] [INFO] [timer.py:166:stop] 0/4160, SamplesPerSec=3.1999717115267194\n",
            "training loss: 4.508683037757874\n",
            "training loss: 4.529923319816589\n",
            "[2021-01-30 04:34:19,593] [INFO] [timer.py:166:stop] 0/4200, SamplesPerSec=3.2010768068399553\n",
            "training loss: 4.37163393497467\n",
            "training loss: 4.400277292728424\n",
            "[2021-01-30 04:35:59,271] [INFO] [timer.py:166:stop] 0/4240, SamplesPerSec=3.2011659135630843\n",
            "training loss: 4.410897886753082\n",
            "training loss: 4.536625957489013\n",
            "[2021-01-30 04:37:38,894] [INFO] [timer.py:166:stop] 0/4280, SamplesPerSec=3.2012696217453303\n",
            "training loss: 4.500269603729248\n",
            "training loss: 4.451987147331238\n",
            "[2021-01-30 04:38:38,659] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/language_model/pretrained/1-17209-4.684265381097793/mp_rank_00_model_states.pt\n",
            "[2021-01-30 04:39:20,295] [INFO] [timer.py:166:stop] 0/4320, SamplesPerSec=3.202382621565344\n",
            "training loss: 4.420726215839386\n",
            "training loss: 4.551388740539551\n",
            "[2021-01-30 04:41:04,260] [INFO] [timer.py:166:stop] 0/4360, SamplesPerSec=3.2011964087616946\n",
            "training loss: 4.539659810066223\n",
            "[2021-01-30 04:41:47,323] [INFO] [logging.py:60:log_dist] [Rank 0] step=2160, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.575371038913727\n",
            "[2021-01-30 04:42:46,666] [INFO] [timer.py:166:stop] 0/4400, SamplesPerSec=3.200486666238477\n",
            "training loss: 4.487273609638214\n",
            "\n",
            " validation loss: 4.693517869710922 \n",
            "\n",
            "training loss: 4.438080525398254\n",
            "[2021-01-30 04:44:26,872] [INFO] [timer.py:166:stop] 0/4440, SamplesPerSec=3.201506131466002\n",
            "training loss: 4.507414603233338\n",
            "training loss: 4.536005330085755\n",
            "[2021-01-30 04:45:59,531] [INFO] [timer.py:166:stop] 0/4480, SamplesPerSec=3.2035960733421227\n",
            "training loss: 4.4009002327919005\n",
            "training loss: 4.447089576721192\n",
            "[2021-01-30 04:47:37,023] [INFO] [timer.py:166:stop] 0/4520, SamplesPerSec=3.20427820196963\n",
            "training loss: 4.440733528137207\n",
            "training loss: 4.477250134944915\n",
            "[2021-01-30 04:49:19,388] [INFO] [timer.py:166:stop] 0/4560, SamplesPerSec=3.20357641904897\n",
            "training loss: 4.364062654972076\n",
            "training loss: 4.392945492267609\n",
            "[2021-01-30 04:50:56,466] [INFO] [timer.py:166:stop] 0/4600, SamplesPerSec=3.20436232081717\n",
            "training loss: 4.564611577987671\n",
            "training loss: 4.608075761795044\n",
            "[2021-01-30 04:52:33,526] [INFO] [timer.py:166:stop] 0/4640, SamplesPerSec=3.2051401390793854\n",
            "training loss: 4.368112504482269\n",
            "training loss: 4.440639972686768\n",
            "[2021-01-30 04:54:09,402] [INFO] [timer.py:166:stop] 0/4680, SamplesPerSec=3.206230003371981\n",
            "training loss: 4.406492710113525\n",
            "[2021-01-30 04:54:38,681] [INFO] [logging.py:60:log_dist] [Rank 0] step=2200, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.482410705089569\n",
            "[2021-01-30 04:55:38,306] [INFO] [timer.py:166:stop] 0/4720, SamplesPerSec=3.209203413097663\n",
            "training loss: 4.465405833721161\n",
            "training loss: 4.49454619884491\n",
            "[2021-01-30 04:57:10,257] [INFO] [timer.py:166:stop] 0/4760, SamplesPerSec=3.2113064696327274\n",
            "training loss: 4.486421036720276\n",
            "training loss: 4.4336590051651\n",
            "[2021-01-30 04:58:49,989] [INFO] [timer.py:166:stop] 0/4800, SamplesPerSec=3.211285302598458\n",
            "training loss: 4.4921520113945\n",
            "\n",
            " validation loss: 4.485174608230591 \n",
            "\n",
            "Ah why ain't like my back on your head Listen listen Listenin' about the night when it ain't sound Whenever we're gonna call my motherfuckin' here And let go and call your motherfuckin me go home Ring the iron fist In the pool of your wife And I ain't gonna shoot your motherfuckers Tell your wig tie like it stand and your nose And where I ain't gonna shut Hold your motherfuckers lie wack motherfuckers know what we swingin' to bed And listen Swallow your wifey is a messin' rock song An all to me is called your best shot again We walk out And do because i'ma for what you like your life I'm covered your motherfuckin' with mothership Talkin' on your motherfuckin about her Because i lassutz Then stop till you ain't ready or motherfuckers and on the slaughter to spazin I ain't gon' Knowah cause she go to do is me again Won't ya motherfuckers gonna work your son Come on my motherfuckinin to the front i'ma if your motherfuckin Or you hear That motherfucka gut your motherfuckers Get your motherfuckin that ya motherfuckin' on your motherfuckers I'ma fuckin motherfuckin Leave your motherfuckers you motherfucka find your motherfuckin Huh you ain't fuck your motherfuckin motherfuckin Imma put motherfuckin You motherfuckin ass motherfuckin motherfuckin your motherfuckin a motherfuckin on your motherfuckers motherfuckin out your motherfuckers Yeah motherfuckin's alright yah motherfuckin motherfuckin motherfuckin shit on your motherfucker<|endoftext|>\n",
            "You you always think of my head head You're just don't want to try to drink with me again And I dont want just a drink of glass And I was so Don't you're not what's buggin regret it But I was spray the towel You're giving up and Gucci Wanna pour up on my back of me sleeve Don't go back down Cause I just need I need My face the lies It's not your makeup on my pillow When I wanted it anymore It's not around me We're not my face is a second I smoke Gonna I need your face I I need Gonna burn go from the lock of aftermath I wanna go when we're my fantasies I want when You're not your face But we fight We're not be alright But in my face I've wanted a trace that You're not the walls I just us We're the truth It's not a temporary day Instead of just a stain Our kiss of the face We're just a reminder of my face It screams are red I can't turnin around We don't want but I I Wanna see your face you When the face is not the face We're not a trace It's not the face We're not your face You're the face We still want to save it I just just about yesterday We're the face of your face I Can't face We're not a face it We're not not your face We're the face is not your face We're not the face We're not a face You're not the face it We're not the face the face We're not the face We're not the face We're not your face the face for a face We're not the face You're not the face We We're not the face the face We're the face You're not the face We're not the face No face We're not the face And we're not the face I stare We're not the face the face We is the face You're not the face We face like face I We're not a face We're not I face and face You're not the face We're not the face I want you You're face Us We're the face We're Not to face We're not the face the face We face We're Not your face the face face We're not face Face the face You're not the face You're face I We're face We're not face We're not the face the face We're on face We're face face<|endoftext|>\n",
            "Tell me things you want me then you want you want to be Paris Hilton your lobby out of Paris Hilton and a gate But you a brush demon is get out of your hotel lobby to be getting stilebebehave you want to be a hotel rain on fire yeah slow you want to get a messin' to hear you already got a working on me want to get on your sleep Hey hey hey hey you want to bed and get anxious for sure that you know I want to be working on you want to get you want to get me naked in the lobby now I get wanted to stay awake got to get away go down on a need to get a dirty I want on getting tired of you want to get out on me baby that'm getting tired Come on me to quit complain get to get annoying to do you to get to be the sheets are getting away if you to get away touch I to get to get you on want to be a scrub your clothes to get to get to get to get to get to be walking to get away from a get get to get to get your working to get to see you get on to get to you get down to get to get to get after day to get to get to get to get<|endoftext|>\n",
            "Calms with the night again keep listening yeah You're gone I'm lying 'bout to tell you're with them all alone nothing I'd be gone away from life and going to me on tv Time is now Time slipping away Time is found Time stops Why don't sit around Time is all I cannot be the same Time never did I've been on Im gone Time is gone Time has frozen Time Time to not what can see Time will can you run Time has no time Time is gone Time only you see Time Time has made Time can hear Time is the Time is gone Time has left Time begins Time Time Time Time you won't have Time ticks Time <|endoftext|>\n",
            "training loss: 4.520884323120117\n",
            "[2021-01-30 05:01:06,498] [INFO] [timer.py:166:stop] 0/4840, SamplesPerSec=3.2108271658918417\n",
            "training loss: 4.5666756391525265\n",
            "training loss: 4.468753957748413\n",
            "[2021-01-30 05:02:35,290] [INFO] [timer.py:166:stop] 0/4880, SamplesPerSec=3.213703252031106\n",
            "training loss: 4.337489926815033\n",
            "training loss: 4.3472837686538695\n",
            "[2021-01-30 05:04:18,529] [INFO] [timer.py:166:stop] 0/4920, SamplesPerSec=3.2127427925396175\n",
            "training loss: 4.439278984069825\n",
            "training loss: 4.441774904727936\n",
            "[2021-01-30 05:05:56,177] [INFO] [timer.py:166:stop] 0/4960, SamplesPerSec=3.213253203145897\n",
            "training loss: 4.488783872127533\n",
            "training loss: 4.565782856941223\n",
            "[2021-01-30 05:07:34,870] [INFO] [timer.py:166:stop] 0/5000, SamplesPerSec=3.213485672276835\n",
            "training loss: 4.520179343223572\n",
            "[2021-01-30 05:08:16,401] [INFO] [logging.py:60:log_dist] [Rank 0] step=2240, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.391019976139068\n",
            "[2021-01-30 05:09:17,497] [INFO] [timer.py:166:stop] 0/5040, SamplesPerSec=3.2127066973055536\n",
            "training loss: 4.508432996273041\n",
            "training loss: 4.50148354768753\n",
            "[2021-01-30 05:10:56,267] [INFO] [timer.py:166:stop] 0/5080, SamplesPerSec=3.212920058433451\n",
            "training loss: 4.397985184192658\n",
            "training loss: 4.555628979206086\n",
            "[2021-01-30 05:12:35,036] [INFO] [timer.py:166:stop] 0/5120, SamplesPerSec=3.213130568769399\n",
            "training loss: 4.502952766418457\n",
            "training loss: 4.537720310688019\n",
            "[2021-01-30 05:14:16,025] [INFO] [timer.py:166:stop] 0/5160, SamplesPerSec=3.2127825228791522\n",
            "training loss: 4.4239097237586975\n",
            "training loss: 4.511605858802795\n",
            "[2021-01-30 05:15:57,828] [INFO] [timer.py:166:stop] 0/5200, SamplesPerSec=3.2122376176788086\n",
            "training loss: 4.478345370292663\n",
            "\n",
            " validation loss: 4.40747721195221 \n",
            "\n",
            "training loss: 4.442755317687988\n",
            "[2021-01-30 05:17:42,172] [INFO] [timer.py:166:stop] 0/5240, SamplesPerSec=3.2119623763062974\n",
            "training loss: 4.428467178344727\n",
            "training loss: 4.604141068458557\n",
            "[2021-01-30 05:19:28,526] [INFO] [timer.py:166:stop] 0/5280, SamplesPerSec=3.210321019966612\n",
            "training loss: 4.546381425857544\n",
            "training loss: 4.520099568367004\n",
            "[2021-01-30 05:21:04,633] [INFO] [timer.py:166:stop] 0/5320, SamplesPerSec=3.2111877867410445\n",
            "training loss: 4.434958803653717\n",
            "[2021-01-30 05:21:45,829] [INFO] [logging.py:60:log_dist] [Rank 0] step=2280, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.42952675819397\n",
            "[2021-01-30 05:22:47,610] [INFO] [timer.py:166:stop] 0/5360, SamplesPerSec=3.2103894869446186\n",
            "training loss: 4.479554212093353\n",
            "training loss: 4.523566699028015\n",
            "[2021-01-30 05:24:23,474] [INFO] [timer.py:166:stop] 0/5400, SamplesPerSec=3.2113008087705093\n",
            "training loss: 4.418182933330536\n",
            "training loss: 4.56122190952301\n",
            "[2021-01-30 05:25:59,504] [INFO] [timer.py:166:stop] 0/5440, SamplesPerSec=3.2121600996564004\n",
            "training loss: 4.541312563419342\n",
            "training loss: 4.398393559455871\n",
            "[2021-01-30 05:27:41,598] [INFO] [timer.py:166:stop] 0/5480, SamplesPerSec=3.2115795934776035\n",
            "training loss: 4.424692225456238\n",
            "training loss: 4.417653441429138\n",
            "[2021-01-30 05:29:22,502] [INFO] [timer.py:166:stop] 0/5520, SamplesPerSec=3.2112854165688165\n",
            "training loss: 4.47182799577713\n",
            "training loss: 4.434528720378876\n",
            "[2021-01-30 05:31:03,203] [INFO] [timer.py:166:stop] 0/5560, SamplesPerSec=3.2110425735804133\n",
            "training loss: 4.350541567802429\n",
            "training loss: 4.459840536117554\n",
            "[2021-01-30 05:32:38,875] [INFO] [timer.py:166:stop] 0/5600, SamplesPerSec=3.2119614967651917\n",
            "training loss: 4.382909595966339\n",
            "\n",
            " validation loss: 4.577460503578186 \n",
            "\n",
            "I tried to make my way to come out A call me through the caption I wouldn't have to change This could do This is there all I did To explain I would turn away my time to make it feel afraid to understand This things I've got to lean on my soul To turn to hold my soul That I feel my soul that love is all away 'Cause I turn around Is it up right This heart is my soul let it all around This is me Is love This is the lights I got to the dark This is the strength to lose to the reason I give up This is the key This is my soul This is the soul This is the keys To feel the heaven yeah This is the soul This is the heavenly spirit free This is the soul This is the soul This is our This is for This is the soul This is my soul This is the soul This is the soul This is the This is the soul This is your soul This is the This is the soul This is This is This is This is This is This time This is that This is This This is This is This is This This is This is This is This is This This to These This This is This This is the This This This This is This is This This is This This This This is This is This is This This This This is This This This This This This is<|endoftext|>\n",
            "Everybody's head off tonight Take a romance Don't get back in the crowd tonight Better keep it to keep running like Put down the money in the sympathy that's true My eyes fit it's in the money Make it to our time to the guest in line Guess it's go get back depending time to the sun Tuesday night or kneel the homeless I have mercy each time I'm lying I'm sitting back I can only if I'm begging Make it bena get back into the feeling guilty of my pay It's the best to the having a man too hard livin' dirty ho gettin' Anybody never had that now eat it was looking so paying dues it's only one to put me ho's gonna price I keep it could gettin' the cash money away What if it back around get it up lookin' me pay ya don't know it away the bride's our ass nigga from me Paying it You can't gettin' to the price niggas gotta money money and money game Joey see that gettin' it done in the loneliness heartache We ain't talking dirty makin' ass gotta take it go if only have it's a dollar and n gim money Fuck it any killers gonna pay the price tag it gettin' money money money money make a hundred dollar bills gettin' now that paper baby I'ma da money's ayy Gotta pay gotta pay the floor Have the loyal yeah you don't gotta say me how it I gotta church any more piece I don't have to gettin' my own money gotta have to make it all the aisle I'ma good payday Get it's always have some game we have it All my brother or money getting money money it if it to be lonely too attached to the kind of all the kids now I gotta keep it ain't gotta have no riches How to have it and half ass sittin' business money pay the dollar all my poor we cany hey I know the last dollar bills Just raise the bus we gotta have a dollar and i gotta have a dollar weekend so rich gotta set it all done we laid back would play all my head wantin'All kmine Where the money where we gotta have it be all equal who did it's it give it it money We can't give it the best if i gotta have it's my best money We gotta have it 'Cause when I'ma give my pocket Singin' money Make it to share of my money had the world Make money And it's it is my name is I\n",
            "Can you hear the voice of a little lady But how you won't me hear me inside my girl What do you drown in your shadow drops the angels sing What is the line above the seas but it ever reclaim Said the brink of this once and if the weight of hearts to sing to the sea Show me now Remind the lord guide last sins of sight of heavens step of my altar of your eyes If the temples are floats on earth No justice if this secrets Your fires lies would you knew suffering die Your enemies telling me I'll pray for what keep me I'm asking U deliverance truth is a face of you deny This is your devils out to clear By the riversorus I don't care It doesn't care if I know that was more dead and the chosen who am the demons swallowed me I'd if you stood there are keep my souls say that the gates of it hurts to understand These are waiting on in the victims of thought I fail and all in these saintsdigal god and still weeping is black fields are this heaven set me now inside I keep the head for me it is not murdered saints to everything is me of a blade This is your world inside my side of my sins that i had told me These days lying sleeping in the damned me Keeps church gates of life is telling me These secrets will find the gates of my sins unfold I'll find me away God's safer but Godly my sins of me I long drown me buried with me we'll know what lies god knowsiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
            "He's a little closer and a little closer Been so close I don't know I've got to be more than this heart Don't love that I'm won't even regret this love With our love herself You know what it's got to be in this You're so much pain You've got this could be this wish I've got this heart You've got closer to the cold I've got it again You've got this hard for yourself You've got this boy but It's got my heart before I didn't got this heart And this heart I've got it's got this heart You've got me feel this heart Now I got this feeling I'm letting you sleep the heart You got me got this love<|endoftext|>\n",
            "training loss: 4.4317400574684145\n",
            "[2021-01-30 05:35:04,082] [INFO] [timer.py:166:stop] 0/5640, SamplesPerSec=3.2119272296948695\n",
            "training loss: 4.484365141391754\n",
            "[2021-01-30 05:35:44,224] [INFO] [logging.py:60:log_dist] [Rank 0] step=2320, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.5392406463623045\n",
            "[2021-01-30 05:36:40,990] [INFO] [timer.py:166:stop] 0/5680, SamplesPerSec=3.2125466536514957\n",
            "training loss: 4.509100246429443\n",
            "training loss: 4.496189022064209\n",
            "[2021-01-30 05:38:24,315] [INFO] [timer.py:166:stop] 0/5720, SamplesPerSec=3.2117097868610727\n",
            "training loss: 4.5003049850463865\n",
            "training loss: 4.433399796485901\n",
            "[2021-01-30 05:40:04,178] [INFO] [timer.py:166:stop] 0/5760, SamplesPerSec=3.2116601549894717\n",
            "training loss: 4.403847980499267\n",
            "training loss: 4.507690572738648\n",
            "[2021-01-30 05:41:43,040] [INFO] [timer.py:166:stop] 0/5800, SamplesPerSec=3.2118337445646232\n",
            "training loss: 4.440931224822998\n",
            "training loss: 4.482335638999939\n",
            "[2021-01-30 05:43:27,349] [INFO] [timer.py:166:stop] 0/5840, SamplesPerSec=3.2108022723946785\n",
            "training loss: 4.341919636726379\n",
            "training loss: 4.384063231945038\n",
            "[2021-01-30 05:45:04,676] [INFO] [timer.py:166:stop] 0/5880, SamplesPerSec=3.2113158826888486\n",
            "training loss: 4.400319623947143\n",
            "training loss: 4.448227345943451\n",
            "[2021-01-30 05:46:39,671] [INFO] [timer.py:166:stop] 0/5920, SamplesPerSec=3.212330912263053\n",
            "training loss: 4.346412003040314\n",
            "training loss: 4.438186633586883\n",
            "[2021-01-30 05:48:24,102] [INFO] [timer.py:166:stop] 0/5960, SamplesPerSec=3.2112901655539416\n",
            "training loss: 4.434166002273559\n",
            "[2021-01-30 05:49:01,124] [INFO] [logging.py:60:log_dist] [Rank 0] step=2360, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.320659470558167\n",
            "[2021-01-30 05:49:56,294] [INFO] [timer.py:166:stop] 0/6000, SamplesPerSec=3.2128945883829076\n",
            "training loss: 4.465874576568604\n",
            "\n",
            " validation loss: 4.517645823955536 \n",
            "\n",
            "training loss: 4.401560759544372\n",
            "[2021-01-30 05:51:39,222] [INFO] [timer.py:166:stop] 0/6040, SamplesPerSec=3.2129642335372646\n",
            "training loss: 4.472104990482331\n",
            "training loss: 4.462518167495728\n",
            "[2021-01-30 05:53:17,757] [INFO] [timer.py:166:stop] 0/6080, SamplesPerSec=3.2131909742081204\n",
            "training loss: 4.40391720533371\n",
            "training loss: 4.483439183235168\n",
            "[2021-01-30 05:54:51,087] [INFO] [timer.py:166:stop] 0/6120, SamplesPerSec=3.2145131741842916\n",
            "training loss: 4.406729996204376\n",
            "training loss: 4.467416739463806\n",
            "[2021-01-30 05:56:36,462] [INFO] [timer.py:166:stop] 0/6160, SamplesPerSec=3.2132926833084148\n",
            "training loss: 4.531859254837036\n",
            "training loss: 4.478707671165466\n",
            "[2021-01-30 05:58:11,636] [INFO] [timer.py:166:stop] 0/6200, SamplesPerSec=3.2142129415928875\n",
            "training loss: 4.4486035943031315\n",
            "training loss: 4.35559846162796\n",
            "[2021-01-30 05:59:55,593] [INFO] [timer.py:166:stop] 0/6240, SamplesPerSec=3.213303765174908\n",
            "training loss: 4.563126122951507\n",
            "training loss: 4.4898120880126955\n",
            "[2021-01-30 06:01:39,558] [INFO] [timer.py:166:stop] 0/6280, SamplesPerSec=3.2124048548022874\n",
            "training loss: 4.352750253677368\n",
            "[2021-01-30 06:02:16,926] [INFO] [logging.py:60:log_dist] [Rank 0] step=2400, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.387270474433899\n",
            "[2021-01-30 06:03:14,850] [INFO] [timer.py:166:stop] 0/6320, SamplesPerSec=3.2132886686161277\n",
            "training loss: 4.528831875324249\n",
            "training loss: 4.351109027862549\n",
            "[2021-01-30 06:04:57,901] [INFO] [timer.py:166:stop] 0/6360, SamplesPerSec=3.212586780348827\n",
            "training loss: 4.433249926567077\n",
            "training loss: 4.4403021216392515\n",
            "[2021-01-30 06:06:30,250] [INFO] [timer.py:166:stop] 0/6400, SamplesPerSec=3.2140523033465755\n",
            "training loss: 4.394483435153961\n",
            "\n",
            " validation loss: 4.368080613017082 \n",
            "\n",
            "Waving in the way in Wall Water and cups over the sea Cover the rain And the fall Lying half of the wind I believe that's feeling so much longer I aim at everyone No more Promise not To fake I know here Promise me Promise I used to Heaven Promise Promise I never knew Promise time Promise promise Promise Promise I promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise Promise\n",
            "This morning these words A bit of line lines like hell I see every street That I been a double up the movies The streets of understanding I wish I knew all my dealer Be running though I'm traveling hard to walkin' bout running in my medication Quench my own you Coupe do it I'll never buy a place To get you there Now I never had been born in my crew girl to wake up a bad or in the freeway Moving on my own Just a diamond And the right or two do it Runnin' soda Love is my life to heaven in the place where I goes the weed You got a a town you have you need you come and roll Come on baby If you started I never met a ocean you live without you're coming home Take your eyes You're born to me with a route Out of oxygen You'll neva run wild Smooth on your kisses and a a marshmallow girl Come on 'Cause you're a rest of a girl who's got a girl don't you belong to get a ride with a ride to run you now they're raised in Come on a ride baby boy You are in Close Come on a a ride With a ride in my honey come on come on go I never known That's arms Today You're a ride with a ride for a ride Chorus Long ride Come on come on my arms and ride on and ride to heaven in a ride You're my arms and ride baby Runnin' arms Life isn't never die<|endoftext|>\n",
            "I see you and frolicple eyes pale skin no deer In my skin up blinds in change the blinds its mouth would even though it like crushes loose like can't exist I know a long mission home walls of the bit sugar house when deep ocean You're only shadows no time has to cheek one touch for hope It takes my life will cast that one will only touch one living in my house an ordinary touch me ground life in my bones In my eighty degrees When waves like holy water heavy waves In my own way up our lives in a hundred time I get rough Lips soft smile those heavy hands on them all untime on a back in my feet by the carpet cut cut the waves<|endoftext|>\n",
            "I love you love me on me Sad to me I willow down you kissed me of love me Oh find your love me Love blow me through You will me alone <|endoftext|>\n",
            "training loss: 4.364191460609436\n",
            "[2021-01-30 06:08:33,846] [INFO] [timer.py:166:stop] 0/6440, SamplesPerSec=3.215198032957984\n",
            "training loss: 4.425577139854431\n",
            "[2021-01-30 06:09:10,196] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/language_model/pretrained/1-19360-4.368080613017082/mp_rank_00_model_states.pt\n",
            "training loss: 4.454479646682739\n",
            "[2021-01-30 06:10:21,391] [INFO] [timer.py:166:stop] 0/6480, SamplesPerSec=3.214512163244267\n",
            "training loss: 4.543008494377136\n",
            "training loss: 4.388150084018707\n",
            "[2021-01-30 06:12:02,384] [INFO] [timer.py:166:stop] 0/6520, SamplesPerSec=3.214227157897077\n",
            "training loss: 4.49846122264862\n",
            "training loss: 4.475976800918579\n",
            "[2021-01-30 06:13:39,922] [INFO] [timer.py:166:stop] 0/6560, SamplesPerSec=3.2146259009533633\n",
            "training loss: 4.435182011127472\n",
            "training loss: 4.46184093952179\n",
            "[2021-01-30 06:15:11,330] [INFO] [timer.py:166:stop] 0/6600, SamplesPerSec=3.2162208362118063\n",
            "training loss: 4.40844304561615\n",
            "[2021-01-30 06:15:50,938] [INFO] [logging.py:60:log_dist] [Rank 0] step=2440, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.36116087436676\n",
            "[2021-01-30 06:16:50,841] [INFO] [timer.py:166:stop] 0/6640, SamplesPerSec=3.2162189565660158\n",
            "training loss: 4.347502875328064\n",
            "training loss: 4.493180894851685\n",
            "[2021-01-30 06:18:30,566] [INFO] [timer.py:166:stop] 0/6680, SamplesPerSec=3.2161757782789984\n",
            "training loss: 4.522309446334839\n",
            "training loss: 4.519145023822785\n",
            "[2021-01-30 06:20:08,812] [INFO] [timer.py:166:stop] 0/6720, SamplesPerSec=3.2164176620224523\n",
            "training loss: 4.3540752053260805\n",
            "training loss: 4.558154630661011\n",
            "[2021-01-30 06:21:48,995] [INFO] [timer.py:166:stop] 0/6760, SamplesPerSec=3.2162861007068626\n",
            "training loss: 4.43423912525177\n",
            "training loss: 4.452256512641907\n",
            "[2021-01-30 06:23:34,957] [INFO] [timer.py:166:stop] 0/6800, SamplesPerSec=3.2150573112348737\n",
            "training loss: 4.55461437702179\n",
            "\n",
            " validation loss: 4.3800654172897335 \n",
            "\n",
            "training loss: 4.507170462608338\n",
            "[2021-01-30 06:25:15,361] [INFO] [timer.py:166:stop] 0/6840, SamplesPerSec=3.215631147612123\n",
            "training loss: 4.438726687431336\n",
            "training loss: 4.42968978881836\n",
            "[2021-01-30 06:26:55,227] [INFO] [timer.py:166:stop] 0/6880, SamplesPerSec=3.2155662426858993\n",
            "training loss: 4.405569994449616\n",
            "training loss: 4.3388830661773685\n",
            "[2021-01-30 06:28:25,432] [INFO] [timer.py:166:stop] 0/6920, SamplesPerSec=3.217307770313472\n",
            "training loss: 4.375400114059448\n",
            "[2021-01-30 06:29:07,491] [INFO] [logging.py:60:log_dist] [Rank 0] step=2480, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.452589619159698\n",
            "[2021-01-30 06:30:06,355] [INFO] [timer.py:166:stop] 0/6960, SamplesPerSec=3.217037129148186\n",
            "training loss: 4.454497861862182\n",
            "training loss: 4.395312798023224\n",
            "[2021-01-30 06:31:42,167] [INFO] [timer.py:166:stop] 0/7000, SamplesPerSec=3.2177147693939774\n",
            "training loss: 4.359455847740174\n",
            "training loss: 4.406374931335449\n",
            "[2021-01-30 06:33:20,015] [INFO] [timer.py:166:stop] 0/7040, SamplesPerSec=3.218010422914827\n",
            "training loss: 4.3954949378967285\n",
            "training loss: 4.557351148128509\n",
            "[2021-01-30 06:35:07,242] [INFO] [timer.py:166:stop] 0/7080, SamplesPerSec=3.2165881098653273\n",
            "training loss: 4.366315770149231\n",
            "training loss: 4.381367933750153\n",
            "[2021-01-30 06:36:42,683] [INFO] [timer.py:166:stop] 0/7120, SamplesPerSec=3.2173239577691897\n",
            "training loss: 4.467533111572266\n",
            "training loss: 4.491901206970215\n",
            "[2021-01-30 06:38:11,904] [INFO] [timer.py:166:stop] 0/7160, SamplesPerSec=3.2191772861077634\n",
            "training loss: 4.34190925359726\n",
            "training loss: 4.477256548404694\n",
            "[2021-01-30 06:39:48,576] [INFO] [timer.py:166:stop] 0/7200, SamplesPerSec=3.219670083266693\n",
            "training loss: 4.396579372882843\n",
            "\n",
            " validation loss: 4.346962654590607 \n",
            "\n",
            "And they're in a crowded room I'm sorry I can stay at keys for a draw between as hell But there's close my Brooklyn Different kind And the world is gone I'm sicker every corner of reason Living your attention I wonder where ya argue there's forgiven Are you alone Your great if the my waist and the just that I'm out the score What is not alone is different reality if I won't get down for me Or then explain My shine Where can I say Who'll make my name's the same And I I meet again lie for you slip or write the long The way I'm crying cause I ask of unknown And you to the king Please lead No they say gone waiting near I said can page I go I look at the only only man I'm sure someone who's no how There's gone and alone Never will almost there's a vision of fiction where have been And ever you'd lie I get rid of point of the land Before I believe the time I'm still be alone And I lie and ask for my own mistakes I'm alone Have I'm still won't know<|endoftext|>\n",
            "Don't wanna fight what is worth Believe what we have to say going to be Believing what we are is what you wanted We hate So many truths I know what what you doing what I have is meant to believe HowDo you chose between Right separate from what we built for what are we change Believe in your world we are we made for what we dream Right right Miam and make up with you I feel inside and what you know what I believe What is what what we do Cause we feel what you know what's what we're feeling is what we do what I chorus <|endoftext|>\n",
            "sung by Harlie in swishounikkag Trip begu Turn your cocky like a shallow Like murder in your screaming out 'Cause I know there is how there's your leisurekir scary spell Knock out on your mouth'Shackled Ex karat kush hikaze me and I learn to me how Ugh Loz they're saying 'cause you're just want me boy everyday I'll never gonna knock you just not there on the love me It's hard I've only reason I overheard your door 'Bout it's a beautiful On the koola I did it to get into my door I'm gonna do the back 'cause when I was like you doing hand 'cause I I was your mind You have left your door There's too late for your door I was amazing baby Everything in your only love it started Something like me won't be tomorrow come undone I won't run 'cause you're unfur balloon I'd be your door I'm just you're the door unlocked I'm only one can't be your door I love you I've seen before you've been to see nobody else It's no one more 'ette 'cause it's no more 'cause I'm never been gone 'cause it You'll be one true Baby I'm like your door<|endoftext|>\n",
            "Days go against goin' I don't know how they wanna dance no how long All that's in the wild and get get's how I'm bouncing off that it feels now Shooting out I can't breathe out with everybody Wantsk up everybody cries about the can see how your eyes feel their slicked up Eye comes one we meet somebody to waiting for our side How can't get how you explain I need their senses How somebody rescue what you do you are you say I wanna hold you what everybody wants take you want no grabs you got in the under me saying their hands like somebody How did you are you fall in a wall I don't care how does 'em do Nobody does anybody see your name everybody does it feel so real things start hating you're so won't you feelin' how my head How did you know how does anybody know how how do you there wants try to be out how you how can anybody hold somebody TO feel too How did you see how does anybody ill how how do they want everytime people get not understand how you do somebody nobody does it feel how do they don't worry me do do<|endoftext|>\n",
            "training loss: 4.40547206401825\n",
            "[2021-01-30 06:41:39,491] [INFO] [timer.py:166:stop] 0/7240, SamplesPerSec=3.220981665557484\n",
            "training loss: 4.468673515319824\n",
            "[2021-01-30 06:42:23,110] [INFO] [logging.py:60:log_dist] [Rank 0] step=2520, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.610777735710144\n",
            "[2021-01-30 06:43:29,625] [INFO] [timer.py:166:stop] 0/7280, SamplesPerSec=3.2190620811859803\n",
            "training loss: 4.419272029399872\n",
            "training loss: 4.330652356147766\n",
            "[2021-01-30 06:45:04,179] [INFO] [timer.py:166:stop] 0/7320, SamplesPerSec=3.2199224159221513\n",
            "training loss: 4.507660961151123\n",
            "training loss: 4.437037634849548\n",
            "[2021-01-30 06:46:39,441] [INFO] [timer.py:166:stop] 0/7360, SamplesPerSec=3.220649187932755\n",
            "training loss: 4.427447056770324\n",
            "training loss: 4.3887876033782955\n",
            "[2021-01-30 06:48:12,958] [INFO] [timer.py:166:stop] 0/7400, SamplesPerSec=3.221674468801449\n",
            "training loss: 4.445787644386291\n",
            "training loss: 4.4622060537338255\n",
            "[2021-01-30 06:49:56,197] [INFO] [timer.py:166:stop] 0/7440, SamplesPerSec=3.2209931696144363\n",
            "training loss: 4.403792452812195\n",
            "training loss: 4.401770627498626\n",
            "[2021-01-30 06:51:31,654] [INFO] [timer.py:166:stop] 0/7480, SamplesPerSec=3.2216691401391944\n",
            "training loss: 4.360119926929474\n",
            "training loss: 4.415506136417389\n",
            "[2021-01-30 06:53:24,042] [INFO] [timer.py:166:stop] 0/7520, SamplesPerSec=3.219417824256417\n",
            "training loss: 4.449716973304748\n",
            "training loss: 4.432117891311646\n",
            "[2021-01-30 06:55:00,371] [INFO] [timer.py:166:stop] 0/7560, SamplesPerSec=3.2199448288693073\n",
            "training loss: 4.3903327822685245\n",
            "[2021-01-30 06:55:41,646] [INFO] [logging.py:60:log_dist] [Rank 0] step=2560, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.404533958435058\n",
            "[2021-01-30 06:56:37,904] [INFO] [timer.py:166:stop] 0/7600, SamplesPerSec=3.220260958859192\n",
            "training loss: 4.349804925918579\n",
            "\n",
            " validation loss: 4.496871930360794 \n",
            "\n",
            "training loss: 4.501328158378601\n",
            "[2021-01-30 06:58:18,801] [INFO] [timer.py:166:stop] 0/7640, SamplesPerSec=3.2205853454410778\n",
            "training loss: 4.499448347091675\n",
            "training loss: 4.560993003845215\n",
            "[2021-01-30 07:00:01,591] [INFO] [timer.py:166:stop] 0/7680, SamplesPerSec=3.2200073724971987\n",
            "training loss: 4.443353950977325\n",
            "training loss: 4.44667809009552\n",
            "[2021-01-30 07:01:42,472] [INFO] [timer.py:166:stop] 0/7720, SamplesPerSec=3.2197560338040594\n",
            "training loss: 4.384583592414856\n",
            "training loss: 4.426364064216614\n",
            "[2021-01-30 07:03:19,179] [INFO] [timer.py:166:stop] 0/7760, SamplesPerSec=3.2202046598904293\n",
            "training loss: 4.460405278205871\n",
            "training loss: 4.340170288085938\n",
            "[2021-01-30 07:04:47,911] [INFO] [timer.py:166:stop] 0/7800, SamplesPerSec=3.2219753569013423\n",
            "training loss: 4.44102668762207\n",
            "training loss: 4.302169120311737\n",
            "[2021-01-30 07:06:28,359] [INFO] [timer.py:166:stop] 0/7840, SamplesPerSec=3.2217892473742773\n",
            "training loss: 4.3534168124198915\n",
            "training loss: 4.496662616729736\n",
            "[2021-01-30 07:08:04,705] [INFO] [timer.py:166:stop] 0/7880, SamplesPerSec=3.222280803370093\n",
            "training loss: 4.317475032806397\n",
            "[2021-01-30 07:08:50,725] [INFO] [logging.py:60:log_dist] [Rank 0] step=2600, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.485675644874573\n",
            "[2021-01-30 07:09:50,698] [INFO] [timer.py:166:stop] 0/7920, SamplesPerSec=3.2211864423964274\n",
            "training loss: 4.441502285003662\n",
            "training loss: 4.34137327671051\n",
            "[2021-01-30 07:11:28,136] [INFO] [timer.py:166:stop] 0/7960, SamplesPerSec=3.2214977432154637\n",
            "training loss: 4.543902599811554\n",
            "training loss: 4.471259427070618\n",
            "[2021-01-30 07:13:04,540] [INFO] [timer.py:166:stop] 0/8000, SamplesPerSec=3.2219738636002355\n",
            "training loss: 4.3600549578666685\n",
            "\n",
            " validation loss: 4.506739807128906 \n",
            "\n",
            "Now I'm for my own Twisted the ploty nights we hope that night and will always put me There will be for what I dreamed for Someday there bring to dream that you've been out there and kiss it all hope that bring me you must be a symphony I will be dream of me your life Insomnia Think I will bring you bring me out C'mon just a dream lucid I feel alright I swear that brings me down don't think you bring me down Someday you bring some kind of all daydream dream away dream dream I was rather think I need Oh Do you bring me if you bring me out I must bring me out bring me with you bring me some super dream yeah You bring me<|endoftext|>\n",
            "Stay on Someone you're miles away the miles away she says you said she heard those miles away from miles away from miles away from miles away Pick up dust away from miles away She went away from the tracks where they call my ear As bright A girl fears take the pain away make make it go away to go away <|endoftext|>\n",
            "feed eyes have been worn over 'til I feel the dreams but I've thrown upon your shadows again and I go running for words from the hungry I've given to the farmer's not here with the farmer grow into poor But the girlsma's the road regret in the gold It's the wind That we could come from dreams that we came from star in the flowers and there's the children and we'll come t' on the farmer farmer fires And damn beautiful grass grow gold and these colder keep it's winds canl ooh ooh oh sire ask me the wind starts dancing the raiteru hunger drought from the farmer farmer hotter to stall when we know where Lord are coming out there with a hungry grey There's gone I know But we come fanned the pavement so people Whoop does the cool to stay daddy's look at the farmer sons grow<|endoftext|>\n",
            "Turn out the music playing playing games some more A no whould manage Music is swiftness White people play when the people hanging around hounds and just like sound knocks the crowd Crowd strikes and round and out they play till the sound turn around turn we turn around people turn around till we round round round turn around turn the round the sound real pats out say that sound gets there a fool comes the sound and round and round and turn round go<|endoftext|>\n",
            "training loss: 4.426248049736023\n",
            "[2021-01-30 07:14:48,274] [INFO] [timer.py:166:stop] 0/8040, SamplesPerSec=3.223072092955157\n",
            "training loss: 4.549222779273987\n",
            "training loss: 4.472252058982849\n",
            "[2021-01-30 07:16:40,516] [INFO] [timer.py:166:stop] 0/8080, SamplesPerSec=3.2209914836630014\n",
            "training loss: 4.539227056503296\n",
            "training loss: 4.43053297996521\n",
            "[2021-01-30 07:18:19,944] [INFO] [timer.py:166:stop] 0/8120, SamplesPerSec=3.220979733976464\n",
            "training loss: 4.481296813488006\n",
            "training loss: 4.470244634151459\n",
            "[2021-01-30 07:19:58,458] [INFO] [timer.py:166:stop] 0/8160, SamplesPerSec=3.2211134273122597\n",
            "training loss: 4.445337307453156\n",
            "training loss: 4.388238108158111\n",
            "[2021-01-30 07:21:41,409] [INFO] [timer.py:166:stop] 0/8200, SamplesPerSec=3.2205438486635694\n",
            "training loss: 4.3254547119140625\n",
            "[2021-01-30 07:22:25,472] [INFO] [logging.py:60:log_dist] [Rank 0] step=2640, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.3577747464179994\n",
            "[2021-01-30 07:23:23,674] [INFO] [timer.py:166:stop] 0/8240, SamplesPerSec=3.220088079852656\n",
            "training loss: 4.382088446617127\n",
            "training loss: 4.402651417255401\n",
            "[2021-01-30 07:24:55,932] [INFO] [timer.py:166:stop] 0/8280, SamplesPerSec=3.2212038894140655\n",
            "training loss: 4.432477903366089\n",
            "training loss: 4.415616846084594\n",
            "[2021-01-30 07:26:32,590] [INFO] [timer.py:166:stop] 0/8320, SamplesPerSec=3.221623373232609\n",
            "training loss: 4.447879135608673\n",
            "training loss: 4.336137318611145\n",
            "[2021-01-30 07:28:10,437] [INFO] [timer.py:166:stop] 0/8360, SamplesPerSec=3.221854151822423\n",
            "training loss: 4.309562397003174\n",
            "training loss: 4.397641932964325\n",
            "[2021-01-30 07:29:53,240] [INFO] [timer.py:166:stop] 0/8400, SamplesPerSec=3.2213172486617436\n",
            "training loss: 4.443614602088928\n",
            "\n",
            " validation loss: 4.513389861583709 \n",
            "\n",
            "training loss: 4.39146329164505\n",
            "[2021-01-30 07:31:28,827] [INFO] [timer.py:166:stop] 0/8440, SamplesPerSec=3.222491311058739\n",
            "training loss: 4.378933119773865\n",
            "training loss: 4.3675146222114565\n",
            "[2021-01-30 07:33:04,765] [INFO] [timer.py:166:stop] 0/8480, SamplesPerSec=3.223007268480901\n",
            "training loss: 4.369979679584503\n",
            "training loss: 4.496624374389649\n",
            "[2021-01-30 07:34:43,076] [INFO] [timer.py:166:stop] 0/8520, SamplesPerSec=3.2231568769621135\n",
            "training loss: 4.491284918785095\n",
            "[2021-01-30 07:35:23,621] [INFO] [logging.py:60:log_dist] [Rank 0] step=2680, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.47109432220459\n",
            "[2021-01-30 07:36:20,710] [INFO] [timer.py:166:stop] 0/8560, SamplesPerSec=3.2234076826675837\n",
            "training loss: 4.3898398876190186\n",
            "training loss: 4.344267785549164\n",
            "[2021-01-30 07:37:52,355] [INFO] [timer.py:166:stop] 0/8600, SamplesPerSec=3.224561337147729\n",
            "training loss: 4.403213024139404\n",
            "[2021-01-30 07:38:03,714] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/language_model/pretrained/1-21511-4.513389861583709/mp_rank_00_model_states.pt\n",
            "training loss: 4.46722776889801\n",
            "[2021-01-30 07:39:30,125] [INFO] [timer.py:166:stop] 0/8640, SamplesPerSec=3.225601164562313\n",
            "training loss: 4.354074728488922\n",
            "training loss: 4.401857781410217\n",
            "[2021-01-30 07:41:00,192] [INFO] [timer.py:166:stop] 0/8680, SamplesPerSec=3.2269724029935865\n",
            "training loss: 4.3995356798172\n",
            "training loss: 4.4064742684364315\n",
            "[2021-01-30 07:42:37,954] [INFO] [timer.py:166:stop] 0/8720, SamplesPerSec=3.2271827553460213\n",
            "training loss: 4.36129652261734\n",
            "training loss: 4.3137723207473755\n",
            "[2021-01-30 07:44:16,588] [INFO] [timer.py:166:stop] 0/8760, SamplesPerSec=3.227261434280769\n",
            "training loss: 4.414614582061768\n",
            "training loss: 4.394849348068237\n",
            "[2021-01-30 07:45:52,193] [INFO] [timer.py:166:stop] 0/8800, SamplesPerSec=3.227787846847819\n",
            "training loss: 4.474413990974426\n",
            "\n",
            " validation loss: 4.4485782325267795 \n",
            "\n",
            "My holy shit is cooked my candles I bad I still trying to kill you Annie Banks's going out of ten bitches Two delights rough She's mom's the same color my brainless cousin cousin Two kids Pardon her mother mother that is a Mbeams like white trillows was buy them big Red Daddy It's a lovely world that crazy dog now she used to Brazil Where are she know the mental is the other niggas friends said You've got them to me cousin Stamonde And you see this mama died a zone cause the show me Damn them drugs in attack October It's Adam Levine Many silky way they sleep oh damn summer cats eat thing Tell them kinda kill ya the road And they death Way ticket in the shirt and a five It sucks what the Jeep I don't it I'm just like Cuz the age of us fuck 'Cause so could kill momma be kickin'TIP And darling daddy's a big daytime to judge meet ya sould Believe me kill you think I kill my niggas often lived a Ride Chamma kill me Niggas to meet 'till but he a folk niggas a feel red boxes one bad And touch with effort on me got a God like one time They want more older mo' not my chain your big homies that South Beach Biznessy and these green One day see me what I'm lettin That is a storm an episode way in my common thought what you know what I was the silky Now when you need to a big two you could drive me Little sister I heard if you and Kept until then watch us to the reason for millions fucker Back away For the old I think that I said New Orleans That you kill you love you could talkin boys And they kill my way I thought holding they don't nothing you kill me and nobody Not like a sixteen one it was a dawg<|endoftext|>\n",
            "There's a shinyines bloom A face it's nothing wrong way To make a beautiful pursuit A beautiful design So strip for a beautiful trick that a tryina blows our minds A beautiful black eyed man Once upon us beautiful mansion Next beautiful man alive Don't you thinkin' to cry for your heart's grown Thinkin' thing Thinkin' 'Cause of you think We're gonna be sick and tired of your way of hurtin' for your heartbeat Followin' up for the rules We must decline We're gonna be there We're sick of the rain Don't make a beautiful thing of the pain of The threats Think of the fightin' of the one thing that don't think it's gonna fail Think of what's gonna trip And undo Think of us don't be sick of the pain of our love Think of the rain Thinkin' Think of us Think of the rain Think of the rain Think of the rain Think of think of a beautiful Think of the rain Think Think of Think of the rain Think of the rain Think of think of the rain Think of love Think of rain Think of Think of us Think Think of Think Think Think of Think of of Think Think Think Think of of Think Think Think of Think Think Think Think Think Think Think of Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think of Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think Think\n",
            "Star rendezvous with me Have you ever ever loved me Oh Caroline and ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever oh ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever<|endoftext|>\n",
            "We've been done with no one We Having fun We have fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun dancing fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun the fun fun fun fun fun fun fun <|endoftext|>\n",
            "training loss: 4.36385133266449\n",
            "[2021-01-30 07:48:04,565] [INFO] [timer.py:166:stop] 0/8840, SamplesPerSec=3.2282568336339796\n",
            "training loss: 4.3286263823509215\n",
            "[2021-01-30 07:48:46,049] [INFO] [logging.py:60:log_dist] [Rank 0] step=2720, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.381557488441468\n",
            "[2021-01-30 07:49:39,010] [INFO] [timer.py:166:stop] 0/8880, SamplesPerSec=3.228944583773767\n",
            "training loss: 4.280449748039246\n",
            "training loss: 4.489754772186279\n",
            "[2021-01-30 07:51:21,246] [INFO] [timer.py:166:stop] 0/8920, SamplesPerSec=3.2284877491796524\n",
            "training loss: 4.299060904979706\n",
            "training loss: 4.394376349449158\n",
            "[2021-01-30 07:52:54,993] [INFO] [timer.py:166:stop] 0/8960, SamplesPerSec=3.2292698748745763\n",
            "training loss: 4.353539991378784\n",
            "training loss: 4.42628732919693\n",
            "[2021-01-30 07:54:34,808] [INFO] [timer.py:166:stop] 0/9000, SamplesPerSec=3.2291663104958532\n",
            "training loss: 4.415660727024078\n",
            "training loss: 4.338390445709228\n",
            "[2021-01-30 07:56:18,099] [INFO] [timer.py:166:stop] 0/9040, SamplesPerSec=3.228562394275529\n",
            "training loss: 4.363368666172027\n",
            "training loss: 4.583709073066712\n",
            "[2021-01-30 07:58:03,818] [INFO] [timer.py:166:stop] 0/9080, SamplesPerSec=3.2276155661040877\n",
            "training loss: 4.391591548919678\n",
            "training loss: 4.42094931602478\n",
            "[2021-01-30 07:59:44,634] [INFO] [timer.py:166:stop] 0/9120, SamplesPerSec=3.227377777786667\n",
            "training loss: 4.4103028059005736\n",
            "training loss: 4.458579480648041\n",
            "[2021-01-30 08:01:22,988] [INFO] [timer.py:166:stop] 0/9160, SamplesPerSec=3.2274919198106655\n",
            "training loss: 4.442619705200196\n",
            "[2021-01-30 08:02:01,254] [INFO] [logging.py:60:log_dist] [Rank 0] step=2760, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.481980991363526\n",
            "[2021-01-30 08:03:01,643] [INFO] [timer.py:166:stop] 0/9200, SamplesPerSec=3.227562597067666\n",
            "training loss: 4.4689546585083\n",
            "\n",
            " validation loss: 4.329602825641632 \n",
            "\n",
            "training loss: 4.409577345848083\n",
            "[2021-01-30 08:04:40,566] [INFO] [timer.py:166:stop] 0/9240, SamplesPerSec=3.228097737261644\n",
            "training loss: 4.375735521316528\n",
            "training loss: 4.418651175498963\n",
            "[2021-01-30 08:06:24,394] [INFO] [timer.py:166:stop] 0/9280, SamplesPerSec=3.2274390917883307\n",
            "training loss: 4.507517206668854\n",
            "training loss: 4.430327081680298\n",
            "[2021-01-30 08:07:59,073] [INFO] [timer.py:166:stop] 0/9320, SamplesPerSec=3.228064723050203\n",
            "training loss: 4.426787316799164\n",
            "training loss: 4.518136119842529\n",
            "[2021-01-30 08:09:33,443] [INFO] [timer.py:166:stop] 0/9360, SamplesPerSec=3.2287283227494785\n",
            "training loss: 4.434372901916504\n",
            "training loss: 4.353546345233918\n",
            "[2021-01-30 08:11:12,545] [INFO] [timer.py:166:stop] 0/9400, SamplesPerSec=3.2287302522669563\n",
            "training loss: 4.331965315341949\n",
            "training loss: 4.381164693832398\n",
            "[2021-01-30 08:12:54,433] [INFO] [timer.py:166:stop] 0/9440, SamplesPerSec=3.228347549584709\n",
            "training loss: 4.32756119966507\n",
            "training loss: 4.390109038352966\n",
            "[2021-01-30 08:14:33,644] [INFO] [timer.py:166:stop] 0/9480, SamplesPerSec=3.228336186717332\n",
            "training loss: 4.391181683540344\n",
            "[2021-01-30 08:15:12,344] [INFO] [logging.py:60:log_dist] [Rank 0] step=2800, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.440205121040345\n",
            "[2021-01-30 08:16:15,508] [INFO] [timer.py:166:stop] 0/9520, SamplesPerSec=3.2279617354840457\n",
            "training loss: 4.426075983047485\n",
            "training loss: 4.4145232796669\n",
            "[2021-01-30 08:17:53,164] [INFO] [timer.py:166:stop] 0/9560, SamplesPerSec=3.228163909095947\n",
            "training loss: 4.328370857238769\n",
            "training loss: 4.465838074684143\n",
            "[2021-01-30 08:19:34,973] [INFO] [timer.py:166:stop] 0/9600, SamplesPerSec=3.227800823580689\n",
            "training loss: 4.420411276817322\n",
            "\n",
            " validation loss: 4.372559499740601 \n",
            "\n",
            "I left home a million miles away from another day yeah I left the first time from one But it's so bad word I got another place on the meaning tries to vent I feel like one wild in my soul I'm worth living in your history I'm standing on vacation in the hogs willing to start and crawl like candy Sundays curtained pavement rain fucking values I know All the groves scorned by submission another battle begins to the colour in the past yearning dates they're living senses I know it's got the hungry yearning souls And all comes to grow in fact Let's take you like that I mention pages of habits morals'clock In this cowboy days Snake the strange hotel subways freeze the votes Oh let the streets of you kissed my hot day go in sunny days when I trip Livin' at the fire in the daylight slippin' alone mortal ground you breathe it's life run free Livin' under the breeze drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip D drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip drip<|endoftext|>\n",
            "She feels like a free with the pearls outside a flower She lives away to be pain but wonder if unspoken please steal take her badly she's the world and then why she's gone too edenions killing me she smiles on your dreams that the time over me sometimes stress a young and she knows that she sees me and my name and turns a world it turned upside on the world needs to reason that will be alone and beaten and hope she's a wastin' on to be a lovely lady she needs aways and out to bother say what dark around to be saved but she goes and beyond the world darkness comes marching hand <|endoftext|>\n",
            "Miss Louis Louis Yess Backstreet is Louis V touches my nigga TIX ordest weekdays Say whats lookin back to my nigga she want niggas flosss big date walk out in Miami We tore yall you fuck Shawty or pull down take areep me in the roof switch when I don't even tryna touch cause I bustin back up clothes when you ridin haaterg together like it was ridin pop you touch it In my hoes like president and tryna fuck it Might seem hoes When they want niggas out the mud man take that niggas you I hit out to the bed instead What' hoes Its for my suit your money like aint look in here do imma truck down the crew in all got peep out here Think I'm gone putstand We gone to beat All I pullin watch me in the beatin high in the sheets bitch out there throw up never have them hands at your bottom Im Leaving them bitches taking shots thats what Im ball all the club nigga Really really mean what Im flyer we all my coat Sitting on my nigga Big dope not even for the curb Imma fucked up I tossed if you in the floor Imma fuck out here Blowin a nigga doin many lifelashed out here last thing Waitin out here a fuckers around like hoes model for when I got this Come come by my room Slide And getting at you sick and done changed my hood goin Footin turn the cuttin high gloss full of my wrist from here with that's Barrone on Uh all night talk my Extra rubberband on the way I'm comin up If you mud Grind with a nigga you can loose sacks tall I'ma blow A nigga When the counter eighty deep breathin shot Short pockets bitch on When I slidin flex I been in the manual club in the low Pointed out the panties nigga wont take it in here deep Shit long black bag How the Bitch Imma blowin in shoes Go bang hit with her butt chick out when I said fuck niggas walking outta here slap it when I turn up in a couple plans bitch ass for a automatics in here we fools you want that knocked off C Pull out my lottery I got plenty I said hey When I fell out here and Imma blow your nails in here nigga when the floor Niggers supposed to my crib ballin right here Late on my thing Imma make it thany I'm barely going straight and go\n",
            "store we have come to loan Dirty choices Struck by Miss R Kicked up until we went to live We met you said goodbye Planet of scene Another Sky high We were crazy thoughts Teenage to hold the prizes Stankids Stealing st Licking and again We spent by your desperate for Hollywood stoned Now stuck by your eyes Terrified unsuspecting results Every time Staring to freaked silently And kept on our emergency Stealing days We talked for more bargain stoned and I always With a way But our heroes Well the stoned Are desperate for this in mind Now that received We're satisfied<|endoftext|>\n",
            "training loss: 4.337736594676971\n",
            "[2021-01-30 08:21:51,101] [INFO] [timer.py:166:stop] 0/9640, SamplesPerSec=3.227406852037471\n",
            "training loss: 4.47962384223938\n",
            "training loss: 4.3174515008926395\n",
            "[2021-01-30 08:23:31,168] [INFO] [timer.py:166:stop] 0/9680, SamplesPerSec=3.2272843235202555\n",
            "training loss: 4.475136232376099\n",
            "training loss: 4.345835721492767\n",
            "[2021-01-30 08:25:15,106] [INFO] [timer.py:166:stop] 0/9720, SamplesPerSec=3.226644427781543\n",
            "training loss: 4.524495100975036\n",
            "training loss: 4.45380597114563\n",
            "[2021-01-30 08:26:54,884] [INFO] [timer.py:166:stop] 0/9760, SamplesPerSec=3.2265648453531526\n",
            "training loss: 4.457139551639557\n",
            "training loss: 4.273001134395599\n",
            "[2021-01-30 08:28:40,210] [INFO] [timer.py:166:stop] 0/9800, SamplesPerSec=3.225749139236109\n",
            "training loss: 4.581574702262879\n",
            "[2021-01-30 08:29:17,963] [INFO] [logging.py:60:log_dist] [Rank 0] step=2840, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.559483420848847\n",
            "[2021-01-30 08:30:11,398] [INFO] [timer.py:166:stop] 0/9840, SamplesPerSec=3.2268097335646857\n",
            "training loss: 4.30599422454834\n",
            "training loss: 4.334444952011109\n",
            "[2021-01-30 08:31:47,632] [INFO] [timer.py:166:stop] 0/9880, SamplesPerSec=3.227197216712951\n",
            "training loss: 4.2334715008735655\n",
            "training loss: 4.410051059722901\n",
            "[2021-01-30 08:33:32,757] [INFO] [timer.py:166:stop] 0/9920, SamplesPerSec=3.226414824174446\n",
            "training loss: 4.4102174043655396\n",
            "training loss: 4.430851125717163\n",
            "[2021-01-30 08:35:14,632] [INFO] [timer.py:166:stop] 0/9960, SamplesPerSec=3.2260636759047583\n",
            "training loss: 4.444396829605102\n",
            "training loss: 4.4071561932563785\n",
            "[2021-01-30 08:36:50,401] [INFO] [timer.py:166:stop] 0/10000, SamplesPerSec=3.226509748902058\n",
            "training loss: 4.304399240016937\n",
            "\n",
            " validation loss: 4.251693600416184 \n",
            "\n",
            "training loss: 4.372011470794678\n",
            "[2021-01-30 08:38:31,318] [INFO] [timer.py:166:stop] 0/10040, SamplesPerSec=3.226746346694739\n",
            "training loss: 4.416631758213043\n",
            "training loss: 4.388953053951264\n",
            "[2021-01-30 08:40:14,812] [INFO] [timer.py:166:stop] 0/10080, SamplesPerSec=3.2261887835016014\n",
            "training loss: 4.462161111831665\n",
            "training loss: 4.354091644287109\n",
            "[2021-01-30 08:41:57,749] [INFO] [timer.py:166:stop] 0/10120, SamplesPerSec=3.2257074140489683\n",
            "training loss: 4.314006662368774\n",
            "[2021-01-30 08:42:35,997] [INFO] [logging.py:60:log_dist] [Rank 0] step=2880, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.357436835765839\n",
            "[2021-01-30 08:43:36,328] [INFO] [timer.py:166:stop] 0/10160, SamplesPerSec=3.2257879962999656\n",
            "training loss: 4.446260297298432\n",
            "training loss: 4.333485639095306\n",
            "[2021-01-30 08:45:04,821] [INFO] [timer.py:166:stop] 0/10200, SamplesPerSec=3.2271549409765963\n",
            "training loss: 4.317260277271271\n",
            "training loss: 4.408703446388245\n",
            "[2021-01-30 08:46:44,464] [INFO] [timer.py:166:stop] 0/10240, SamplesPerSec=3.227094006159934\n",
            "training loss: 4.526072525978089\n",
            "training loss: 4.3989012956619264\n",
            "[2021-01-30 08:48:17,137] [INFO] [timer.py:166:stop] 0/10280, SamplesPerSec=3.227916441013634\n",
            "training loss: 4.387613427639008\n",
            "training loss: 4.437036764621735\n",
            "[2021-01-30 08:49:49,242] [INFO] [timer.py:166:stop] 0/10320, SamplesPerSec=3.2288049469620312\n",
            "training loss: 4.362122249603272\n",
            "training loss: 4.348887860774994\n",
            "[2021-01-30 08:51:30,269] [INFO] [timer.py:166:stop] 0/10360, SamplesPerSec=3.228564108789651\n",
            "training loss: 4.473175263404846\n",
            "training loss: 4.418309164047241\n",
            "[2021-01-30 08:53:08,132] [INFO] [timer.py:166:stop] 0/10400, SamplesPerSec=3.2287216815975426\n",
            "training loss: 4.3115702867507935\n",
            "\n",
            " validation loss: 4.429459011554718 \n",
            "\n",
            "Could you be the chance Chorus Rave me Not so fake leave me the thief Not that old Chorus And let me know What's just a fool Trust me know it's just like it Trust is realer baby Dusty Trust me<|endoftext|>\n",
            "I've got dirty mind Got my mind Brass Monkey business Monkey And I ain't got no mind I got no future Mister Land under That I got it Got no snake You got no evil YMCMB I ain't no good as I'm lay Bitch I got no better I got no good Fuck niggas where I got to kick you feeling good Asking Well you up in the mansion And money Got no good to get it and fucking like a Gotta scooters Took an and I Got problems but they got a groove no good My mental The head Goddamn I got no good I ain't got me diggin' high as I got no will be thirsty for mercy no good as pumpkin pie good as fuck 'em pretty as no good at I got nothing at the silicone Washin' I got no good as I got hip hopscot Good milk that tree From town 'em to ride crummy Fat heart I'm what's love ain't got shit Sss box On some good as the juvenile Bad bitches Got no funky Monkey Ball Anxiety got mud 'emeter Screw it as good I got tongue on a part Record Bad bitch In the ground Nothing much sock Lord make it whole lames got five I got a gutter SIN Let's the V I got love on the white I got beef bread St Kon Artistic Motherfuckin' love bad to be hang around you Got the speakers While I got the moon I got to hate Bad bitch And I got mud Got my people Blag bad ass PO can leave the good Fuck numb Bust my dog Hoes Bad doctor Need it straight to get good Good God For Godiva What I got the smell me see I got got no sense on God got a great dog And a good shit That's the system Hit the black as good Bad babes boat ho oh Bad bitches wanna be long head most Hoe no pill Got no good no space Make it down to get it Good junk I got it bad on an booty good as fake dumb dumb body pounds Got the bitch got top Bad Fish got the moth muthafucka hit the good good We know I got up on me mad shit just got one blues Hit spamic view<|endoftext|>\n",
            "Oh It Coming Ah Ah girl Well I will hunt youll always hunt you are playing band Between me And I will hunt me On the game With you on you With everything So deep within And you are the deep With the lies are sunny wind blows through so deep within gray skies Now our backs against The looks all the fear travier than the one Whis lies Kept fall We are all descend of sleepless mind What is all which all we are so lost within As our only cut through all Within these waters we done Two sewn by broken dreams that thrill grows So mi i become Be a deep within With all i ah ah now let the clouds make up from the darkness becomes the covers<|endoftext|>\n",
            "I've been jumping like Flying down like some magic You'll hear the heat of the blood You gotta reach and elevate me Cause I've got something I'll be one you like I've got my air Cause you've got One more You won't mind outer space on those heavens No space We've got one No space space outer space space space outer space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space outer space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space line space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space place space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space D space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space\n",
            "training loss: 4.231815385818481\n",
            "[2021-01-30 08:55:20,369] [INFO] [timer.py:166:stop] 0/10440, SamplesPerSec=3.2291756488204344\n",
            "training loss: 4.512830471992492\n",
            "[2021-01-30 08:56:02,028] [INFO] [logging.py:60:log_dist] [Rank 0] step=2920, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.373447847366333\n",
            "[2021-01-30 08:56:58,862] [INFO] [timer.py:166:stop] 0/10480, SamplesPerSec=3.2292513842811843\n",
            "training loss: 4.396983134746551\n",
            "training loss: 4.325717496871948\n",
            "[2021-01-30 08:58:38,898] [INFO] [timer.py:166:stop] 0/10520, SamplesPerSec=3.2291352829746187\n",
            "training loss: 4.324425292015076\n",
            "training loss: 4.363570201396942\n",
            "[2021-01-30 09:00:15,457] [INFO] [timer.py:166:stop] 0/10560, SamplesPerSec=3.22944932548388\n",
            "training loss: 4.483875727653503\n",
            "training loss: 4.512449431419372\n",
            "[2021-01-30 09:01:57,406] [INFO] [timer.py:166:stop] 0/10600, SamplesPerSec=3.229098095984773\n",
            "training loss: 4.391984748840332\n",
            "training loss: 4.2821897029876705\n",
            "[2021-01-30 09:03:35,172] [INFO] [timer.py:166:stop] 0/10640, SamplesPerSec=3.2292619664254407\n",
            "training loss: 4.389058303833008\n",
            "training loss: 4.387883508205414\n",
            "[2021-01-30 09:05:17,281] [INFO] [timer.py:166:stop] 0/10680, SamplesPerSec=3.228894552839238\n",
            "training loss: 4.265194475650787\n",
            "training loss: 4.501848459243774\n",
            "[2021-01-30 09:07:01,855] [INFO] [timer.py:166:stop] 0/10720, SamplesPerSec=3.228230407095349\n",
            "training loss: 4.311777460575104\n",
            "training loss: 4.392610692977906\n",
            "[2021-01-30 09:08:33,399] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/language_model/pretrained/1-23662-4.429459011554718/mp_rank_00_model_states.pt\n",
            "[2021-01-30 09:08:48,655] [INFO] [timer.py:166:stop] 0/10760, SamplesPerSec=3.2279068006725598\n",
            "training loss: 4.4232746124267575\n",
            "[2021-01-30 09:09:28,078] [INFO] [logging.py:60:log_dist] [Rank 0] step=2960, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.242198967933655\n",
            "[2021-01-30 09:10:26,398] [INFO] [timer.py:166:stop] 0/10800, SamplesPerSec=3.2280754239421774\n",
            "training loss: 4.351160633563995\n",
            "\n",
            " validation loss: 4.473159432411194 \n",
            "\n",
            "training loss: 4.5490254163742065\n",
            "[2021-01-30 09:12:10,756] [INFO] [timer.py:166:stop] 0/10840, SamplesPerSec=3.2278969761893683\n",
            "training loss: 4.451187193393707\n",
            "training loss: 4.391882085800171\n",
            "[2021-01-30 09:13:51,427] [INFO] [timer.py:166:stop] 0/10880, SamplesPerSec=3.2277137598319676\n",
            "training loss: 4.412699496746063\n",
            "training loss: 4.535489356517791\n",
            "[2021-01-30 09:15:33,848] [INFO] [timer.py:166:stop] 0/10920, SamplesPerSec=3.2273233085614783\n",
            "training loss: 4.544868910312653\n",
            "training loss: 4.477988934516906\n",
            "[2021-01-30 09:17:22,513] [INFO] [timer.py:166:stop] 0/10960, SamplesPerSec=3.2261942418443867\n",
            "training loss: 4.368579292297364\n",
            "training loss: 4.334099876880646\n",
            "[2021-01-30 09:19:08,693] [INFO] [timer.py:166:stop] 0/11000, SamplesPerSec=3.225367877316809\n",
            "training loss: 4.414161324501038\n",
            "training loss: 4.384923887252808\n",
            "[2021-01-30 09:20:42,710] [INFO] [timer.py:166:stop] 0/11040, SamplesPerSec=3.2259808973170605\n",
            "training loss: 4.353687059879303\n",
            "training loss: 4.33148341178894\n",
            "[2021-01-30 09:22:20,254] [INFO] [timer.py:166:stop] 0/11080, SamplesPerSec=3.22617538675224\n",
            "training loss: 4.373153388500214\n",
            "[2021-01-30 09:22:56,803] [INFO] [logging.py:60:log_dist] [Rank 0] step=3000, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.411366415023804\n",
            "[2021-01-30 09:23:51,532] [INFO] [timer.py:166:stop] 0/11120, SamplesPerSec=3.227101932316318\n",
            "training loss: 4.3005645155906675\n",
            "training loss: 4.504636371135712\n",
            "[2021-01-30 09:25:30,135] [INFO] [timer.py:166:stop] 0/11160, SamplesPerSec=3.22716754339936\n",
            "training loss: 4.39109320640564\n",
            "training loss: 4.401311910152435\n",
            "[2021-01-30 09:27:05,330] [INFO] [timer.py:166:stop] 0/11200, SamplesPerSec=3.2276289515080654\n",
            "training loss: 4.300042259693146\n",
            "\n",
            " validation loss: 4.338464230298996 \n",
            "\n",
            "Can ya cook with the field Think I'm fallin' with an island don't nobody 'bout the streets Even though something You wanna layin' your old highway On the bay I tried to face Cause for you stand before the reason You know that ain't slow 'bout no sweatin' it You sure won't took the thunder When I saw the devils calling me You say that I stopped blowing that I felt this feeling Waitin' And that there Your words say no gold I love with the time You felt that fall When you take me inside your long Till I like rain I could never Long before the reason When we make you to feel like one thought that hate you Just wanna see you Oh take me Remember when I feel you'll hit you Your body holding me when I saw you 'til the reason I thought that pouring out When I feel the grey You turned green light hit the gift I think I thought of mine When I felt like a reason I saw that I saw you knew there beside you I could fallin' on the darkest hour before I hit the time Deep into the battle cry white clouds I understand You saw your eyes I love Feel the space I saw you standing Your lips of the reason I thought that I feel the reason that I felt the reason Like a reason I feel alright I felt so many times When the reason that felt like a reason that When you<|endoftext|>\n",
            "Nothing may never met Haven't never ever since we go and I've been thinking I take my way to you Or maybe I'm a smile Here in fear of you here with a place For me You don't believe that I cry But you're eyes Excuse me Catch me babe I'm a breathin' Baby babe or never been scared of the time We would it gets mad about a day We played a man Babe I see your sleepin' you You go Will be there but we used to be Sorry Goodbye baby We got the reason down falling outrun me Will you to hear my place We get me insane Said we're to save me If awake again If you Things aren't keep me crazy I could take off tonight See the reason to lay with me again But we held with me baby See I'm so much feelways Said I feel my baby No need my mind I'm falling More than miss you're times With your eyes Something called you to keep hanging around still look at the door I lost I guess I'm scared of life Just mystified With words and confused my face I'm crazy about my reasons but its been crazy how could be ashamed if I'm beginning tonight Baby I felt like you Seems to cryin' crazy I'm falling in love with you Tonight I'm where you Remember how we kid again I'm falling just can't we forgot about you anymore Just making plans Take me baby Tell me fallin' insane We should really don't you again We day With you Got up the pain I'm addicted I'll break away I keep falling Im falling Just to fallin' you Tomorrow and fall asleep and I'm fall in love again Where I fall asleep Dream about you feelin' for you baby baby I fell for you crawl back in love I'm falling in love again I'm falling in my baby Almost crashed you baby with you With you but I threw away We fallin' turns us But I'm falling in love you <|endoftext|>\n",
            "It is a one thing of one thing but the skies with one one line Yeah he's the line that he's only one hand on his clothes that he's ended It's the truth He's left for a one another one that he ran away And there's a man he's you baby He's arm in his arms so he makes you But he's his way he's just a married to take him Tell me tell you what prepared to you with Caught up his eye And yeah that it don't know he better with a man we're put this whole of his lover's all that the same Now you know he's get she's just a very closely She's hand You're the Lord's hand it's his son's hand up her mind He's hand and he's just one right hand out makin out in the time he's just a hand He's hand He's's the father makin' his girlfriend's hand himself up the finger in the hand his hand hand he's hand how we take a hang around to Then I know his hand ear it's hand hand hand hand hand hand hand yeah Oh one hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand man hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand you hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand dance hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand arm hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand hand\n",
            "It's way that's where does it happen in vain Left the bottle ends Don't stop dancing lips Brown Cancha see your eyes you get a dime So tonight While everybody wanna go up sing until we can watch you hear it show to party on We will last till we dance till it's dance until we can do Let's go Where do it burn it getcha gonna getcha left till the floor 'Cause you Got em'cha gonna take it all night before they gonna do it's go Putcha gonna take it all go don't deny it Getchacha yo head Getcha gotta docha time Party's do it's Workcha'll docha putcha no time gone Ok Babe you shake it all night Oh we just the way We won'tcha run we gonna docha gonna waste your seat we gonna dance And I'm really gonna getcha ready to bake nocha go getcha grab this all night long Party time party just ask you can't deny thatchachachacha number one Yeah now we can go getcha do getcha gonna becha Ischa ya'll taste like playinga docha dooby docha dance I gotta docha seen ya'd you wanted to andchacha Wcha gonna messcha'chaThis time we just for you I saidcha gonna playchachachachachachachachachachachachachachachacha youchachachachachachachachachacha getchachachachachachachacha soldierschachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachuchachachachachachachachachachachachachachachachachachachachachachacha babychachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachachacha\n",
            "training loss: 4.453476691246033\n",
            "[2021-01-30 09:29:45,386] [INFO] [timer.py:166:stop] 0/11240, SamplesPerSec=3.2268252334061924\n",
            "training loss: 4.404888272285461\n",
            "training loss: 4.339786553382874\n",
            "[2021-01-30 09:31:20,623] [INFO] [timer.py:166:stop] 0/11280, SamplesPerSec=3.2272796248792663\n",
            "training loss: 4.4457387089729306\n",
            "training loss: 4.411243057250976\n",
            "[2021-01-30 09:32:56,087] [INFO] [timer.py:166:stop] 0/11320, SamplesPerSec=3.2277049221876113\n",
            "training loss: 4.233992898464203\n",
            "training loss: 4.301407337188721\n",
            "[2021-01-30 09:34:31,829] [INFO] [timer.py:166:stop] 0/11360, SamplesPerSec=3.2280952916489376\n",
            "training loss: 4.481922245025634\n",
            "training loss: 4.409862756729126\n",
            "[2021-01-30 09:36:05,205] [INFO] [timer.py:166:stop] 0/11400, SamplesPerSec=3.2287536576509397\n",
            "training loss: 4.420371437072754\n",
            "[2021-01-30 09:36:47,267] [INFO] [logging.py:60:log_dist] [Rank 0] step=3040, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.449759101867675\n",
            "[2021-01-30 09:37:41,176] [INFO] [timer.py:166:stop] 0/11440, SamplesPerSec=3.2291118787557145\n",
            "training loss: 4.319043028354645\n",
            "training loss: 4.432309329509735\n",
            "[2021-01-30 09:39:36,224] [INFO] [timer.py:166:stop] 0/11480, SamplesPerSec=3.2273024088134865\n",
            "training loss: 4.441172039508819\n",
            "training loss: 4.38543541431427\n",
            "[2021-01-30 09:41:13,582] [INFO] [timer.py:166:stop] 0/11520, SamplesPerSec=3.2275060228266668\n",
            "training loss: 4.3536389708518985\n",
            "training loss: 4.320861601829529\n",
            "[2021-01-30 09:42:53,401] [INFO] [timer.py:166:stop] 0/11560, SamplesPerSec=3.227431194688533\n",
            "training loss: 4.460554778575897\n",
            "training loss: 4.241462361812592\n",
            "[2021-01-30 09:44:26,756] [INFO] [timer.py:166:stop] 0/11600, SamplesPerSec=3.228082548744501\n",
            "training loss: 4.258353734016419\n",
            "\n",
            " validation loss: 4.3372089803218845 \n",
            "\n",
            "training loss: 4.419365572929382\n",
            "[2021-01-30 09:46:04,004] [INFO] [timer.py:166:stop] 0/11640, SamplesPerSec=3.2287169425611295\n",
            "training loss: 4.367758250236511\n",
            "training loss: 4.3662078261375425\n",
            "[2021-01-30 09:47:45,314] [INFO] [timer.py:166:stop] 0/11680, SamplesPerSec=3.228472206950648\n",
            "training loss: 4.376983344554901\n",
            "training loss: 4.37545200586319\n",
            "[2021-01-30 09:49:31,381] [INFO] [timer.py:166:stop] 0/11720, SamplesPerSec=3.227700317610864\n",
            "training loss: 4.401719558238983\n",
            "[2021-01-30 09:50:18,243] [INFO] [logging.py:60:log_dist] [Rank 0] step=3080, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.398867225646972\n",
            "[2021-01-30 09:51:14,510] [INFO] [timer.py:166:stop] 0/11760, SamplesPerSec=3.227259377960319\n",
            "training loss: 4.253537118434906\n",
            "training loss: 4.294895911216736\n",
            "[2021-01-30 09:52:52,298] [INFO] [timer.py:166:stop] 0/11800, SamplesPerSec=3.227410853056639\n",
            "training loss: 4.412048506736755\n",
            "training loss: 4.344579124450684\n",
            "[2021-01-30 09:54:23,244] [INFO] [timer.py:166:stop] 0/11840, SamplesPerSec=3.228314182644528\n",
            "training loss: 4.295592308044434\n",
            "training loss: 4.408404541015625\n",
            "[2021-01-30 09:56:02,449] [INFO] [timer.py:166:stop] 0/11880, SamplesPerSec=3.2283057712223138\n",
            "training loss: 4.247231078147888\n",
            "training loss: 4.282088124752045\n",
            "[2021-01-30 09:57:44,455] [INFO] [timer.py:166:stop] 0/11920, SamplesPerSec=3.227991398307672\n",
            "training loss: 4.501157569885254\n",
            "training loss: 4.449496591091156\n",
            "[2021-01-30 09:59:30,278] [INFO] [timer.py:166:stop] 0/11960, SamplesPerSec=3.227263427357605\n",
            "training loss: 4.323052537441254\n",
            "training loss: 4.352462875843048\n",
            "[2021-01-30 10:01:08,568] [INFO] [timer.py:166:stop] 0/12000, SamplesPerSec=3.227357895022797\n",
            "training loss: 4.231108164787292\n",
            "\n",
            " validation loss: 4.61691455245018 \n",
            "\n",
            "Last time you look what you may meet When you want to the few times you talk to Once you tell me up stood and what might do what you've made do I do what any means to lonely And each other Me and you That's not new friends to do to just saying what you If I've had you'll do that I do You just want another friend again My friend always there's not blue blue blue I please you Oh x So I won't let blue friend Please help your friend to do Ev'ry one blue cold blue Whatever we'll leave you do you Oh blue eyes Well if I do to give you once upon you Oh blue sky so bluebird way My blue I don't can leave the other I'll make you Goodbye blue blue<|endoftext|>\n",
            "I'm the night love helpless willing to play the fire I'm right your love with a god of love desire Chorus I'm rocking like they rock and rocking these boys rocking rocking I'm rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking shaking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rock rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking It rocking rock rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking Rock rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking watching rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking A rocking rocking rocking rocking god rocking rocking rocking rocking rocking rocking rocking rocking right rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking losing rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking that rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking rocking Rock rocking rocking rocking rocking rocking\n",
            "I see ya These lives on my home without my side No hurt them I'm so far away The fall asleep lies inside my head I hate this and wrong you still here's alright It's no more try to surrender giving away No one up your life Don't deny you No one can't deny for you It's not your love Dont deny it No Don't deny the feelings inside you deny it No one knows how it's so good to So I deny you Sometimes you I hide your eyes Don't deny you deny it I deny it's no deny it No one knows the loneliness<|endoftext|>\n",
            "New Jersey Please give me words a case Another time to fall again Soul Break me break me on As fast as freeek Here come unrefine Hit me As far as the water was scared Here comes me As far gone He went nowhere As she ran away Here I come As far as she comes my fate bright as I go As far As the time As I wanna sing as he sings as I've opened my patience as a ticket As I need As I go As I run As I <|endoftext|>\n",
            "training loss: 4.308694326877594\n",
            "[2021-01-30 10:03:09,862] [INFO] [timer.py:166:stop] 0/12040, SamplesPerSec=3.2277622781636155\n",
            "training loss: 4.297510659694671\n",
            "[2021-01-30 10:03:51,282] [INFO] [logging.py:60:log_dist] [Rank 0] step=3120, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.425976181030274\n",
            "[2021-01-30 10:04:48,715] [INFO] [timer.py:166:stop] 0/12080, SamplesPerSec=3.2277937758935047\n",
            "training loss: 4.38917920589447\n",
            "training loss: 4.292124724388122\n",
            "[2021-01-30 10:06:31,029] [INFO] [timer.py:166:stop] 0/12120, SamplesPerSec=3.227453182143233\n",
            "training loss: 4.446211814880371\n",
            "training loss: 4.3471421003341675\n",
            "[2021-01-30 10:08:14,089] [INFO] [timer.py:166:stop] 0/12160, SamplesPerSec=3.227035058843775\n",
            "training loss: 4.321302354335785\n",
            "training loss: 4.3710222244262695\n",
            "[2021-01-30 10:09:52,235] [INFO] [timer.py:166:stop] 0/12200, SamplesPerSec=3.227144140096872\n",
            "training loss: 4.437497866153717\n",
            "training loss: 4.30209344625473\n",
            "[2021-01-30 10:11:43,414] [INFO] [timer.py:166:stop] 0/12240, SamplesPerSec=3.225866586954921\n",
            "training loss: 4.48088767528534\n",
            "training loss: 4.387430346012115\n",
            "[2021-01-30 10:13:18,033] [INFO] [timer.py:166:stop] 0/12280, SamplesPerSec=3.226352393731617\n",
            "training loss: 4.397847855091095\n",
            "training loss: 4.342566168308258\n",
            "[2021-01-30 10:14:53,973] [INFO] [timer.py:166:stop] 0/12320, SamplesPerSec=3.2266955348069004\n",
            "training loss: 4.272674238681793\n",
            "training loss: 4.3690593361854555\n",
            "[2021-01-30 10:16:29,042] [INFO] [timer.py:166:stop] 0/12360, SamplesPerSec=3.227128360791615\n",
            "training loss: 4.350435888767242\n",
            "[2021-01-30 10:17:11,086] [INFO] [logging.py:60:log_dist] [Rank 0] step=3160, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.298025238513946\n",
            "[2021-01-30 10:18:08,682] [INFO] [timer.py:166:stop] 0/12400, SamplesPerSec=3.227078546986531\n",
            "training loss: 4.414829695224762\n",
            "\n",
            " validation loss: 4.3959570825099945 \n",
            "\n",
            "training loss: 4.387842881679535\n",
            "[2021-01-30 10:19:54,825] [INFO] [timer.py:166:stop] 0/12440, SamplesPerSec=3.226742804439224\n",
            "training loss: 4.274322295188904\n",
            "training loss: 4.31905734539032\n",
            "[2021-01-30 10:21:35,571] [INFO] [timer.py:166:stop] 0/12480, SamplesPerSec=3.2265790649171633\n",
            "training loss: 4.321248638629913\n",
            "training loss: 4.326576340198517\n",
            "[2021-01-30 10:23:11,651] [INFO] [timer.py:166:stop] 0/12520, SamplesPerSec=3.2269015996967885\n",
            "training loss: 4.3101547956466675\n",
            "training loss: 4.28654248714447\n",
            "[2021-01-30 10:24:42,256] [INFO] [timer.py:166:stop] 0/12560, SamplesPerSec=3.2277897399478004\n",
            "training loss: 4.399268519878388\n",
            "training loss: 4.47825059890747\n",
            "[2021-01-30 10:26:25,785] [INFO] [timer.py:166:stop] 0/12600, SamplesPerSec=3.2273366349815635\n",
            "training loss: 4.383112215995789\n",
            "training loss: 4.3047664880752565\n",
            "[2021-01-30 10:28:00,012] [INFO] [timer.py:166:stop] 0/12640, SamplesPerSec=3.2278447428666945\n",
            "training loss: 4.391101861000061\n",
            "training loss: 4.425540566444397\n",
            "[2021-01-30 10:29:42,283] [INFO] [timer.py:166:stop] 0/12680, SamplesPerSec=3.2275234531285952\n",
            "training loss: 4.333972680568695\n",
            "[2021-01-30 10:30:27,129] [INFO] [logging.py:60:log_dist] [Rank 0] step=3200, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.463146591186524\n",
            "[2021-01-30 10:31:32,002] [INFO] [timer.py:166:stop] 0/12720, SamplesPerSec=3.2264419390264836\n",
            "training loss: 4.41853883266449\n",
            "training loss: 4.179449892044067\n",
            "[2021-01-30 10:33:13,937] [INFO] [timer.py:166:stop] 0/12760, SamplesPerSec=3.2261615808179327\n",
            "training loss: 4.394434916973114\n",
            "training loss: 4.208690190315247\n",
            "[2021-01-30 10:34:53,263] [INFO] [timer.py:166:stop] 0/12800, SamplesPerSec=3.2261482903294936\n",
            "training loss: 4.348695790767669\n",
            "\n",
            " validation loss: 4.151578831672668 \n",
            "\n",
            "But you don't want me here With a ghetto boys fling stones We were youngin bastard Darling dont give us something new names We dont blame in a heavenly peace Dont want to hear a child Dont you going to get over way Dont ask me anymore Dont ask me too Can you know me some money That needs me to me want me Give me to I want somebody to be in my bed Dont you Beg me to look at me And now dont sit down once more shot now Dont and take your head up with me why but one more Dont ask youre just some more ten more ask on my way Dont ever ask is shot If you ask me an ear Milly lady dont pump through the wrong dont ask me some more They hold you can I ask me now Dont ask me where you cant explain Dont you ask me just ask me Dont ask me that Dont ask me whats left me Please dont ask me what is popping me Ill ask me Dont ask me ask me what Ill tell me it anymore Please ask me Anyone Jesus why dont ask me ask me please dont ask me Please dont ask me to ask me why Dont ask me why<|endoftext|>\n",
            "Not even those who did you say saying but something Do you live like to the sweet sweetest song These words mean mean sister But you're much better working son to little bit to my friends And I give in you back And most weak These are ever love you Do simple and them to me Yes I know what they're doing to you're mum Listen to me What can you're doing enjoy Drinking sweetest and my song These shadows fails to my guestlistant I know Look through All my sweetest song How alone through their song <|endoftext|>\n",
            "I heard your number number one heard the drop drop Coming to the rain you love the drop the rain Let me take me take me take me rain In the rain Let me take you please Oh take me take me take that My fingers bleed me Oh take me please take me take me there This only a take me take the rain rain upon me When I take me in my oxygen Take I pound I take take the music in the wind Take me home I asked what I took you take take take Take take take take take take take take take take take take take where you there hot Take take take Take take take take<|endoftext|>\n",
            "Why can't carry you gonna carry me I don't believe what to bring me down and carry on sunny afternoon lamp I still living dead or glory and you can't carry me down to carry me off the railroad track or a time I could turn cold eternal death and carry a strong side to carry me far away carry straight and carry me away<|endoftext|>\n",
            "training loss: 4.303389108180999\n",
            "[2021-01-30 10:36:40,648] [INFO] [timer.py:166:stop] 0/12840, SamplesPerSec=3.2265891359055936\n",
            "training loss: 4.407035517692566\n",
            "training loss: 4.423855280876159\n",
            "[2021-01-30 10:38:21,228] [INFO] [timer.py:166:stop] 0/12880, SamplesPerSec=3.2264477843050527\n",
            "training loss: 4.3352187991142275\n",
            "training loss: 4.302977061271667\n",
            "[2021-01-30 10:39:26,020] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: drive/MyDrive/language_model/pretrained/1-25813-4.151578831672668/mp_rank_00_model_states.pt\n",
            "EPOCH: 2\n",
            "training loss: 4.402765274047852\n",
            "\n",
            " validation loss: 4.092034459114075 \n",
            "\n",
            "You stack my yard from the second worst Kill Tailgit cushions You won't turn around with my toe Hit me up witheredder give a list thats all summer on your crew Save me You are a gun Decided I forget i feared son It's turned around Emptons Chorus Boys Almation pinotusquotaked and pure Bill X Kick down your grocery store Spinnin your eyes down lets go round Representing their gun Into new pray Around Its your boys will be broken boyfriendsstil Dress upsets with an asshole cans Outlaw dream A cut by the guillation Violence breeds rage from the next manbangin round the yard Uncut Hit Me and silent and broke down your number and hoeveli Big spoons of a helping hand Get down come loaded gun Fight Horsaurus decked Every second year Get the worst times and return Salinas call your knots and they clutch spits Rochistic Fight Drinkin'em Wants become a current amount to play us Been weighed of time Pick em and fuck All of your last one Let your mote tear and u under em It'll be done with the Rained Pretty Kill an ice Everyday in your mad First act prey To Killers wont return Quick to the child gettin head I spent too clear Resisting bangin' raise em off your brain Tel Vps cuntssee You actin' in the cash Rips romped em Flashback and get down Falling Bow down Manzay uhh Cuz y'all bitches on the first born while you with every new bucks Its cool This is the cha wanna leadtin' the worst part Fuck how bout to nothing at smotchins and bang Trucks break Through the grandmothers Get em Wit' hoes my dough and believe your fast and lows Now we gon' get bad Ahhh get down Kick'd stuff Fast cars on r Simon Am I see Win a stackzibit bitches nlicks Verse Hail my an gringo we goin's weed from other girls The ckin' the othersnt mention Chorus Them kids Lickin'all lord cuz the get highs <|endoftext|>\n",
            "So sing your folks say on coats because I'm glad you're reaching for sure for long Nothing Would you struggle unless blood lust me in your bed and I'd be bought all because I wish I'm living in case Sure we'll be living in all us care if I'll be living in your own you Anything but I'm living right here If you see what I'm living right Nothing but your boys or s feels right Well if it feels good we'll be living right Cause right Nothing feels right Nothing but It feels right Nothing feels right Nothing feels right Nothing feels right Nothing feels right Nothing feels right Nothing right Nothing feels right Nothing right Nothing feels right Nothing feels Living life Nothing right Nothing feels right Nothing feels right Nothing right Nothing feels right Nothing feels right Nothing feels right Nothing right Nothing but right Nothing right right Nothing Nothing Nothing right right Nothing right Nothing feels right Nothing right Nothing Nothing all Nothing Nothing right Nothing right Nothing but right Nothing is right Nothing Nothing right Nothing Nothing Nothing Nothing right Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing right Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing I Nothing Nothing Nothing Oh Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing  Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing just<|endoftext|>\n",
            "Your temporary house all soft as nails My bones collapse to see your door Onward i penetrate Line Start to get wits i'm program a motherfucking whore wine the vicious slice of breath oh uh oh we'll go or hear you come and floor Thewire form dis i slide But i'm suppose a liar i'm hahine but i'm running into the end i got it's running out cause love i broke year old ohh yeah yeahhme to doin' Ohh these million miles i don't go separate comin' them fuck Well i'm running out of line we come running out of mine line when hell are made to killing everythang it run ohh ohh<|endoftext|>\n",
            "Can't you show me There's nothing that you love me I'm floating on mechanical overload Yeah Fire Fire like lightning Don't aeroplane machine Yeah she's like it's flowing through it's an overload Yeah yeah Like I'm on the light like a long Yeah yeahcause you It's you Yeah come on you can hardly when she do it's crazy Don't you Yeah Yeah she do her like a distraction yeah I'm thirsty just like the stormy Yeah they do it's crazy like you You think I'm sweeter You know I'm into the night like you And taking too much like Like I waste my satellites I'm using like the night like I got you like it like that She like You know your shuttle Yeah you Yeah yeah I'm like it Yeah she said she kinda like that Yeah She like you Yeah yeah I like it like it like me She like it's like you like it She like it I like Oh like like it Like yeah like like Yeah yeah Hey she like that I like like it like like it like it it<|endoftext|>\n",
            "[2021-01-30 10:40:45,864] [INFO] [timer.py:166:stop] 0/12920, SamplesPerSec=3.226476597363692\n",
            "training loss: 4.341912472248078\n",
            "training loss: 4.292597115039825\n",
            "[2021-01-30 10:42:20,905] [INFO] [timer.py:166:stop] 0/12960, SamplesPerSec=3.2268927497796596\n",
            "training loss: 4.349806547164917\n",
            "training loss: 4.259156250953675\n",
            "[2021-01-30 10:43:56,979] [INFO] [timer.py:166:stop] 0/13000, SamplesPerSec=3.227203092775742\n",
            "training loss: 4.366617727279663\n",
            "[2021-01-30 10:44:39,346] [INFO] [logging.py:60:log_dist] [Rank 0] step=3240, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.478179407119751\n",
            "[2021-01-30 10:45:44,049] [INFO] [timer.py:166:stop] 0/13040, SamplesPerSec=3.2264136833223893\n",
            "training loss: 4.340141153335571\n",
            "training loss: 4.319975197315216\n",
            "[2021-01-30 10:47:27,655] [INFO] [timer.py:166:stop] 0/13080, SamplesPerSec=3.225974111512317\n",
            "training loss: 4.221518182754517\n",
            "training loss: 4.405133748054505\n",
            "[2021-01-30 10:49:10,811] [INFO] [timer.py:166:stop] 0/13120, SamplesPerSec=3.225581968142008\n",
            "training loss: 4.49438647031784\n",
            "training loss: 4.271964979171753\n",
            "[2021-01-30 10:50:52,279] [INFO] [timer.py:166:stop] 0/13160, SamplesPerSec=3.2253590534527135\n",
            "training loss: 4.464478600025177\n",
            "training loss: 4.272636187076569\n",
            "[2021-01-30 10:52:35,322] [INFO] [timer.py:166:stop] 0/13200, SamplesPerSec=3.224982333006314\n",
            "training loss: 4.365691781044006\n",
            "training loss: 4.322328782081604\n",
            "[2021-01-30 10:54:14,968] [INFO] [timer.py:166:stop] 0/13240, SamplesPerSec=3.224941874459132\n",
            "training loss: 4.308234024047851\n",
            "training loss: 4.3068609714508055\n",
            "[2021-01-30 10:55:51,618] [INFO] [timer.py:166:stop] 0/13280, SamplesPerSec=3.225194656063346\n",
            "training loss: 4.368315732479095\n",
            "training loss: 4.345262634754181\n",
            "\n",
            " validation loss: 4.186881810426712 \n",
            "\n",
            "[2021-01-30 10:57:32,114] [INFO] [timer.py:166:stop] 0/13320, SamplesPerSec=3.2254369805571734\n",
            "training loss: 4.312834548950195\n",
            "[2021-01-30 10:58:07,404] [INFO] [logging.py:60:log_dist] [Rank 0] step=3280, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.341071331501007\n",
            "[2021-01-30 10:59:05,325] [INFO] [timer.py:166:stop] 0/13360, SamplesPerSec=3.2260217577530437\n",
            "training loss: 4.2832444190979\n",
            "training loss: 4.320658195018768\n",
            "[2021-01-30 11:00:37,461] [INFO] [timer.py:166:stop] 0/13400, SamplesPerSec=3.22670765643502\n",
            "training loss: 4.4704688549041744\n",
            "training loss: 4.368821942806244\n",
            "[2021-01-30 11:02:20,169] [INFO] [timer.py:166:stop] 0/13440, SamplesPerSec=3.2263658229092376\n",
            "training loss: 4.266625714302063\n",
            "training loss: 4.30718492269516\n",
            "[2021-01-30 11:03:57,895] [INFO] [timer.py:166:stop] 0/13480, SamplesPerSec=3.2265070388076444\n",
            "training loss: 4.340013742446899\n",
            "training loss: 4.462064099311829\n",
            "[2021-01-30 11:05:40,462] [INFO] [timer.py:166:stop] 0/13520, SamplesPerSec=3.2261814204007315\n",
            "training loss: 4.286994349956513\n",
            "training loss: 4.295309853553772\n",
            "[2021-01-30 11:07:22,931] [INFO] [timer.py:166:stop] 0/13560, SamplesPerSec=3.2258671469466385\n",
            "training loss: 4.3186093926429745\n",
            "training loss: 4.3141959547996525\n",
            "[2021-01-30 11:09:03,055] [INFO] [timer.py:166:stop] 0/13600, SamplesPerSec=3.2257792491218664\n",
            "training loss: 4.34355046749115\n",
            "training loss: 4.307759487628937\n",
            "[2021-01-30 11:10:44,698] [INFO] [timer.py:166:stop] 0/13640, SamplesPerSec=3.2255468828065994\n",
            "training loss: 4.342935085296631\n",
            "[2021-01-30 11:11:25,719] [INFO] [logging.py:60:log_dist] [Rank 0] step=3320, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "training loss: 4.40435882806778\n",
            "[2021-01-30 11:12:29,750] [INFO] [timer.py:166:stop] 0/13680, SamplesPerSec=3.224991836092247\n",
            "training loss: 4.322582757472992\n",
            "training loss: 4.328222095966339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdMQ2ZonR9FQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}