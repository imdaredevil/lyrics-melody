{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decoupled_single_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCYigWSRBRwy",
        "outputId": "73326335-0e15-41b1-d050-b3dd26ce864d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.4.0-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install deepspeed==0.3.10\n",
        "!pip install transformers\n",
        "!git clone https://github.com/gulnazaki/performer-pytorch.git\n",
        "!pip install ./performer-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.4.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-fast-transformers==0.4.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.4.0) (1.19.5)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.4.0\n",
            "Collecting deepspeed==0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bd/b2b544ca1286252e9a559b1508e64d0d61af7a73b6bf6737568858128e11/deepspeed-0.3.10.tar.gz (281kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (0.9.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 32.0MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed==0.3.10) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed==0.3.10) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed==0.3.10) (56.0.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.10-cp37-none-any.whl size=272622 sha256=da6e85734510405b3bffcb297173b8a054f8b357d8427f40b975d714ce545322\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/3c/9c/39a16330874a2c55f61fe2c501e120258975d509177ffdcda7\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.10 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 37.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Cloning into 'performer-pytorch'...\n",
            "remote: Enumerating objects: 523, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 523 (delta 64), reused 63 (delta 31), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (523/523), 35.02 MiB | 28.89 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n",
            "Processing ./performer-pytorch\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/44/55/140bd9670e1a64380162f49535a43af8ddb6e7fe61eb0983633bcd0c8294/local_attention-1.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (0.4.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (1.8.1+cu101)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: performer-pytorch, axial-positional-embedding\n",
            "  Building wheel for performer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for performer-pytorch: filename=performer_pytorch-0.15.0-cp37-none-any.whl size=12557 sha256=e8404fb0e822b47fa97a51812b05291527f741cd69ac48cd2f26e2e295b95f9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/73/93/041f7dd55e6f33ef90455a36e217ed2811faeb9dd9fe343159\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp37-none-any.whl size=2905 sha256=972ffbe8213b320467bd573b792edc60b4ea2f78ceb47bddc7926d685b17a4c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built performer-pytorch axial-positional-embedding\n",
            "Installing collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 local-attention-1.4.1 performer-pytorch-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAut8Lf5BW9N",
        "outputId": "1fbe4da5-def2-4955-aee0-889b39cebd02"
      },
      "source": [
        "%%writefile decoupled_performer.py\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from performer_pytorch.performer_pytorch import PerformerLM\n",
        "from performer_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n",
        "\n",
        "ENC_PREFIX = 'enc_'\n",
        "LM_PREFIX = 'lm_'\n",
        "DEC_PREFIX = 'dec_'\n",
        "\n",
        "def group_dict_by_key(cond, d):\n",
        "    return_val = [dict(),dict()]\n",
        "    for key in d.keys():\n",
        "        match = bool(cond(key))\n",
        "        ind = int(not match)\n",
        "        return_val[ind][key] = d[key]\n",
        "    return (*return_val,)\n",
        "\n",
        "def string_begins_with(prefix, str):\n",
        "    return bool(re.match(f'^{prefix}', str))\n",
        "\n",
        "def group_by_key_prefix(prefix, d):\n",
        "    return group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n",
        "\n",
        "def group_by_key_prefix_and_remove_prefix(prefix, d):\n",
        "    kwargs_with_prefix, kwargs = group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n",
        "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
        "    return kwargs_without_prefix, kwargs\n",
        "\n",
        "def extract_enc_lm_dec_kwargs(kwargs):\n",
        "    enc_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(ENC_PREFIX, kwargs)\n",
        "    lm_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(LM_PREFIX, kwargs)\n",
        "    dec_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(DEC_PREFIX, kwargs)\n",
        "    return enc_kwargs, lm_kwargs, dec_kwargs, kwargs\n",
        "\n",
        "def extract_and_set_enc_lm_dec_kwargs(kwargs):\n",
        "    enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_enc_lm_dec_kwargs(kwargs)\n",
        "    if 'mask' in enc_kwargs:\n",
        "        dec_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "    if 'mask' in lm_kwargs:\n",
        "        dec_kwargs.setdefault('second_context_mask', lm_kwargs['mask'][:, :-1])\n",
        "    return enc_kwargs, lm_kwargs, dec_kwargs, kwargs\n",
        "\n",
        "class DecoupledPerformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        tie_token_embeds = False,\n",
        "        no_projection = False,\n",
        "        pretrained_lm = \"\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, _ = extract_enc_lm_dec_kwargs(kwargs)\n",
        "        \n",
        "        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs and 'dim' not in lm_kwargs\n",
        "\n",
        "        enc_kwargs['dim'] = lm_kwargs['dim'] = dec_kwargs['dim'] = dim\n",
        "        enc_kwargs['no_projection'] = lm_kwargs['no_projection'] = dec_kwargs['no_projection'] = no_projection\n",
        "\n",
        "        lm_kwargs['causal'] = True\n",
        "        # cross attention has to be set explicitly\n",
        "        if not 'cross_attend' in lm_kwargs:\n",
        "            lm_kwargs['cross_attend'] = False\n",
        "        \n",
        "        self.lm_cross_attending = lm_kwargs['cross_attend']\n",
        "\n",
        "        dec_kwargs['causal'] = True\n",
        "        dec_kwargs['cross_attend'] = True\n",
        "        dec_kwargs['second_cross_attend'] = True\n",
        "\n",
        "        enc = PerformerLM(**enc_kwargs)\n",
        "        lm = PerformerLM(**lm_kwargs)\n",
        "        dec = PerformerLM(**dec_kwargs)\n",
        "\n",
        "        if tie_token_embeds:\n",
        "            enc.token_emb = lm.token_emb = dec.token_emb\n",
        "\n",
        "        self.enc = enc\n",
        "        if pretrained_lm:\n",
        "            pretrained = torch.load(pretrained_lm)\n",
        "            from collections import OrderedDict\n",
        "            new_pretrained = OrderedDict()\n",
        "            if lm_kwargs['reversible']:\n",
        "                if lm_kwargs['cross_attend']:\n",
        "                    for k, v in pretrained.items():\n",
        "                        if len(k.split('.')) >= 5:\n",
        "                            new_pretrained['performer.net.blocks.{}.{}'.format(int(k.split('.')[3])*2, k.split('.', 4)[-1])] = pretrained[k]\n",
        "                        else:\n",
        "                            new_pretrained[k] = pretrained[k]\n",
        "                else:\n",
        "                    new_pretrained = pretrained\n",
        "            else:\n",
        "                for k, v in pretrained.items():\n",
        "                    if len(k.split('.')) >= 5 and k.split('.')[4] == 'f':\n",
        "                        new_pretrained['performer.net.layers.{}.0.{}'.format(k.split('.')[3], k.split('.', 6)[-1])] = pretrained[k]\n",
        "                    elif len(k.split('.')) >= 5 and k.split('.')[4] == 'g':\n",
        "                        new_pretrained['performer.net.layers.{}.{}.{}'.format(k.split('.')[3], 2 if lm_kwargs['cross_attend'] else 1, k.split('.', 6)[-1])] = pretrained[k]\n",
        "                    else:\n",
        "                        new_pretrained[k] = pretrained[k]\n",
        "            lm.load_state_dict(new_pretrained, strict=False)\n",
        "            print(\"Loaded pretrained language model: {}\".format(pretrained_lm))\n",
        "        self.lm = AutoregressiveWrapper(lm)\n",
        "        self.dec = AutoregressiveWrapper(dec)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, instrumental, lyrics_start, lyrics_len, vocals_start, vocals_len, **kwargs):\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_and_set_enc_lm_dec_kwargs(kwargs)\n",
        "        instrumental_encodings = self.enc(instrumental, return_encodings = True, **enc_kwargs)\n",
        "        if self.lm_cross_attending:\n",
        "            lm_kwargs.setdefault('context', instrumental_encodings)\n",
        "            lm_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "        lyrics_encodings, lyrics = self.lm.generate(lyrics_start, lyrics_len, return_also_encodings = True, **{**lm_kwargs, **kwargs})\n",
        "        vocals = self.dec.generate(vocals_start, vocals_len, context = instrumental_encodings, second_context = lyrics_encodings, **{**dec_kwargs, **kwargs})\n",
        "        return lyrics, vocals\n",
        "\n",
        "    def forward(self, instrumental, lyrics, vocals, **kwargs):\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_and_set_enc_lm_dec_kwargs(kwargs)\n",
        "        instrumental_encodings = self.enc(instrumental, return_encodings = True, **enc_kwargs)\n",
        "        if self.lm_cross_attending:\n",
        "            lm_kwargs.setdefault('context', instrumental_encodings)\n",
        "            lm_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "        lyrics_encodings, lyrics_loss = self.lm(lyrics, return_also_encodings = True, **lm_kwargs)\n",
        "        vocals_loss = self.dec(vocals, context = instrumental_encodings, second_context = lyrics_encodings, **dec_kwargs)\n",
        "        return lyrics_loss + vocals_loss\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing decoupled_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE9HaQ_WBZww",
        "outputId": "addfeac4-4593-49bf-9fec-60304ed2f02b"
      },
      "source": [
        "%%writefile generate_decoupled.py\n",
        "\n",
        "from decoupled_performer import DecoupledPerformer\n",
        "import argparse\n",
        "import random\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import AutoTokenizer\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Generate vocals for single instrumental using the decoupled architecture')\n",
        "\n",
        "    parser.add_argument('--input', '-i', type=str, required=True,\n",
        "                        help='Input newline separated txt')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "\n",
        "    parser.add_argument('--pretrained-model', '-pm', type=str,\n",
        "                        help='Pretrained model to load')\n",
        "\n",
        "    parser.add_argument('--tokenizer', '-tok', type=str,\n",
        "                        help='Hugginface tokenizer to use')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, required=True,\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-instrumental-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer instrumental sequences')\n",
        "\n",
        "    parser.add_argument('--max-lyrics-sequence-length', '-maxl', type=int, default=1024,\n",
        "                        help='If provided it will truncate samples with longer lyrics sequences')\n",
        "    \n",
        "    parser.add_argument('--max-vocal-sequence-length', '-maxv', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer vocal melody sequences')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class DecoupledDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_instrumental_length, max_lyrics_length, max_vocal_length, tokenizer):\n",
        "        super().__init__()\n",
        "        instrumental_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.instrumental_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_instrumental_vocab = {l: w for w, l in self.instrumental_vocab.items()}\n",
        "            self.vocal_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_vocal_vocab = {l: w for w, l in self.vocal_vocab.items()}\n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, use_fast=True)\n",
        "          \n",
        "        with open(dataset_file, 'r') as f:\n",
        "            self.instrumental = self.encode([i.strip() for i in f.readlines()], seq_type='instrumental', max_length=max_instrumental_length).long()\n",
        "\n",
        "        self.max_instrumental_length = max_instrumental_length\n",
        "        self.max_lyrics_length = max_lyrics_length\n",
        "        self.max_vocal_length = max_vocal_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instrumental[index], self.lyrics[index], self.vocals[index]), self.files[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def truncate(self, sequence, max_length):\n",
        "        if max_length >= 0:\n",
        "            return sequence[:max_length]\n",
        "        return sequence\n",
        "\n",
        "    def encode(self, event_sequence, seq_type, max_length=-1, max_syllables=-1):\n",
        "        if seq_type == 'instrumental':\n",
        "            return torch.tensor([self.instrumental_vocab[e] for e in self.truncate(event_sequence, max_length - 1)] + [self.instrumental_vocab['<eos>']])\n",
        "        elif seq_type == 'lyrics':\n",
        "            tokenized = self.tokenizer(''.join(event_sequence), max_length=max_length - 2, truncation=True, return_overflowing_tokens=True)\n",
        "            if len(tokenized.encodings) == 2:\n",
        "                last_word_index = tokenized[0].word_ids[-1]\n",
        "                if last_word_index == tokenized[1].word_ids[0]:\n",
        "                    tokens = [tokenized[0].tokens[i] for i in range(len(tokenized[0])) if tokenized[0].word_ids[i] < last_word_index]\n",
        "                else:\n",
        "                    tokens = tokenized[0].tokens\n",
        "                size = len(self.tokenizer.convert_tokens_to_string(tokens).strip())\n",
        "                max_syllables = 0\n",
        "                chars = 0\n",
        "                for l in event_sequence:\n",
        "                    chars += len(l)\n",
        "                    if chars > size:\n",
        "                        break\n",
        "                    max_syllables += 1                \n",
        "                ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            else:\n",
        "                ids = tokenized[0].ids\n",
        "            return torch.tensor([self.tokenizer.bos_token_id] + ids + [self.tokenizer.eos_token_id]), max_syllables\n",
        "        else:\n",
        "            if max_syllables >= 0:\n",
        "                last_index = -1\n",
        "                syllables = 0\n",
        "                for i, e in enumerate(event_sequence):\n",
        "                    if '_' not in e:\n",
        "                        syllables += 1\n",
        "                        if syllables > max_syllables:\n",
        "                            last_index = i\n",
        "                            break\n",
        "                if last_index >= 0:\n",
        "                    event_sequence = event_sequence[:last_index]\n",
        "            return torch.tensor([self.vocal_vocab['<bos>']] + [self.vocal_vocab[e] for e in self.truncate([e for e in event_sequence if '_' in e], max_length - 2)] + [self.vocal_vocab['<eos>']])\n",
        "\n",
        "    def decode(self, event_sequence, seq_type, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "            true_size = size\n",
        "        if seq_type == 'instrumental':\n",
        "            return [self.reverse_instrumental_vocab[i.item()] for i in event_sequence[:true_size]]\n",
        "        elif seq_type == 'lyrics':\n",
        "            return self.tokenizer.decode(event_sequence[:true_size])\n",
        "        else:\n",
        "            return [self.reverse_vocal_vocab[o.item()] for o in event_sequence[:true_size]]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = DecoupledDataset(dataset_file=args.input,\n",
        "                               monophonic=args.monophonic,\n",
        "                               vocabulary_prefix=args.vocabulary_prefix,\n",
        "                               max_instrumental_length=args.max_instrumental_sequence_length,\n",
        "                               max_lyrics_length=args.max_lyrics_sequence_length,\n",
        "                               max_vocal_length=args.max_vocal_sequence_length,\n",
        "                               tokenizer=args.tokenizer)\n",
        "    \n",
        "    model = DecoupledPerformer(\n",
        "        dim = 768,\n",
        "        enc_heads = 6,\n",
        "        lm_heads = 12,\n",
        "        dec_heads = 6,\n",
        "        enc_depth = 6,\n",
        "        lm_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_ff_chunks = 10,\n",
        "        lm_ff_chunks = 1,\n",
        "        dec_ff_chunks = 10,\n",
        "        enc_num_tokens = len(dataset.instrumental_vocab),\n",
        "        lm_num_tokens = len(dataset.tokenizer),\n",
        "        dec_num_tokens = len(dataset.vocal_vocab),\n",
        "        enc_max_seq_len = dataset.max_instrumental_length,\n",
        "        lm_max_seq_len = args.max_lyrics_sequence_length,\n",
        "        dec_max_seq_len = dataset.max_vocal_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        lm_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        lm_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        lm_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_tie_embed = True,\n",
        "        lm_tie_embed = True,\n",
        "        dec_tie_embed = True,\n",
        "        enc_reversible = True,\n",
        "        lm_reversible = True,\n",
        "        dec_reversible = True,\n",
        "        # pretrained_lm = args.pretrained_language_model,\n",
        "        # lm_cross_attend = True\n",
        "    ).to(device)\n",
        "\n",
        "    def valid_events(vocab, previous):\n",
        "        if all(previous < 0):\n",
        "            valid_events.waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "            valid_events.ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "            valid_events.offs = [vocab['_OFF_']]\n",
        "            valid_events.boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "            valid_events.phonemes = [0, 1, 2] + [vocab['_R_']]\n",
        "            valid_events.notes_on = torch.tensor([False]).expand(previous.size(0), -1)\n",
        "            return torch.tensor(valid_events.waits + valid_events.ons + valid_events.boundaries).expand(previous.size(0), -1).to(device)\n",
        "        else:\n",
        "            valids = []\n",
        "            for i, p in enumerate(previous):\n",
        "                if p == valid_events.waits[-1]:\n",
        "                    v = valid_events.waits + (valid_events.offs if valid_events.notes_on[i] else valid_events.boundaries + valid_events.phonemes + valid_events.ons)\n",
        "                elif p in valid_events.waits:\n",
        "                    v = valid_events.offs if valid_events.notes_on[i] else valid_events.boundaries + valid_events.phonemes + valid_events.ons\n",
        "                elif p in valid_events.ons:\n",
        "                    valid_events.notes_on[i] = True\n",
        "                    v = valid_events.waits\n",
        "                elif p in valid_events.offs:\n",
        "                    valid_events.notes_on[i] = False\n",
        "                    v = valid_events.waits + valid_events.boundaries + valid_events.phonemes + valid_events.ons\n",
        "                elif p in valid_events.boundaries:\n",
        "                    if p == valid_events.boundaries[-1]:\n",
        "                        v = valid_events.boundaries[:-1] + valid_events.phonemes + valid_events.ons\n",
        "                    else:\n",
        "                        v = valid_events.phonemes + valid_events.ons\n",
        "                else:\n",
        "                    v = valid_events.ons\n",
        "                valids.append(v)\n",
        "            return torch.tensor(valids).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(args.pretrained_model))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\"Let's go!\")\n",
        "        constrain_fn = partial(valid_events, dataset.vocal_vocab)\n",
        "        start_time = time.time()\n",
        "        instrumental = dataset.instrumental.view(1,-1)\n",
        "            \n",
        "        # <bos> token\n",
        "        vocals_start = torch.ones(1,1).long()\n",
        "        lyrics_start = torch.full((1,1), dataset.tokenizer.bos_token_id).long()\n",
        "\n",
        "        lyrics, vocals = model.generate(instrumental=instrumental.to(device),\n",
        "                                                      lyrics_start=lyrics_start.to(device),\n",
        "                                                      lyrics_len=args.max_lyrics_sequence_length,\n",
        "                                                      vocals_start=vocals_start.to(device),\n",
        "                                                      vocals_len=dataset.max_vocal_length,\n",
        "                                                      # enc_mask=instrumental_mask.to(device),\n",
        "                                                      lm_eos_token=dataset.tokenizer.eos_token_id,\n",
        "                                                      dec_eos_token=2,\n",
        "                                                      dec_constrain_fn=constrain_fn)\n",
        "        decoded_lyrics = dataset.decode(lyrics[0], seq_type='lyrics')\n",
        "        decoded_vocals = dataset.decode(vocals[0], seq_type='vocals')\n",
        "\n",
        "        # print((time.time() - start_time)/(len(decoded_vocals)+len(lyrics[0])))\n",
        "        with open(os.path.join(args.save_dir, 'single_instrumentals.txt'), 'a') as f:\n",
        "            f.write(\"{}\\n----------------\\n{}\\n----------------\\n\\n\"\\\n",
        "                            .format(decoded_lyrics, decoded_vocals))\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing generate_decoupled.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4SSE2LQBcDe",
        "outputId": "78961572-0d75-4e98-d352-e1cb0df7a325"
      },
      "source": [
        "!python3 generate_decoupled.py -i drive/MyDrive/decoupled_performer/eurovision/euromidi_chords.txt -v drive/MyDrive/decoupled_performer/decoupled_chords_ -pm drive/MyDrive/decoupled_performer/chords/model.pt -tok distilgpt2 -sd drive/MyDrive/decoupled_performer/eurovision -maxi 11731 -maxl 1024 -maxv 4816"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: 100% 762/762 [00:00<00:00, 687kB/s]\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.61MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.39MB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 3.32MB/s]\n",
            "Let's go!\n",
            "2021-05-05 18:21:56.709118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "0.04677457231223781\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}