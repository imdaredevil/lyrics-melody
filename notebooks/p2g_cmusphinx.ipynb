{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p2g-cmusphinx.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbsgcsPANUJ3",
        "outputId": "46647259-5950-443e-9eeb-8fcbfd632267"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%tensorflow_version 1.x\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install tensorflow-gpu==1.13.1\n",
        "!pip install tensor2tensor==1.6.6\n",
        "!git clone https://github.com/cmusphinx/g2p-seq2seq.git\n",
        "!wget https://raw.githubusercontent.com/cmusphinx/cmudict/master/cmudict.dict"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Collecting tensorflow==1.13.1\n",
            "  Using cached https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (54.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorflow-1.13.1\n",
            "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.4.0)\n",
            "Requirement already satisfied: tensor2tensor==1.6.6 in /usr/local/lib/python3.7/dist-packages (1.6.6)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (1.7.12)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (2.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (1.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (0.16.0)\n",
            "Requirement already satisfied: bz2file in /tensorflow-1.15.2/python3.7 (from tensor2tensor==1.6.6) (0.98)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (4.41.1)\n",
            "Requirement already satisfied: gunicorn in /tensorflow-1.15.2/python3.7 (from tensor2tensor==1.6.6) (20.0.4)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (0.17.3)\n",
            "Requirement already satisfied: gevent in /tensorflow-1.15.2/python3.7 (from tensor2tensor==1.6.6) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (2.23.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (1.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensor2tensor==1.6.6) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor==1.6.6) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor==1.6.6) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor==1.6.6) (0.17.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor==1.6.6) (4.7.2)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor==1.6.6) (1.27.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor==1.6.6) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor==1.6.6) (0.0.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor==1.6.6) (1.2.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->tensor2tensor==1.6.6) (54.0.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor==1.6.6) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor==1.6.6) (1.3.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /tensorflow-1.15.2/python3.7 (from gevent->tensor2tensor==1.6.6) (0.4.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor==1.6.6) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor==1.6.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor==1.6.6) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor==1.6.6) (2020.12.5)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor==1.6.6) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor==1.6.6) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor==1.6.6) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor==1.6.6) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor==1.6.6) (4.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor==1.6.6) (1.1.1)\n",
            "fatal: destination path 'g2p-seq2seq' already exists and is not an empty directory.\n",
            "--2021-03-05 14:03:57--  https://raw.githubusercontent.com/cmusphinx/cmudict/master/cmudict.dict\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3618063 (3.5M) [text/plain]\n",
            "Saving to: ‘cmudict.dict.1’\n",
            "\n",
            "cmudict.dict.1      100%[===================>]   3.45M   995KB/s    in 3.5s    \n",
            "\n",
            "2021-03-05 14:04:04 (995 KB/s) - ‘cmudict.dict.1’ saved [3618063/3618063]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dBVlgNMazM_",
        "outputId": "a23d5fb8-c41b-4767-ed16-c0e483f5baad"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Mar  5 14:03:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBrkRFKUbcK3",
        "outputId": "ecae985c-47cf-4b5f-9784-b00658034066"
      },
      "source": [
        "%%writefile g2p-seq2seq/g2p_seq2seq/g2p.py\n",
        "\n",
        "# Copyright 2016 AC Technologies LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Binary for training translation models and decoding from them.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import contextlib\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "import sys\n",
        "\n",
        "from tensor2tensor.data_generators.problem import problem_hparams_to_features\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.estimator import estimator as estimator_lib\n",
        "from tensorflow.python.framework import graph_util\n",
        "\n",
        "# Dependency imports\n",
        "\n",
        "from tensor2tensor import models # pylint: disable=unused-import\n",
        "\n",
        "from g2p_seq2seq import g2p_problem\n",
        "from g2p_seq2seq import g2p_trainer_utils\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import usr_dir\n",
        "from tensor2tensor.utils import decoding\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "from six.moves import input\n",
        "from six import text_type\n",
        "import json\n",
        "\n",
        "EOS = text_encoder.EOS\n",
        "\n",
        "\n",
        "class G2PModel(object):\n",
        "  \"\"\"Grapheme-to-Phoneme translation model class.\n",
        "  \"\"\"\n",
        "  def __init__(self, params, train_path=\"\", dev_path=\"\", test_path=\"\",\n",
        "               cleanup=False, p2g_mode=False):\n",
        "    # Point out the current directory with t2t problem specified for g2p task.\n",
        "    usr_dir.import_usr_dir(os.path.dirname(os.path.abspath(__file__)))\n",
        "    self.p2g_mode = p2g_mode\n",
        "    self.params = params\n",
        "    self.test_path = test_path\n",
        "    if not os.path.exists(self.params.model_dir):\n",
        "      os.makedirs(self.params.model_dir)\n",
        "\n",
        "    # Register g2p problem.\n",
        "    self.problem = registry._PROBLEMS[self.params.problem_name](\n",
        "        self.params.model_dir, train_path=train_path, dev_path=dev_path,\n",
        "        test_path=test_path, cleanup=cleanup, p2g_mode=p2g_mode)\n",
        "\n",
        "    self.frozen_graph_filename = os.path.join(self.params.model_dir,\n",
        "                                              \"frozen_model.pb\")\n",
        "    self.inputs, self.features, self.input_fn = None, None, None\n",
        "    self.mon_sess, self.estimator_spec, self.g2p_gt_map = None, None, None\n",
        "    self.first_ex = False\n",
        "    if train_path:\n",
        "      self.train_preprocess_file_path, self.dev_preprocess_file_path =\\\n",
        "          None, None\n",
        "      self.estimator, self.decode_hp, self.hparams =\\\n",
        "          self.__prepare_model(train_mode=True)\n",
        "      self.train_preprocess_file_path, self.dev_preprocess_file_path =\\\n",
        "          self.problem.generate_preprocess_data()\n",
        "\n",
        "    elif os.path.exists(self.frozen_graph_filename):\n",
        "      self.estimator, self.decode_hp, self.hparams =\\\n",
        "          self.__prepare_model()\n",
        "      self.__load_graph()\n",
        "      self.checkpoint_path = tf.train.latest_checkpoint(self.params.model_dir)\n",
        "\n",
        "    else:\n",
        "      self.estimator, self.decode_hp, self.hparams =\\\n",
        "          self.__prepare_model()\n",
        "\n",
        "  def __prepare_model(self, train_mode=False):\n",
        "    \"\"\"Prepare utilities for decoding.\"\"\"\n",
        "    hparams = registry.hparams(self.params.hparams_set)\n",
        "    hparams.problem = self.problem\n",
        "    hparams.problem_hparams = self.problem.get_hparams(hparams)\n",
        "    if self.params.hparams:\n",
        "      tf.logging.info(\"Overriding hparams in %s with %s\",\n",
        "                      self.params.hparams_set,\n",
        "                      self.params.hparams)\n",
        "      hparams = hparams.parse(self.params.hparams)\n",
        "    trainer_run_config = g2p_trainer_utils.create_run_config(hparams,\n",
        "        self.params)\n",
        "    if train_mode:\n",
        "      exp_fn = g2p_trainer_utils.create_experiment_fn(self.params, self.problem)\n",
        "      self.exp = exp_fn(trainer_run_config, hparams)\n",
        "\n",
        "    decode_hp = decoding.decode_hparams(self.params.decode_hparams)\n",
        "    estimator = trainer_lib.create_estimator(\n",
        "        self.params.model_name,\n",
        "        hparams,\n",
        "        trainer_run_config,\n",
        "        decode_hparams=decode_hp,\n",
        "        use_tpu=False)\n",
        "\n",
        "    return estimator, decode_hp, hparams\n",
        "\n",
        "  def __prepare_interactive_model(self):\n",
        "    \"\"\"Create monitored session and generator that reads from the terminal and\n",
        "    yields \"interactive inputs\".\n",
        "\n",
        "    Due to temporary limitations in tf.learn, if we don't want to reload the\n",
        "    whole graph, then we are stuck encoding all of the input as one fixed-size\n",
        "    numpy array.\n",
        "\n",
        "    We yield int32 arrays with shape [const_array_size].  The format is:\n",
        "    [num_samples, decode_length, len(input ids), <input ids>, <padding>]\n",
        "\n",
        "    Raises:\n",
        "      ValueError: Could not find a trained model in model_dir.\n",
        "      ValueError: if batch length of predictions are not same.\n",
        "    \"\"\"\n",
        "\n",
        "    def input_fn():\n",
        "      \"\"\"Input function returning features which is a dictionary of\n",
        "        string feature name to `Tensor` or `SparseTensor`. If it returns a\n",
        "        tuple, first item is extracted as features. Prediction continues until\n",
        "        `input_fn` raises an end-of-input exception (`OutOfRangeError` or\n",
        "        `StopIteration`).\"\"\"\n",
        "      gen_fn = decoding.make_input_fn_from_generator(\n",
        "          self.__interactive_input_fn())\n",
        "      example = gen_fn()\n",
        "      example = decoding._interactive_input_tensor_to_features_dict(\n",
        "          example, self.hparams)\n",
        "      return example\n",
        "\n",
        "    self.res_iter = self.estimator.predict(input_fn)\n",
        "\n",
        "    if os.path.exists(self.frozen_graph_filename):\n",
        "      return\n",
        "\n",
        "    # List of `SessionRunHook` subclass instances. Used for callbacks inside\n",
        "    # the prediction call.\n",
        "    hooks = estimator_lib._check_hooks_type(None)\n",
        "\n",
        "    # Check that model has been trained.\n",
        "    # Path of a specific checkpoint to predict. The latest checkpoint\n",
        "    # in `model_dir` is used\n",
        "    checkpoint_path = estimator_lib.saver.latest_checkpoint(\n",
        "        self.params.model_dir)\n",
        "    if not checkpoint_path:\n",
        "      raise ValueError('Could not find trained model in model_dir: {}.'\n",
        "                       .format(self.params.model_dir))\n",
        "\n",
        "    with estimator_lib.ops.Graph().as_default() as graph:\n",
        "\n",
        "      estimator_lib.random_seed.set_random_seed(\n",
        "          self.estimator._config.tf_random_seed)\n",
        "      self.estimator._create_and_assert_global_step(graph)\n",
        "\n",
        "      self.features, input_hooks = self.estimator._get_features_from_input_fn(\n",
        "          input_fn, estimator_lib.model_fn_lib.ModeKeys.PREDICT)\n",
        "      self.estimator_spec = self.estimator._call_model_fn(\n",
        "          self.features, None, estimator_lib.model_fn_lib.ModeKeys.PREDICT,\n",
        "          self.estimator.config)\n",
        "      try:\n",
        "        self.mon_sess = estimator_lib.training.MonitoredSession(\n",
        "            session_creator=estimator_lib.training.ChiefSessionCreator(\n",
        "                checkpoint_filename_with_path=checkpoint_path,\n",
        "                scaffold=self.estimator_spec.scaffold,\n",
        "                config=self.estimator._session_config),\n",
        "            hooks=hooks)\n",
        "      except:\n",
        "        raise StandardError(\"Invalid model in {}\".format(self.params.model_dir))\n",
        "\n",
        "  def decode_word(self, word):\n",
        "    \"\"\"Decode word.\n",
        "\n",
        "    Args:\n",
        "      word: word for decoding.\n",
        "\n",
        "    Returns:\n",
        "      pronunciation: a decoded phonemes sequence for input word.\n",
        "    \"\"\"\n",
        "    num_samples = 1\n",
        "    decode_length = 100\n",
        "    vocabulary = self.problem.source_vocab\n",
        "    # This should be longer than the longest input.\n",
        "    const_array_size = 10000\n",
        "\n",
        "    input_ids = vocabulary.encode(word)\n",
        "    input_ids.append(text_encoder.EOS_ID)\n",
        "    self.inputs = [num_samples, decode_length, len(input_ids)] + input_ids\n",
        "    assert len(self.inputs) < const_array_size\n",
        "    self.inputs += [0] * (const_array_size - len(self.inputs))\n",
        "\n",
        "    result = next(self.res_iter)\n",
        "    pronunciations = []\n",
        "    if self.decode_hp.return_beams:\n",
        "      beams = np.split(result[\"outputs\"], self.decode_hp.beam_size, axis=0)\n",
        "      for k, beam in enumerate(beams):\n",
        "        tf.logging.info(\"BEAM %d:\" % k)\n",
        "        beam_string = self.problem.target_vocab.decode(\n",
        "            decoding._save_until_eos(beam, is_image=False))\n",
        "        pronunciations.append(beam_string)\n",
        "        tf.logging.info(beam_string)\n",
        "    else:\n",
        "      if self.decode_hp.identity_output:\n",
        "        tf.logging.info(\" \".join(map(str, result[\"outputs\"].flatten())))\n",
        "      else:\n",
        "        res = result[\"outputs\"].flatten()\n",
        "        if text_encoder.EOS_ID in res:\n",
        "          index = list(res).index(text_encoder.EOS_ID)\n",
        "          res = res[0:index]\n",
        "        pronunciations.append(self.problem.target_vocab.decode(res))\n",
        "    return pronunciations\n",
        "\n",
        "  def __interactive_input_fn(self):\n",
        "    num_samples = self.decode_hp.num_samples if self.decode_hp.num_samples > 0\\\n",
        "        else 1\n",
        "    decode_length = self.decode_hp.extra_length\n",
        "    input_type = \"text\"\n",
        "    p_hparams = self.hparams.problem_hparams\n",
        "    has_input = \"inputs\" in p_hparams.input_modality\n",
        "    vocabulary = p_hparams.vocabulary[\"inputs\" if has_input else \"targets\"]\n",
        "    # Import readline if available for command line editing and recall.\n",
        "    try:\n",
        "      import readline  # pylint: disable=g-import-not-at-top,unused-variable\n",
        "    except ImportError:\n",
        "      pass\n",
        "    while True:\n",
        "      features = {\n",
        "          \"inputs\": np.array(self.inputs).astype(np.int32),\n",
        "      }\n",
        "      for k, v in six.iteritems(problem_hparams_to_features(p_hparams)):\n",
        "        features[k] = np.array(v).astype(np.int32)\n",
        "      yield features\n",
        "\n",
        "  def __run_op(self, sess, decode_op, feed_input):\n",
        "    \"\"\"Run tensorflow operation for decoding.\"\"\"\n",
        "    results = sess.run(decode_op,\n",
        "                       feed_dict={\"inp_decode:0\" : [feed_input]})\n",
        "    return results\n",
        "\n",
        "  def train(self):\n",
        "    \"\"\"Run training.\"\"\"\n",
        "    print('Training started.')\n",
        "    execute_schedule(self.exp, self.params)\n",
        "\n",
        "  def interactive(self):\n",
        "    \"\"\"Interactive decoding.\"\"\"\n",
        "    self.inputs = []\n",
        "    self.__prepare_interactive_model()\n",
        "\n",
        "    if os.path.exists(self.frozen_graph_filename):\n",
        "      with tf.Session(graph=self.graph) as sess:\n",
        "        saver = tf.train.import_meta_graph(self.checkpoint_path + \".meta\",\n",
        "                                           import_scope=None,\n",
        "                                           clear_devices=True)\n",
        "        saver.restore(sess, self.checkpoint_path)\n",
        "        inp = tf.placeholder(tf.string, name=\"inp_decode\")[0]\n",
        "        decode_op = tf.py_func(self.decode_word, [inp], tf.string)\n",
        "        while True:\n",
        "          word = get_word()\n",
        "          pronunciations = self.__run_op(sess, decode_op, word)\n",
        "          print (\" \".join(pronunciations))\n",
        "    else:\n",
        "      while not self.mon_sess.should_stop():\n",
        "        word = get_word()\n",
        "        pronunciations = self.decode_word(word)\n",
        "        print(\" \".join(pronunciations))\n",
        "        # To make sure the output buffer always flush at this level\n",
        "        sys.stdout.flush()\n",
        "\n",
        "  def decode(self, output_file_path):\n",
        "    \"\"\"Run decoding mode.\"\"\"\n",
        "    if os.path.exists(self.frozen_graph_filename):\n",
        "      with tf.Session(graph=self.graph) as sess:\n",
        "        inp = tf.placeholder(tf.string, name=\"inp_decode\")[0]\n",
        "        decode_op = tf.py_func(self.__decode_from_file, [inp],\n",
        "                               [tf.string, tf.string])\n",
        "        [inputs, decodes] = self.__run_op(sess, decode_op, self.test_path)\n",
        "    else:\n",
        "      outfile = None\n",
        "      # If path to the output file pointed out, dump decoding results to the file\n",
        "      if output_file_path:\n",
        "        tf.logging.info(\"Writing decodes into %s\" % output_file_path)\n",
        "        outfile = tf.gfile.Open(output_file_path, \"w\")\n",
        "\n",
        "      inputs, decodes = self.__decode_from_file(self.test_path, outfile)\n",
        "\n",
        "  def evaluate(self):\n",
        "    \"\"\"Run evaluation mode.\"\"\"\n",
        "    words, pronunciations = [], []\n",
        "    for case in self.problem.generator(self.test_path,\n",
        "                                       self.problem.source_vocab,\n",
        "                                       self.problem.target_vocab):\n",
        "      word = self.problem.source_vocab.decode(case[\"inputs\"]).replace(\n",
        "          EOS, \"\").strip()\n",
        "      pronunciation = self.problem.target_vocab.decode(case[\"targets\"]).replace(\n",
        "          EOS, \"\").strip()\n",
        "      words.append(word)\n",
        "      pronunciations.append(pronunciation)\n",
        "\n",
        "    self.g2p_gt_map = create_g2p_gt_map(words, pronunciations)\n",
        "\n",
        "    if os.path.exists(self.frozen_graph_filename):\n",
        "      with tf.Session(graph=self.graph) as sess:\n",
        "        inp = tf.placeholder(tf.string, name=\"inp_decode\")[0]\n",
        "        decode_op = tf.py_func(self.calc_errors, [inp], [tf.int64, tf.int64])\n",
        "        [correct, errors] = self.__run_op(sess, decode_op, self.test_path)\n",
        "\n",
        "    else:\n",
        "      correct, errors = self.calc_errors(self.test_path)\n",
        "\n",
        "    print(\"Words: %d\" % (correct+errors))\n",
        "    print(\"Errors: %d\" % errors)\n",
        "    print(\"WER: %.3f\" % (float(errors)/(correct+errors)))\n",
        "    print(\"Accuracy: %.3f\" % float(1.-(float(errors)/(correct+errors))))\n",
        "\n",
        "  def freeze(self):\n",
        "    \"\"\"Freeze pre-trained model.\"\"\"\n",
        "    # We retrieve our checkpoint fullpath\n",
        "    checkpoint = tf.train.get_checkpoint_state(self.params.model_dir)\n",
        "    input_checkpoint = checkpoint.model_checkpoint_path\n",
        "\n",
        "    # We precise the file fullname of our freezed graph\n",
        "    absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\n",
        "    output_graph = absolute_model_folder + \"/frozen_model.pb\"\n",
        "\n",
        "    # Before exporting our graph, we need to precise what is our output node\n",
        "    # This is how TF decides what part of the Graph he has to keep and what\n",
        "    # part it can dump\n",
        "    # NOTE: this variable is plural, because you can have multiple output nodes\n",
        "    output_node_names = [\"transformer/parallel_0_4/transformer/transformer/body/encoder/\"\n",
        "        \"layer_0/self_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/encoder/\"\n",
        "        \"layer_1/self_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/encoder/\"\n",
        "        \"layer_2/self_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/decoder/\"\n",
        "        \"layer_0/self_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/decoder/\"\n",
        "        \"layer_0/encdec_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/decoder/\"\n",
        "        \"layer_1/self_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/decoder/\"\n",
        "        \"layer_1/encdec_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/decoder/\"\n",
        "        \"layer_2/self_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\",\n",
        "        \"transformer/parallel_0_4/transformer/transformer/body/decoder/\"\n",
        "        \"layer_2/encdec_attention/multihead_attention/dot_product_attention/\"\n",
        "        \"attention_weights\"]\n",
        "\n",
        "    # We clear devices to allow TensorFlow to control on which device it will\n",
        "    # load operations\n",
        "    clear_devices = True\n",
        "    # We import the meta graph and retrieve a Saver\n",
        "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta',\n",
        "                                       clear_devices=clear_devices)\n",
        "\n",
        "    # We retrieve the protobuf graph definition\n",
        "    graph = tf.get_default_graph()\n",
        "    input_graph_def = graph.as_graph_def()\n",
        "\n",
        "    # We start a session and restore the graph weights\n",
        "    with tf.Session() as sess:\n",
        "      saver.restore(sess, input_checkpoint)\n",
        "\n",
        "      # We use a built-in TF helper to export variables to constants\n",
        "      output_graph_def = graph_util.convert_variables_to_constants(\n",
        "          sess, # The session is used to retrieve the weights\n",
        "          input_graph_def, # The graph_def is used to retrieve the nodes\n",
        "          output_node_names, # The output node names are used to select the\n",
        "                             #usefull nodes\n",
        "          variable_names_blacklist=['global_step'])\n",
        "\n",
        "      # Finally we serialize and dump the output graph to the filesystem\n",
        "      with tf.gfile.GFile(output_graph, \"wb\") as output_graph_file:\n",
        "        output_graph_file.write(output_graph_def.SerializeToString())\n",
        "      print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
        "\n",
        "  def __load_graph(self):\n",
        "    \"\"\"Load freezed graph.\"\"\"\n",
        "    # We load the protobuf file from the disk and parse it to retrieve the\n",
        "    # unserialized graph_def\n",
        "    with tf.gfile.GFile(self.frozen_graph_filename, \"rb\") as frozen_graph_file:\n",
        "      graph_def = tf.GraphDef()\n",
        "      graph_def.ParseFromString(frozen_graph_file.read())\n",
        "\n",
        "    # Then, we import the graph_def into a new Graph and returns it\n",
        "    with tf.Graph().as_default() as self.graph:\n",
        "      # The name var will prefix every op/nodes in your graph\n",
        "      # Since we load everything in a new graph, this is not needed\n",
        "      tf.import_graph_def(graph_def, name=\"import\")\n",
        "\n",
        "  def __decode_from_file(self, filename, outfile=None):\n",
        "    \"\"\"Compute predictions on entries in filename and write them out.\"\"\"\n",
        "\n",
        "    if not self.decode_hp.batch_size:\n",
        "      self.decode_hp.batch_size = 32\n",
        "      tf.logging.info(\"decode_hp.batch_size not specified; default=%d\" %\n",
        "                      self.decode_hp.batch_size)\n",
        "\n",
        "    p_hparams = self.hparams.problem_hparams\n",
        "    inputs_vocab = p_hparams.vocabulary[\"inputs\"]\n",
        "    targets_vocab = p_hparams.vocabulary[\"targets\"]\n",
        "    problem_name = \"grapheme_to_phoneme_problem\"\n",
        "    tf.logging.info(\"Performing decoding from a file.\")\n",
        "    # inputs = _get_inputs(filename, self.p2g_mode)\n",
        "    with open(filename, 'r') as f:\n",
        "        input = json.load(f)\n",
        "        inputs = [i for i in input if i.isupper()]\n",
        "    num_decode_batches = (len(inputs) - 1) // self.decode_hp.batch_size + 1\n",
        "\n",
        "    def input_fn():\n",
        "      \"\"\"Function for inputs generator.\"\"\"\n",
        "      input_gen = _decode_batch_input_fn(\n",
        "          num_decode_batches, inputs, inputs_vocab,\n",
        "          self.decode_hp.batch_size, self.decode_hp.max_input_size)\n",
        "      gen_fn = decoding.make_input_fn_from_generator(input_gen)\n",
        "      example = gen_fn()\n",
        "      return decoding._decode_input_tensor_to_features_dict(example,\n",
        "                                                            self.hparams)\n",
        "\n",
        "    decodes = []\n",
        "    result_iter = self.estimator.predict(input_fn)\n",
        "    try:\n",
        "      for result in result_iter:\n",
        "        if self.decode_hp.return_beams:\n",
        "          decoded_inputs = inputs_vocab.decode(\n",
        "              decoding._save_until_eos(result[\"inputs\"], False))\n",
        "          beam_decodes = []\n",
        "          output_beams = np.split(result[\"outputs\"], self.decode_hp.beam_size,\n",
        "                                  axis=0)\n",
        "          for k, beam in enumerate(output_beams):\n",
        "            decoded_outputs = targets_vocab.decode(\n",
        "                decoding._save_until_eos(beam, False))\n",
        "            beam_decodes.append(decoded_outputs)\n",
        "            if outfile:\n",
        "              outfile.write(\"%s %s%s\" % (decoded_inputs, decoded_outputs,\n",
        "                  self.decode_hp.delimiter))\n",
        "            else:\n",
        "              print(\"%s %s%s\" % (decoded_inputs, decoded_outputs,\n",
        "                  self.decode_hp.delimiter))\n",
        "          decodes.append(beam_decodes)\n",
        "        else:\n",
        "          decoded_inputs = inputs_vocab.decode(\n",
        "              decoding._save_until_eos(result[\"inputs\"], False))\n",
        "          decoded_outputs = targets_vocab.decode(\n",
        "              decoding._save_until_eos(result[\"outputs\"], False))\n",
        "\n",
        "          if outfile:\n",
        "            outfile.write(\"%s %s%s\" % (decoded_inputs, decoded_outputs,\n",
        "                self.decode_hp.delimiter))\n",
        "          else:\n",
        "            pass\n",
        "            # print(\"%s %s%s\" % (decoded_inputs, decoded_outputs,\n",
        "            #     self.decode_hp.delimiter))\n",
        "\n",
        "          decodes.append(decoded_outputs)\n",
        "      o = ''\n",
        "      d_o = 0\n",
        "      for i in input:\n",
        "        if i.isupper():\n",
        "          o += decodes[d_o]\n",
        "          d_o += 1\n",
        "        else:\n",
        "          o += i\n",
        "      print(o)\n",
        "    except:\n",
        "      raise StandardError(\"Invalid model in {}\".format(self.params.model_dir))\n",
        "\n",
        "    return [inputs, decodes]\n",
        "\n",
        "  def calc_errors(self, decode_file_path):\n",
        "    \"\"\"Calculate a number of prediction errors.\"\"\"\n",
        "    inputs, decodes = self.__decode_from_file(decode_file_path)\n",
        "\n",
        "    correct, errors = 0, 0\n",
        "    for index, word in enumerate(inputs):\n",
        "      if self.decode_hp.return_beams:\n",
        "        beam_correct_found = False\n",
        "        for beam_decode in decodes[index]:\n",
        "          if beam_decode in self.g2p_gt_map[word]:\n",
        "            beam_correct_found = True\n",
        "            break\n",
        "        if beam_correct_found:\n",
        "          correct += 1\n",
        "        else:\n",
        "          errors += 1\n",
        "      else:\n",
        "        if decodes[index] in self.g2p_gt_map[word]:\n",
        "          correct += 1\n",
        "        else:\n",
        "          errors += 1\n",
        "\n",
        "    return correct, errors\n",
        "\n",
        "\n",
        "def get_word():\n",
        "  \"\"\"Get next word in the interactive mode.\"\"\"\n",
        "  word = \"\"\n",
        "  try:\n",
        "    word = input(\"> \")\n",
        "    #if not issubclass(type(word), text_type):\n",
        "    #  word = text_type(word, encoding=\"utf-8\", errors=\"replace\")\n",
        "  except EOFError:\n",
        "    pass\n",
        "  if not word:\n",
        "    pass\n",
        "  return word\n",
        "\n",
        "\n",
        "def create_g2p_gt_map(words, pronunciations):\n",
        "  \"\"\"Create grapheme-to-phoneme ground true mapping.\"\"\"\n",
        "  g2p_gt_map = {}\n",
        "  for word, pronunciation in zip(words, pronunciations):\n",
        "    if word in g2p_gt_map:\n",
        "      g2p_gt_map[word].append(pronunciation)\n",
        "    else:\n",
        "      g2p_gt_map[word] = [pronunciation]\n",
        "  return g2p_gt_map\n",
        "\n",
        "\n",
        "def _get_inputs(filename, p2g_mode, delimiters=\"\\t \"):\n",
        "  \"\"\"Returning inputs.\n",
        "\n",
        "  Args:\n",
        "    filename: path to file with inputs, 1 per line.\n",
        "    delimiters: str, delimits records in the file.\n",
        "\n",
        "  Returns:\n",
        "    a list of inputs\n",
        "\n",
        "  \"\"\"\n",
        "  tf.logging.info(\"Getting inputs\")\n",
        "  delimiters_regex = re.compile(\"[\" + delimiters + \"]+\")\n",
        "\n",
        "  inputs = []\n",
        "  with tf.gfile.Open(filename) as input_file:\n",
        "    lines = input_file.readlines()\n",
        "    for line in lines:\n",
        "      if set(\"[\" + delimiters + \"]+$\").intersection(line) and not p2g_mode:\n",
        "        items = re.split(delimiters_regex, line.strip(), maxsplit=1)\n",
        "        inputs.append(items[0])\n",
        "      else:\n",
        "        inputs.append(line.strip())\n",
        "  return inputs\n",
        "\n",
        "\n",
        "def _decode_batch_input_fn(num_decode_batches, inputs,\n",
        "                           vocabulary, batch_size, max_input_size):\n",
        "  \"\"\"Decode batch\"\"\"\n",
        "  for batch_idx in range(num_decode_batches):\n",
        "    tf.logging.info(\"Decoding batch %d out of %d\" % (batch_idx, num_decode_batches))\n",
        "    batch_length = 0\n",
        "    batch_inputs = []\n",
        "    for _inputs in inputs[batch_idx * batch_size:(batch_idx + 1) * batch_size]:\n",
        "      input_ids = vocabulary.encode(_inputs)\n",
        "      if max_input_size > 0:\n",
        "        # Subtract 1 for the EOS_ID.\n",
        "        input_ids = input_ids[:max_input_size - 1]\n",
        "      input_ids.append(text_encoder.EOS_ID)\n",
        "      batch_inputs.append(input_ids)\n",
        "      if len(input_ids) > batch_length:\n",
        "        batch_length = len(input_ids)\n",
        "    final_batch_inputs = []\n",
        "    for input_ids in batch_inputs:\n",
        "      assert len(input_ids) <= batch_length\n",
        "      encoded_input = input_ids + [0] * (batch_length - len(input_ids))\n",
        "      final_batch_inputs.append(encoded_input)\n",
        "\n",
        "    yield {\n",
        "        \"inputs\": np.array(final_batch_inputs).astype(np.int32),\n",
        "        \"problem_choice\": np.array(0).astype(np.int32),\n",
        "    }\n",
        "\n",
        "\n",
        "def execute_schedule(exp, params):\n",
        "  if not hasattr(exp, params.schedule):\n",
        "    raise ValueError(\n",
        "            \"Experiment has no method %s, from --schedule\" % params.schedule)\n",
        "  with profile_context(params):\n",
        "    getattr(exp, params.schedule)()\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def profile_context(params):\n",
        "  if params.profile:\n",
        "    with tf.contrib.tfprof.ProfileContext(\"t2tprof\",\n",
        "            trace_steps=range(100),\n",
        "            dump_steps=range(100)) as pctx:\n",
        "      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()\n",
        "      pctx.add_auto_profiling(\"op\", opts, range(100))\n",
        "      yield\n",
        "  else:\n",
        "    yield"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting g2p-seq2seq/g2p_seq2seq/g2p.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6vCtj_DeqQ3"
      },
      "source": [
        "rev_dict = {}\n",
        "\n",
        "with open('cmudict.dict', 'r') as r, open('reverse_cmudict.dict', 'w') as w:\n",
        "    for l in r.readlines():\n",
        "        [word, phonemes] = l.split(maxsplit=1)\n",
        "        w.write(\"{} {}\\n\".format(phonemes.split('#')[0].strip(), word.strip().split('(')[0]))\n",
        "        rev_dict[phonemes.split('#')[0].strip()] = word.strip().split('(')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsKKvNaRagTg",
        "outputId": "a5ced474-0dc5-4e8b-fec0-f7f33572d2f4"
      },
      "source": [
        "%cd g2p-seq2seq\n",
        "!python3 setup.py install\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/g2p-seq2seq\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating g2p_seq2seq.egg-info\n",
            "writing g2p_seq2seq.egg-info/PKG-INFO\n",
            "writing dependency_links to g2p_seq2seq.egg-info/dependency_links.txt\n",
            "writing entry points to g2p_seq2seq.egg-info/entry_points.txt\n",
            "writing requirements to g2p_seq2seq.egg-info/requires.txt\n",
            "writing top-level names to g2p_seq2seq.egg-info/top_level.txt\n",
            "writing manifest file 'g2p_seq2seq.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'g2p_seq2seq.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/app.py -> build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/__init__.py -> build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/params.py -> build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/g2p_trainer_utils.py -> build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/g2p_encoder.py -> build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/g2p_problem.py -> build/lib/g2p_seq2seq\n",
            "copying g2p_seq2seq/g2p.py -> build/lib/g2p_seq2seq\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/app.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/__init__.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/params.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/g2p_trainer_utils.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/g2p_encoder.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/g2p_problem.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "copying build/lib/g2p_seq2seq/g2p.py -> build/bdist.linux-x86_64/egg/g2p_seq2seq\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/app.py to app.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/params.py to params.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/g2p_trainer_utils.py to g2p_trainer_utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/g2p_encoder.py to g2p_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/g2p_problem.py to g2p_problem.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/g2p_seq2seq/g2p.py to g2p.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying g2p_seq2seq.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying g2p_seq2seq.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying g2p_seq2seq.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying g2p_seq2seq.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying g2p_seq2seq.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying g2p_seq2seq.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "g2p_seq2seq.__pycache__.g2p.cpython-37: module references __file__\n",
            "creating dist\n",
            "creating 'dist/g2p_seq2seq-6.2.2a0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing g2p_seq2seq-6.2.2a0-py3.7.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg\n",
            "Extracting g2p_seq2seq-6.2.2a0-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding g2p-seq2seq 6.2.2a0 to easy-install.pth file\n",
            "Installing g2p-seq2seq script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg\n",
            "Processing dependencies for g2p-seq2seq==6.2.2a0\n",
            "Searching for tensor2tensor==1.6.6\n",
            "Best match: tensor2tensor 1.6.6\n",
            "Adding tensor2tensor 1.6.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for future==0.16.0\n",
            "Best match: future 0.16.0\n",
            "Adding future 0.16.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for gevent==1.4.0\n",
            "Best match: gevent 1.4.0\n",
            "Adding gevent 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /tensorflow-1.15.2/python3.7\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for oauth2client==4.1.3\n",
            "Best match: oauth2client 4.1.3\n",
            "Adding oauth2client 4.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for sympy==1.7.1\n",
            "Best match: sympy 1.7.1\n",
            "Adding sympy 1.7.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for h5py==2.10.0\n",
            "Best match: h5py 2.10.0\n",
            "Adding h5py 2.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Flask==1.1.2\n",
            "Best match: Flask 1.1.2\n",
            "Adding Flask 1.1.2 to easy-install.pth file\n",
            "Installing flask script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for gunicorn==20.0.4\n",
            "Best match: gunicorn 20.0.4\n",
            "Adding gunicorn 20.0.4 to easy-install.pth file\n",
            "Installing gunicorn script to /usr/local/bin\n",
            "\n",
            "Using /tensorflow-1.15.2/python3.7\n",
            "Searching for google-api-python-client==1.7.12\n",
            "Best match: google-api-python-client 1.7.12\n",
            "Adding google-api-python-client 1.7.12 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tqdm==4.41.1\n",
            "Best match: tqdm 4.41.1\n",
            "Adding tqdm 4.41.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for gym==0.17.3\n",
            "Best match: gym 0.17.3\n",
            "Adding gym 0.17.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for bz2file==0.98\n",
            "Best match: bz2file 0.98\n",
            "Adding bz2file 0.98 to easy-install.pth file\n",
            "\n",
            "Using /tensorflow-1.15.2/python3.7\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for greenlet==0.4.15\n",
            "Best match: greenlet 0.4.15\n",
            "Adding greenlet 0.4.15 to easy-install.pth file\n",
            "\n",
            "Using /tensorflow-1.15.2/python3.7\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for httplib2==0.17.4\n",
            "Best match: httplib2 0.17.4\n",
            "Adding httplib2 0.17.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for rsa==4.7.2\n",
            "Best match: rsa 4.7.2\n",
            "Adding rsa 4.7.2 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for mpmath==1.2.1\n",
            "Best match: mpmath 1.2.1\n",
            "Adding mpmath 1.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for itsdangerous==1.1.0\n",
            "Best match: itsdangerous 1.1.0\n",
            "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Jinja2==2.11.3\n",
            "Best match: Jinja2 2.11.3\n",
            "Adding Jinja2 2.11.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for click==7.1.2\n",
            "Best match: click 7.1.2\n",
            "Adding click 7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==54.0.0\n",
            "Best match: setuptools 54.0.0\n",
            "Adding setuptools 54.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth-httplib2==0.0.4\n",
            "Best match: google-auth-httplib2 0.0.4\n",
            "Adding google-auth-httplib2 0.0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for uritemplate==3.0.1\n",
            "Best match: uritemplate 3.0.1\n",
            "Adding uritemplate 3.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth==1.27.0\n",
            "Best match: google-auth 1.27.0\n",
            "Adding google-auth 1.27.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyglet==1.5.0\n",
            "Best match: pyglet 1.5.0\n",
            "Adding pyglet 1.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cloudpickle==1.3.0\n",
            "Best match: cloudpickle 1.3.0\n",
            "Adding cloudpickle 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2020.12.5\n",
            "Best match: certifi 2020.12.5\n",
            "Adding certifi 2020.12.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for MarkupSafe==1.1.1\n",
            "Best match: MarkupSafe 1.1.1\n",
            "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cachetools==4.2.1\n",
            "Best match: cachetools 4.2.1\n",
            "Adding cachetools 4.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for g2p-seq2seq==6.2.2a0\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzxXRSniNh20",
        "outputId": "3c855bf7-671d-43d7-ab2f-03e4808ec768"
      },
      "source": [
        "!g2p-seq2seq --train ./reverse_cmudict.dict --model_dir drive/MyDrive/p2g --p2g"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "INFO:tensorflow:Importing user module g2p_seq2seq from path /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg\n",
            "INFO:tensorflow:Overriding hparams in transformer_base with eval_drop_long_sequences=1,batch_size=4096,num_hidden_layers=3,hidden_size=256,filter_size=512,num_heads=4,length_bucket_step=1.5,max_length=30,min_length_bucket=6\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/trainer_lib.py:165: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:schedule=continuous_train_and_eval\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
            "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fed690783d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", '_save_checkpoints_steps': 2000, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 1, '_model_dir': 'drive/MyDrive/p2g', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fed1ee8bd90>}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fed69039a70>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fed1ee8b590>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", '_save_checkpoints_steps': 2000, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 1, '_model_dir': 'drive/MyDrive/p2g', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fed1ee8b510>}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fed69074710>) includes params argument, but params are not passed to Estimator.\n",
            "Training started.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2000 or save_checkpoints_secs None.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Reading data files from drive/MyDrive/p2g/train.preprocessed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/data_generators/problem.py:141: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/data_reader.py:31: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/data_reader.py:103: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.group_by_window(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/data_generators/problem.py:1045: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_100_256.bottom\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py:1007: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_100_256.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/models/transformer.py:78: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/layers/common_layers.py:3090: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_100_256.top\n",
            "INFO:tensorflow:Base learning rate: 2.000000\n",
            "INFO:tensorflow:Trainable Variables Total size: 3979264\n",
            "INFO:tensorflow:Using optimizer Adam\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2021-03-03 13:19:42.277800: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-03-03 13:19:42.280988: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-03-03 13:19:42.281206: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x564527587e40 executing computations on platform Host. Devices:\n",
            "2021-03-03 13:19:42.281237: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-12000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 12000 into drive/MyDrive/p2g/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.33670244, step = 12001\n",
            "INFO:tensorflow:global_step/sec: 0.401782\n",
            "INFO:tensorflow:loss = 0.17703758, step = 12101 (248.892 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.413553\n",
            "INFO:tensorflow:loss = 0.24733824, step = 12201 (241.807 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.408437\n",
            "INFO:tensorflow:loss = 0.1621955, step = 12301 (244.835 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.3973\n",
            "INFO:tensorflow:loss = 0.3134968, step = 12401 (251.699 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.398876\n",
            "INFO:tensorflow:loss = 0.2685482, step = 12501 (250.705 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.41309\n",
            "INFO:tensorflow:loss = 0.12639587, step = 12601 (242.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.411831\n",
            "INFO:tensorflow:loss = 0.40691498, step = 12701 (242.818 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415236\n",
            "INFO:tensorflow:loss = 0.2778592, step = 12801 (240.827 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415926\n",
            "INFO:tensorflow:loss = 0.25979733, step = 12901 (240.427 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.414926\n",
            "INFO:tensorflow:loss = 0.43561083, step = 13001 (241.007 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.413695\n",
            "INFO:tensorflow:loss = 0.1601842, step = 13101 (241.724 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.413127\n",
            "INFO:tensorflow:loss = 0.14257063, step = 13201 (242.056 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.417587\n",
            "INFO:tensorflow:loss = 0.2644667, step = 13301 (239.471 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416744\n",
            "INFO:tensorflow:loss = 0.31667292, step = 13401 (239.956 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.410205\n",
            "INFO:tensorflow:loss = 0.16743845, step = 13501 (243.781 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.39989\n",
            "INFO:tensorflow:loss = 0.4092253, step = 13601 (250.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416847\n",
            "INFO:tensorflow:loss = 0.23665102, step = 13701 (239.896 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.414811\n",
            "INFO:tensorflow:loss = 0.14913352, step = 13801 (241.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.417708\n",
            "INFO:tensorflow:loss = 0.23671383, step = 13901 (239.402 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 14000 into drive/MyDrive/p2g/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Reading data files from drive/MyDrive/p2g/eval.preprocessed\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_100_256.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_100_256.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_100_256.top\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/bleu_hook.py:142: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-03T14:41:02Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-14000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [20/200]\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-03-14:41:21\n",
            "INFO:tensorflow:Saving dict for global step 14000: global_step = 14000, loss = 0.27698344, metrics-grapheme_to_phoneme_problem/targets/accuracy = 0.8918009, metrics-grapheme_to_phoneme_problem/targets/accuracy_per_sequence = 0.4046588, metrics-grapheme_to_phoneme_problem/targets/accuracy_top5 = 0.99701625, metrics-grapheme_to_phoneme_problem/targets/approx_bleu_score = 0.61644405, metrics-grapheme_to_phoneme_problem/targets/neg_log_perplexity = -0.3892617, metrics-grapheme_to_phoneme_problem/targets/rouge_2_fscore = 0.70717007, metrics-grapheme_to_phoneme_problem/targets/rouge_L_fscore = 0.7629767\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14000: drive/MyDrive/p2g/model.ckpt-14000\n",
            "INFO:tensorflow:global_step/sec: 0.377137\n",
            "INFO:tensorflow:loss = 0.18235908, step = 14001 (265.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415221\n",
            "INFO:tensorflow:loss = 0.3129122, step = 14101 (240.836 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416164\n",
            "INFO:tensorflow:loss = 0.19844817, step = 14201 (240.290 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.418416\n",
            "INFO:tensorflow:loss = 0.15859988, step = 14301 (238.996 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415845\n",
            "INFO:tensorflow:loss = 0.42899808, step = 14401 (240.475 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415564\n",
            "INFO:tensorflow:loss = 0.20837675, step = 14501 (240.636 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.406502\n",
            "INFO:tensorflow:loss = 0.26584673, step = 14601 (246.001 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.397725\n",
            "INFO:tensorflow:loss = 0.3619047, step = 14701 (251.430 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.412662\n",
            "INFO:tensorflow:loss = 0.23378147, step = 14801 (242.329 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.411765\n",
            "INFO:tensorflow:loss = 0.21558036, step = 14901 (242.857 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.41581\n",
            "INFO:tensorflow:loss = 0.25080538, step = 15001 (240.495 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415947\n",
            "INFO:tensorflow:loss = 0.24626668, step = 15101 (240.415 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.413912\n",
            "INFO:tensorflow:loss = 0.20092092, step = 15201 (241.597 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416376\n",
            "INFO:tensorflow:loss = 0.43973997, step = 15301 (240.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415546\n",
            "INFO:tensorflow:loss = 0.24496716, step = 15401 (240.647 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.413782\n",
            "INFO:tensorflow:loss = 0.1866431, step = 15501 (241.673 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.411843\n",
            "INFO:tensorflow:loss = 0.20921649, step = 15601 (242.811 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.404869\n",
            "INFO:tensorflow:loss = 0.14567597, step = 15701 (246.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.405707\n",
            "INFO:tensorflow:loss = 0.23542023, step = 15801 (246.483 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416029\n",
            "INFO:tensorflow:loss = 0.18196002, step = 15901 (240.368 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 16000 into drive/MyDrive/p2g/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from drive/MyDrive/p2g/eval.preprocessed\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_100_256.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_100_256.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_100_256.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-03T16:02:10Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-16000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [20/200]\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-03-16:02:30\n",
            "INFO:tensorflow:Saving dict for global step 16000: global_step = 16000, loss = 0.2977957, metrics-grapheme_to_phoneme_problem/targets/accuracy = 0.88522273, metrics-grapheme_to_phoneme_problem/targets/accuracy_per_sequence = 0.3747947, metrics-grapheme_to_phoneme_problem/targets/accuracy_top5 = 0.9965975, metrics-grapheme_to_phoneme_problem/targets/approx_bleu_score = 0.6096467, metrics-grapheme_to_phoneme_problem/targets/neg_log_perplexity = -0.41998404, metrics-grapheme_to_phoneme_problem/targets/rouge_2_fscore = 0.6996465, metrics-grapheme_to_phoneme_problem/targets/rouge_L_fscore = 0.75710034\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 16000: drive/MyDrive/p2g/model.ckpt-16000\n",
            "INFO:tensorflow:global_step/sec: 0.379787\n",
            "INFO:tensorflow:loss = 0.14326836, step = 16001 (263.305 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.411422\n",
            "INFO:tensorflow:loss = 0.4332888, step = 16101 (243.061 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.415757\n",
            "INFO:tensorflow:loss = 0.22967649, step = 16201 (240.525 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.418491\n",
            "INFO:tensorflow:loss = 0.2634489, step = 16301 (238.954 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416175\n",
            "INFO:tensorflow:loss = 0.3199985, step = 16401 (240.284 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.417154\n",
            "INFO:tensorflow:loss = 0.10803985, step = 16501 (239.720 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.416679\n",
            "INFO:tensorflow:loss = 0.1752646, step = 16601 (239.993 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.417136\n",
            "INFO:tensorflow:loss = 0.2076234, step = 16701 (239.730 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.40565\n",
            "INFO:tensorflow:loss = 0.2569063, step = 16801 (246.518 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.414959\n",
            "INFO:tensorflow:loss = 0.15366335, step = 16901 (240.987 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.418111\n",
            "INFO:tensorflow:loss = 0.3897616, step = 17001 (239.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.40707\n",
            "INFO:tensorflow:loss = 0.19543661, step = 17101 (245.658 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.402352\n",
            "INFO:tensorflow:loss = 0.23528011, step = 17201 (248.539 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.403937\n",
            "INFO:tensorflow:loss = 0.1494746, step = 17301 (247.564 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.402956\n",
            "INFO:tensorflow:loss = 0.10266441, step = 17401 (248.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.405893\n",
            "INFO:tensorflow:loss = 0.23519564, step = 17501 (246.370 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.398914\n",
            "INFO:tensorflow:loss = 0.24346197, step = 17601 (250.681 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.402887\n",
            "INFO:tensorflow:loss = 0.11177493, step = 17701 (248.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.398336\n",
            "INFO:tensorflow:loss = 0.40504843, step = 17801 (251.045 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.396678\n",
            "INFO:tensorflow:loss = 0.2504042, step = 17901 (252.093 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 18000 into drive/MyDrive/p2g/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from drive/MyDrive/p2g/eval.preprocessed\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_100_256.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_100_256.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_100_256.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-03T17:24:14Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-18000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [20/200]\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-03-17:24:34\n",
            "INFO:tensorflow:Saving dict for global step 18000: global_step = 18000, loss = 0.28296837, metrics-grapheme_to_phoneme_problem/targets/accuracy = 0.89211494, metrics-grapheme_to_phoneme_problem/targets/accuracy_per_sequence = 0.39211586, metrics-grapheme_to_phoneme_problem/targets/accuracy_top5 = 0.9970512, metrics-grapheme_to_phoneme_problem/targets/approx_bleu_score = 0.6204387, metrics-grapheme_to_phoneme_problem/targets/neg_log_perplexity = -0.3975633, metrics-grapheme_to_phoneme_problem/targets/rouge_2_fscore = 0.7092297, metrics-grapheme_to_phoneme_problem/targets/rouge_L_fscore = 0.76280755\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 18000: drive/MyDrive/p2g/model.ckpt-18000\n",
            "INFO:tensorflow:global_step/sec: 0.361186\n",
            "INFO:tensorflow:loss = 0.26383987, step = 18001 (276.865 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.397215\n",
            "INFO:tensorflow:loss = 0.2736438, step = 18101 (251.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.396558\n",
            "INFO:tensorflow:loss = 0.13984272, step = 18201 (252.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.395798\n",
            "INFO:tensorflow:loss = 0.27979532, step = 18301 (252.654 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.389636\n",
            "INFO:tensorflow:loss = 0.20257507, step = 18401 (256.649 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.393297\n",
            "INFO:tensorflow:loss = 0.3660514, step = 18501 (254.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.392997\n",
            "INFO:tensorflow:loss = 0.120252624, step = 18601 (254.455 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.393957\n",
            "INFO:tensorflow:loss = 0.31679893, step = 18701 (253.835 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394872\n",
            "INFO:tensorflow:loss = 0.20525576, step = 18801 (253.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.390818\n",
            "INFO:tensorflow:loss = 0.10779546, step = 18901 (255.874 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.400026\n",
            "INFO:tensorflow:loss = 0.40271062, step = 19001 (249.984 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.399625\n",
            "INFO:tensorflow:loss = 0.095775396, step = 19101 (250.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.402188\n",
            "INFO:tensorflow:loss = 0.21231604, step = 19201 (248.640 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.389982\n",
            "INFO:tensorflow:loss = 0.10714436, step = 19301 (256.422 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.398024\n",
            "INFO:tensorflow:loss = 0.1101655, step = 19401 (251.241 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.400977\n",
            "INFO:tensorflow:loss = 0.31442404, step = 19501 (249.391 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.399253\n",
            "INFO:tensorflow:loss = 0.16945301, step = 19601 (250.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.399997\n",
            "INFO:tensorflow:loss = 0.2691585, step = 19701 (250.001 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.400394\n",
            "INFO:tensorflow:loss = 0.328501, step = 19801 (249.755 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.399172\n",
            "INFO:tensorflow:loss = 0.17164257, step = 19901 (250.518 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20000 into drive/MyDrive/p2g/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from drive/MyDrive/p2g/eval.preprocessed\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_100_256.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_100_256.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_100_256.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-03T18:48:40Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-20000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [20/200]\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-03-18:49:00\n",
            "INFO:tensorflow:Saving dict for global step 20000: global_step = 20000, loss = 0.28128308, metrics-grapheme_to_phoneme_problem/targets/accuracy = 0.8998622, metrics-grapheme_to_phoneme_problem/targets/accuracy_per_sequence = 0.42989397, metrics-grapheme_to_phoneme_problem/targets/accuracy_top5 = 0.99459094, metrics-grapheme_to_phoneme_problem/targets/approx_bleu_score = 0.633531, metrics-grapheme_to_phoneme_problem/targets/neg_log_perplexity = -0.39437008, metrics-grapheme_to_phoneme_problem/targets/rouge_2_fscore = 0.7186396, metrics-grapheme_to_phoneme_problem/targets/rouge_L_fscore = 0.7700723\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20000: drive/MyDrive/p2g/model.ckpt-20000\n",
            "INFO:tensorflow:global_step/sec: 0.363878\n",
            "INFO:tensorflow:loss = 0.2168765, step = 20001 (274.817 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394824\n",
            "INFO:tensorflow:loss = 0.20197032, step = 20101 (253.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.3904\n",
            "INFO:tensorflow:loss = 0.35508853, step = 20201 (256.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.390609\n",
            "INFO:tensorflow:loss = 0.12718125, step = 20301 (256.011 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.389672\n",
            "INFO:tensorflow:loss = 0.37417367, step = 20401 (256.626 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.393028\n",
            "INFO:tensorflow:loss = 0.22915035, step = 20501 (254.435 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.383839\n",
            "INFO:tensorflow:loss = 0.1192999, step = 20601 (260.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.392231\n",
            "INFO:tensorflow:loss = 0.29539716, step = 20701 (254.952 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.391001\n",
            "INFO:tensorflow:loss = 0.08085113, step = 20801 (255.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.39295\n",
            "INFO:tensorflow:loss = 0.19507125, step = 20901 (254.486 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.395007\n",
            "INFO:tensorflow:loss = 0.17218778, step = 21001 (253.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.395077\n",
            "INFO:tensorflow:loss = 0.09812742, step = 21101 (253.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394256\n",
            "INFO:tensorflow:loss = 0.28120348, step = 21201 (253.643 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.393374\n",
            "INFO:tensorflow:loss = 0.2240738, step = 21301 (254.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394427\n",
            "INFO:tensorflow:loss = 0.23268841, step = 21401 (253.533 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.38874\n",
            "INFO:tensorflow:loss = 0.2588968, step = 21501 (257.241 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.383313\n",
            "INFO:tensorflow:loss = 0.21032824, step = 21601 (260.883 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.382779\n",
            "INFO:tensorflow:loss = 0.14735045, step = 21701 (261.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.390527\n",
            "INFO:tensorflow:loss = 0.13438986, step = 21801 (256.064 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.395952\n",
            "INFO:tensorflow:loss = 0.30157405, step = 21901 (252.556 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22000 into drive/MyDrive/p2g/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from drive/MyDrive/p2g/eval.preprocessed\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_100_256.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_100_256.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_100_256.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-03T20:14:15Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-22000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [20/200]\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-03-20:14:36\n",
            "INFO:tensorflow:Saving dict for global step 22000: global_step = 22000, loss = 0.32632536, metrics-grapheme_to_phoneme_problem/targets/accuracy = 0.89947826, metrics-grapheme_to_phoneme_problem/targets/accuracy_per_sequence = 0.4213827, metrics-grapheme_to_phoneme_problem/targets/accuracy_top5 = 0.98743695, metrics-grapheme_to_phoneme_problem/targets/approx_bleu_score = 0.6350016, metrics-grapheme_to_phoneme_problem/targets/neg_log_perplexity = -0.44307446, metrics-grapheme_to_phoneme_problem/targets/rouge_2_fscore = 0.7199731, metrics-grapheme_to_phoneme_problem/targets/rouge_L_fscore = 0.7694079\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22000: drive/MyDrive/p2g/model.ckpt-22000\n",
            "INFO:tensorflow:global_step/sec: 0.360719\n",
            "INFO:tensorflow:loss = 0.16107397, step = 22001 (277.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394885\n",
            "INFO:tensorflow:loss = 0.29042238, step = 22101 (253.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.39513\n",
            "INFO:tensorflow:loss = 0.14187759, step = 22201 (253.081 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394689\n",
            "INFO:tensorflow:loss = 0.17872876, step = 22301 (253.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394661\n",
            "INFO:tensorflow:loss = 0.17439353, step = 22401 (253.382 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.391423\n",
            "INFO:tensorflow:loss = 0.076665044, step = 22501 (255.478 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.385216\n",
            "INFO:tensorflow:loss = 0.19825418, step = 22601 (259.595 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.387475\n",
            "INFO:tensorflow:loss = 0.1588353, step = 22701 (258.081 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.395678\n",
            "INFO:tensorflow:loss = 0.41636294, step = 22801 (252.732 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.39632\n",
            "INFO:tensorflow:loss = 0.33370996, step = 22901 (252.320 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.402989\n",
            "INFO:tensorflow:loss = 0.25889987, step = 23001 (248.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.400825\n",
            "INFO:tensorflow:loss = 0.22143675, step = 23101 (249.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.400508\n",
            "INFO:tensorflow:loss = 0.19729663, step = 23201 (249.683 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.398328\n",
            "INFO:tensorflow:loss = 0.13093458, step = 23301 (251.050 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.39856\n",
            "INFO:tensorflow:loss = 0.17100894, step = 23401 (250.904 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.407887\n",
            "INFO:tensorflow:loss = 0.17693235, step = 23501 (245.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.406972\n",
            "INFO:tensorflow:loss = 0.3281472, step = 23601 (245.717 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.394921\n",
            "INFO:tensorflow:loss = 0.094050154, step = 23701 (253.215 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD_QBzLpLisE",
        "outputId": "daa5782c-c0b0-4a95-c8e8-5fa9b5b57852"
      },
      "source": [
        "!g2p-seq2seq --model_dir drive/MyDrive/p2g --freeze"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "INFO:tensorflow:Importing user module g2p_seq2seq from path /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg\n",
            "INFO:tensorflow:Overriding hparams in transformer_base with eval_drop_long_sequences=1,batch_size=4096,num_hidden_layers=3,hidden_size=256,filter_size=512,num_heads=4,length_bucket_step=1.5,max_length=30,min_length_bucket=6\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/trainer_lib.py:165: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:schedule=continuous_train_and_eval\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
            "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a350a50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", '_save_checkpoints_steps': 409, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 1, '_model_dir': 'drive/MyDrive/p2g', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f1c4a350b10>}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f1c925a3710>) includes params argument, but params are not passed to Estimator.\n",
            "2021-03-03 21:57:43.995768: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-03-03 21:57:43.999124: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-03-03 21:57:43.999385: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55761f039c80 executing computations on platform Host. Devices:\n",
            "2021-03-03 21:57:43.999423: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-22000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg/g2p_seq2seq/g2p.py:396: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n",
            "INFO:tensorflow:Froze 101 variables.\n",
            "INFO:tensorflow:Converted 101 variables to const ops.\n",
            "2890 ops in the final graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNaghuvzKodP",
        "outputId": "76b035a1-8e22-4c2e-d4eb-5507d60cd2d8"
      },
      "source": [
        "%%writefile w.txt\n",
        "[\"L IH0 NG Z\", \" \", \"N AW1 OW0\", \" \", \"SH IY1 Z\", \" \", \"DH AH0\", \" \", \"N OW1\", \" \", \"T AH0 SH EH1\", \" \", \"T ER1\", \" \", \"DH AH0\", \" \", \"W AY1\", \" \", \"HH IY1 R\", \" \", \"DH AH0 P ER0\", \" \", \"DH AH0\", \" \", \"IH1 T\", \" \", \"DH AH0\", \" \", \"IH1 Z\", \" \", \"DH AH0\", \" \", \"B EH1 L\", \" \", \"G UH2\", \" \", \"M AO1 R\", \" \", \"DH AH0\", \" \", \"DH AH0\", \" \", \"Y UW1\", \"\\n\", \"B L OW1\", \" \", \"T R AH1 S T\", \" \", \"IH1 T S\", \" \", \"M AY1\", \" \", \"IH1 T\", \" \", \"S AH1\", \" \", \"DH AH0\", \" \", \"HH AE1 D\", \" \", \"DH AH0\", \"\\n\\n\", \"AY1\", \" \", \"Y UH0\", \" \", \"R EY1\", \" \", \"IH2\", \" \", \"W EH1 R\", \" \", \"Y UW1\", \" \", \"AH0\", \" \", \"W AA1 Z\", \" \", \"R OW1 S K OW0\", \" \", \"S T R EY1 N JH\", \" \", \"DH AH0 V ER0\", \" \", \"HH AA1 R T\", \" \", \"S M AY1 L\", \" \", \"Y UW1\", \" \", \"HH OW1 L D\", \" \", \"IH1 T\", \" \", \"M IH1\", \" \", \"IH0 N\", \" \", \"HH AE1 Z\", \" \", \"DH AH0\", \" \", \"HH EH1 R L AH0\", \" \", \"B AE1 D\", \" \", \"W AA1 Z\", \" \", \"S T R AO1 NG\", \" \", \"HH EH1\", \" \", \"DH AH0\", \" \", \"W AH1 T\", \" \", \"DH AH0\", \" \", \"AO1 L\", \" \", \"T AH0\", \" \", \"AA1 R M Z\", \" \", \"AY1 Z\", \" \", \"S IH1 N S\", \" \", \"Y AO1 R\", \"\\n\\n\", \"L EH1 T\", \" \", \"DH AH0\", \" \", \"S AO1\", \" \", \"IH1 T\", \" \", \"N AY1 T\", \" \", \"Y UW1\", \" \", \"R AA1 K\", \" \", \"K AE1 N\", \" \", \"DH AH0\", \" \", \"K AE1 N\", \" \", \"HH AE1 D\", \"\\n\", \"DH AH0\", \" \", \"B EY1\", \" \", \"B IH0\", \" \", \"AY1 V\", \" \", \"R OW1 L\", \" \", \"IH0 N\", \" \", \"Y AO1 R\", \" \", \"R EY1\", \" \", \"AA1 N\", \" \", \"DH AH0\", \" \", \"D OW1 N T\", \" \", \"M AO1 R\", \" \", \"IH1 T\", \" \", \"K L OW1 S\", \",\", \"\\n\\n\", \"T EY1 K\", \" \", \"SH IY1\", \" \", \"N EH1 K S T\", \" \", \"D AH1 Z N T\", \" \", \"AH1 P\", \" \", \"AA1 R\", \" \", \"AW1\", \" \", \"K AE1 N\", \" \", \"K AE1 N\", \" \", \"HH EY1\", \" \", \"DH AH0\", \" \", \"S T R AO1\", \"\\n\", \"G IH1 V\", \"\\n\", \"AY1\", \" \", \"W AY1 M B ER0\", \" \", \"N OW1\", \" \", \"W AA1 Z\", \"\\n\", \"AH0\", \" \", \"AA1 R M Z\", \" \", \"IH1 T\", \" \", \"N IY1\", \" \", \"AH0\", \"\\n\", \"HH UW1\", \" \", \"IH0 N P AH0\", \" \", \"IH1 T L IY0\", \" \", \"IH1 Z\", \" \", \"G AA1 N IH0 NG\", \" \", \"L AO1 S T\", \" \", \"S P R IH1\", \" \", \"AH1 V\", \" \", \"T AY1 M\", \" \", \"Y UW1\", \" \", \"F AY1 N\", \" \", \"AY1 V\", \" \", \"Y UW1\", \" \", \"IH1 T\", \" \", \"AA1\", \" \", \"AH0 N D\", \" \", \"T EH1 L\", \" \", \"D IH0\", \" \", \"TH IH1 NG K\", \" \", \"HH EH1\", \" \", \"B IH1\", \" \", \"G AA1\", \" \", \"IH1 Z\", \" \", \"AH0 N D\", \" \", \"AE1 S K T\", \" \", \"DH EY1 D\", \" \", \"IH1 T\", \" \", \"TH IH1 NG K\", \" \", \"TH IH1 NG K\", \" \", \"Y UH1 R\", \" \", \"W IH1\", \" \", \"AH0 N D\", \" \", \"AY1\", \" \", \"AH1 V\", \" \", \"Y UW1\", \" \", \"SH UH1 R\", \"\\n\\n\", \"N OW1\", \"\\n\\n\", \"B AH1 T\", \" \", \"HH AW1\", \" \", \"L UW1 Z\", \" \", \"L IY1 TH IH2 NG\", \" \", \"D AW1 N\", \" \", \"IH1 Z\", \" \", \"HH EH1 L P\", \" \", \"T EH1 L\", \" \", \"AH1 V\", \" \", \"DH AH0\", \" \", \"F EH1\", \" \", \"D AA1\", \" \", \"T OY1\", \" \", \"AY1\", \" \", \"DH AH0\", \" \", \"T R AY1\", \" \", \"AY1 Z\", \" \", \"Y AO1 R\", \" \", \"Y UW1\", \" \", \"L AY1 K\", \" \", \"Y UW1\", \" \", \"AA1 N T IH0 NG\", \" \", \"K AH1 M\", \" \", \"P EH1\", \" \", \"Y UW1\", \" \", \"HH AE1 V\", \" \", \"L AE1 N D\", \" \", \"T AY1 M\", \" \", \"DH AH0\", \" \", \"AY1\", \"\\n\\n\", \"D OW1 N T OW2\", \"\\n\", \"K AH0 Z\", \" \", \"G AA1 T\", \" \", \"S T AA1 P\", \" \", \"AH1 V\", \" \", \"HH OW1 L D\", \" \", \"DH AE1 T\", \" \", \"IH1 T\", \" \", \"T AY1 M\", \" \", \"Y UW1\", \" \", \"AA1 R\", \" \", \"M EH1\", \" \", \"P IY1\", \" \", \"G UH2\", \" \", \"S IY1 M\", \" \", \"IH1 Z\", \" \", \"W AA1\", \" \", \"Y UW1\", \"\\n\\n\", \"S OW1\", \" \", \"DH AH0\", \" \", \"W IH1\", \" \", \"Y UW1\", \" \", \"Y UW1\", \" \", \"AH0\", \" \", \"IH1 T\", \" \", \"Y UW1\", \" \", \"D AO1 R\", \" \", \"R IY1 L\", \" \", \"IH1 T\", \" \", \"W ER1 D Z\", \" \", \"AY1 L\", \" \", \"T AY1 M\", \" \", \"AH0\", \" \", \"K AH0\", \"\\n\\n\", \"S OW1\", \" \", \"D IH1\", \" \", \"B R EH1 TH\", \" \", \"DH AH0\", \" \", \"B IY1\", \" \", \"Y UW1\", \" \", \"HH AA1 R T M IH0 N\", \" \", \"W EY1\", \" \", \"D AW1 N K IH0 NG\", \" \", \"IH0 N\", \" \", \"L AY1 K S\", \" \", \"DH AH0\", \"\\n\", \"L EH1 T\", \" \", \"Y UW1 L\", \" \", \"AW1 R\", \"\\n\", \"M OW1\", \" \", \"AW1 T\", \" \", \"AH0\", \"\\n\", \"L EH1 T\", \" \", \"DH AE1 T\", \" \", \"AH0\", \" \", \"N OW1\", \" \", \"Y UW1\", \" \", \"K AE1 N\", \" \", \"G AA1 T\", \" \", \"DH AH0\", \" \", \"AY1 V\", \",\", \"\\n\\n\", \"G R AE1\", \" \", \"W AY1\", \"\\n\", \"D UW1\", \" \", \"DH AH0\", \" \", \"M AE2\", \" \", \"AH1 V\", \"\\n\", \"L IH1\", \" \", \"B R IY1 DH\", \" \", \"Y UW1\", \" \", \"K L OW1 S\", \" \", \"DH AH0\", \" \", \"Y UW1\", \" \", \"AE1\", \" \", \"D OW1 N T T AH0 N\", \" \", \"S W IY1 T\", \" \", \"AE1 S K T\", \" \", \"IH1 T\", \" \", \"Y UW1\", \"\\n\", \"D UW1\", \" \", \"D IH0\", \"\\n\", \"W AH1 N\", \" \", \"P L EY1\", \" \", \"S AO1 NG\", \" \", \"Y UW1\", \" \", \"L EH1 T\", \" \", \"AE1 N F OW2 N\", \" \", \"S AH1\", \" \", \"Y UW1 S\", \" \", \"N OW1\", \"\\n\", \"B IH0\", \" \", \"Y UW1 M AH0 S\", \" \", \"EH1\", \" \", \"T UW1\", \" \", \"Y UW1\", \" \", \"AH0 N D M ER0\", \"\\n\", \"AH0 N D\", \" \", \"M AO1 R\", \" \", \"IH1 Z\", \" \", \"G OW1\", \" \", \"JH AH1 S T\", \" \", \"M IY1\", \"\\n\", \"B AH1 T\", \" \", \"B IH0\", \"\\n\", \"K R IY1\", \" \", \"IH1 T K AH0 N\", \" \", \"Y UW1\", \" \", \"W ER1 L D\", \" \", \"AO2\", \"\\n\\n\", \"AH0 N D\", \" \", \"N OW1\", \" \", \"W OW1\", \"\\n\", \"Y UW1\", \" \", \"B OY1\", \" \", \"F L AY1\", \" \", \"DH AH0\", \"\\n\", \"L EY1 D CH IH0 NG\", \" \", \"TH IH1 NG K\", \" \", \"DH AH0\", \" \", \"DH EH1 R\", \"\\n\", \"HH EY1\", \"\\n\", \"N AW1\", \" \", \"AY1 Z\", \" \", \"M EY1 K\", \" \", \"R IY1\", \" \", \"AH1 V\", \"\\n\", \"D AE1\", \" \", \"T UW1 AH0\", \" \", \"N OW1\", \" \", \"AW1 R N JH AH0 L\", \"\\n\", \"B AH1 T\", \" \", \"Y UW1 N AH0\", \" \", \"L AH1 V\", \" \", \"Y UW1\"]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting w.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCSzX5fMDObM",
        "outputId": "13a711fb-d93c-4386-c8f0-68e699937f01"
      },
      "source": [
        "!g2p-seq2seq --decode w.txt --model_dir drive/MyDrive/p2g --p2g"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "INFO:tensorflow:Importing user module g2p_seq2seq from path /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg\n",
            "INFO:tensorflow:Overriding hparams in transformer_base with eval_drop_long_sequences=1,batch_size=4096,num_hidden_layers=3,hidden_size=256,filter_size=512,num_heads=4,length_bucket_step=1.5,max_length=30,min_length_bucket=6\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/trainer_lib.py:165: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:schedule=continuous_train_and_eval\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
            "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb70c443a50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", '_save_checkpoints_steps': 409, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 1, '_model_dir': 'drive/MyDrive/p2g', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fb70c443b10>}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fb7546b2a70>) includes params argument, but params are not passed to Estimator.\n",
            "2021-03-05 14:45:37.719955: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-03-05 14:45:37.723093: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-03-05 14:45:37.723295: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fe5c7494a0 executing computations on platform Host. Devices:\n",
            "2021-03-05 14:45:37.723326: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/g2p_seq2seq-6.2.2a0-py3.7.egg/g2p_seq2seq/g2p.py:294: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "INFO:tensorflow:decode_hp.batch_size not specified; default=32\n",
            "INFO:tensorflow:Performing decoding from a file.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Decoding batch 0 out of 10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/utils/decoding.py:694: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'infer'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Greedy Decoding\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py:1007: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/layers/modalities.py:112: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/models/transformer.py:78: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/layers/common_layers.py:3090: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensor2tensor/models/transformer.py:915: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/p2g/model.ckpt-22000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Decoding batch 1 out of 10\n",
            "INFO:tensorflow:Decoding batch 2 out of 10\n",
            "INFO:tensorflow:Decoding batch 3 out of 10\n",
            "INFO:tensorflow:Decoding batch 4 out of 10\n",
            "INFO:tensorflow:Decoding batch 5 out of 10\n",
            "INFO:tensorflow:Decoding batch 6 out of 10\n",
            "INFO:tensorflow:Decoding batch 7 out of 10\n",
            "INFO:tensorflow:Decoding batch 8 out of 10\n",
            "INFO:tensorflow:Decoding batch 9 out of 10\n",
            "lings nowo shees tha no tashe ter tha wy hear thaper tha it tha iz tha bell goo mor tha tha you\n",
            "blow trust its mye it suh tha hadd tha\n",
            "\n",
            "i yu ray i wehr you ah waas rosco strange thaver hart smile you hold it mih in haz tha herla badd waas strong heh tha what tha all ta arms i's since yor\n",
            "\n",
            "lett tha saw it night you rock qann tha qann hadd\n",
            "tha bay be ive roehl in yor ray on tha don't mor it qlose,\n",
            "\n",
            "take shee nexed duzn't up r ow qann qann hay tha straw\n",
            "giv\n",
            "i weimber no waas\n",
            "ah arms it nee ah\n",
            "hoo enpa itley iz gonning lost spri ove time you phine ive you it ah and tell de think heh bih ga iz and asked thade it think think your wih and i ove you shure\n",
            "\n",
            "no\n",
            "\n",
            "but how lews leathing down iz help tell ove tha pheh dah toy i tha tri i's yor you like you onting qum peh you hav land time tha i\n",
            "\n",
            "doneto\n",
            "quas gott stopp ove hold that it time you r meh pee goo seam iz wah you\n",
            "\n",
            "sow tha wih you you ah it you dorr reel it words ile time ah qa\n",
            "\n",
            "sow dih breth tha bee you hartmin way downking in likes tha\n",
            "lett youle our\n",
            "mo out ah\n",
            "lett that ah no you qann gott tha ive,\n",
            "\n",
            "gra wy\n",
            "du tha mah ove\n",
            "li breathe you qlose tha you ah donetton sweet asked it you\n",
            "du de\n",
            "one play song you lett anphone suh yous no\n",
            "be umus eh too you undmer\n",
            "and mor iz goe just mee\n",
            "but be\n",
            "qree itken you world awe\n",
            "\n",
            "and no woh\n",
            "you boy pfly tha\n",
            "ladching think tha there\n",
            "hay\n",
            "nao i's make ree ove\n",
            "dah tua no ourngell\n",
            "but una love you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "BLO3YWLzeXdc",
        "outputId": "7b174b2f-fb4d-407c-bc92-006929e2f091"
      },
      "source": [
        "input()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-9c8b639daf2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}