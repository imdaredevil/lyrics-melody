{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanilla_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWYdBK7TMSbW",
        "outputId": "8bb79169-b0b1-4937-f583-4e8b45c8bcbd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install /content/drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
        "!git clone https://github.com/gulnazaki/performer-pytorch.git\n",
        "!pip install ./performer-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.3.0\n",
            "Cloning into 'performer-pytorch'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 523 (delta 64), reused 64 (delta 32), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (523/523), 35.02 MiB | 43.89 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n",
            "Processing ./performer-pytorch\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/86/f1df73868c1c433a9184d94e86cdd970951ecf14d8b556b41302febb9a12/local_attention-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (1.7.1+cu101)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: performer-pytorch, axial-positional-embedding\n",
            "  Building wheel for performer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for performer-pytorch: filename=performer_pytorch-0.15.0-cp37-none-any.whl size=12557 sha256=87a5b5d65cfe6ddbabece584f25b5fc0384c9d63b0de326e25aaa145a115ace1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/73/93/041f7dd55e6f33ef90455a36e217ed2811faeb9dd9fe343159\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp37-none-any.whl size=2905 sha256=9a12fce9c1ccc8267628e4c50715ca9043c44f8acfdf19c8338ba60fb5b5df2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built performer-pytorch axial-positional-embedding\n",
            "Installing collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 local-attention-1.2.2 performer-pytorch-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4CamFrYWsEp",
        "outputId": "a6aea489-aff3-45af-c0ba-b53cd05b6e7b"
      },
      "source": [
        "%%writefile generate_vanilla.py\n",
        "\n",
        "from performer_pytorch import PerformerEncDec\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from itertools import cycle\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Train Vanilla Performer on Lakh Midi Dataset Instruments-Lyrics-Vocal Melody')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, default='',\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "    \n",
        "    parser.add_argument('--pretrained-model', '-pm', type=str, required=True,\n",
        "                        help='Pretrained model filepath')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-instrumental-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer instrumental sequences')\n",
        "    \n",
        "    parser.add_argument('--max-vocal-sequence-length', '-maxv', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer vocal melody sequences')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--validate-batch-size', '-vss', type=int, default=1,\n",
        "                        help='Batch size for validation dataset')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_instrumental_length, max_vocal_length):\n",
        "        super().__init__()\n",
        "        instrumental_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.instrumental_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_instrumental_vocab = {l: w for w, l in self.instrumental_vocab.items()}\n",
        "            self.vocal_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_vocal_vocab = {l: w for w, l in self.vocal_vocab.items()}\n",
        "            \n",
        "        df = pd.read_parquet(dataset_file)\n",
        "\n",
        "        self.files = list(df['file'])\n",
        "        self.instrumental = [self.encode(json.loads(f), seq_type='instrumental', max_length=max_instrumental_length) for f in df[instrumental_type]]\n",
        "        self.vocals = [self.encode(json.loads(v), seq_type='vocals', max_length=max_vocal_length) for v in df['vocal']]\n",
        "\n",
        "        self.max_instrumental_length = max([len(f) for f in self.instrumental])\n",
        "        self.max_vocal_length = max([len(f) for f in self.vocals])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instrumental[index], self.vocals[index]), self.files[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def truncate(self, sequence, max_length):\n",
        "        if max_length >= 0:\n",
        "            return sequence[:max_length]\n",
        "        return sequence\n",
        "\n",
        "    def encode(self, event_sequence, seq_type, max_length=-1):\n",
        "        if seq_type == 'instrumental':\n",
        "            return torch.tensor([self.instrumental_vocab[e] for e in self.truncate(event_sequence, max_length - 1)] + [self.instrumental_vocab['<eos>']])\n",
        "        else:\n",
        "            return torch.tensor([self.vocal_vocab['<bos>']] + [self.vocal_vocab[e] for e in self.truncate(event_sequence, max_length - 2)] + [self.vocal_vocab['<eos>']])\n",
        "\n",
        "    def decode(self, event_sequence, seq_type, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "            true_size = size\n",
        "        if seq_type == 'instrumental':\n",
        "            return [self.reverse_instrumental_vocab[i.item()] for i in event_sequence[:true_size]]\n",
        "        else:\n",
        "            return [self.reverse_vocal_vocab[o.item()] for o in event_sequence[:true_size]]\n",
        "\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    data, files = zip(*batch)\n",
        "    instrumental, vocals = zip(*data)\n",
        "    batch_size = len(files)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        instrumental = instrumental[0].view(1, -1)\n",
        "        vocals = vocals[0].view(1, -1)\n",
        "        instrumental_masks = torch.ones_like(instrumental).bool()\n",
        "        vocal_masks = torch.ones_like(vocals).bool()\n",
        "        return (instrumental.long(), instrumental_masks), (vocals.long(), vocal_masks), files[0]\n",
        "\n",
        "    instrumental_lengths = [seq.size(0) for seq in instrumental]\n",
        "    instrumental_max_length = max(instrumental_lengths)\n",
        "    instrumental_masks = torch.arange(instrumental_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(instrumental_lengths).view(-1, 1)\n",
        "    padded_instrumental = torch.zeros(batch_size, instrumental_max_length)\n",
        "    for i, l in enumerate(instrumental_lengths):\n",
        "        padded_instrumental[i, :l] = instrumental[i]\n",
        "\n",
        "    vocal_lengths = [seq.size(0) for seq in vocals]\n",
        "    vocal_max_length = max(vocal_lengths)\n",
        "    vocal_masks = torch.arange(vocal_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(vocal_lengths).view(-1, 1)\n",
        "    padded_vocals = torch.zeros(batch_size, vocal_max_length)\n",
        "    for i, l in enumerate(vocal_lengths):\n",
        "        padded_vocals[i, :l] = vocals[i]\n",
        "\n",
        "    return (padded_instrumental.long(), instrumental_masks), (padded_vocals.long(), vocal_masks), files\n",
        "\n",
        "\n",
        "def valid_structure_metric(sequence, vocab):\n",
        "    def get_valids_for_next(e, note_was_on):\n",
        "        if e == waits[-1]:\n",
        "            valid_events = waits + offs + boundaries + phonemes + ons\n",
        "        elif e in waits:\n",
        "            valid_events = offs + boundaries + phonemes\n",
        "        elif e in ons:\n",
        "            note_was_on = True\n",
        "            valid_events = waits\n",
        "        elif e in offs:\n",
        "            note_was_on = False\n",
        "            valid_events = waits + boundaries + phonemes\n",
        "        elif e in boundaries:\n",
        "            if e == boundaries[-1]:\n",
        "                valid_events = boundaries[:-1] + phonemes\n",
        "            else:\n",
        "                valid_events = phonemes\n",
        "        else:\n",
        "            valid_events = ons\n",
        "        return valid_events, note_was_on\n",
        "\n",
        "    sequence = sequence.tolist()\n",
        "    waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "    ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "    offs = [vocab['_OFF_']]\n",
        "    boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "    phonemes = [i for e, i in vocab.items() if not '_' in e or e == '_R_']\n",
        "    \n",
        "    valid_count = 0\n",
        "    valid_events = waits + phonemes + boundaries\n",
        "    note_was_on = False\n",
        "    for e in sequence:\n",
        "        if e in valid_events and \\\n",
        "        (e not in ons or note_was_on == False) and \\\n",
        "        (e not in offs or note_was_on == True):\n",
        "            valid_count += 1\n",
        "        valid_events, note_was_on = get_valids_for_next(e, note_was_on)\n",
        "\n",
        "    size = len(sequence) - 1 if sequence[-1] == 2 else len(sequence)\n",
        "    if size == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return valid_count / size\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = MidiDataset(dataset_file=args.dataset_file,\n",
        "                          monophonic=args.monophonic,\n",
        "                          vocabulary_prefix=args.vocabulary_prefix,\n",
        "                          max_instrumental_length=args.max_instrumental_sequence_length,\n",
        "                          max_vocal_length=args.max_vocal_sequence_length)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    model = PerformerEncDec(\n",
        "        dim = 512,\n",
        "        enc_heads = 8,\n",
        "        dec_heads = 8,\n",
        "        enc_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_ff_chunks = 10,\n",
        "        dec_ff_chunks = 10,\n",
        "        enc_num_tokens = len(dataset.instrumental_vocab),\n",
        "        dec_num_tokens = len(dataset.vocal_vocab),\n",
        "        enc_max_seq_len = dataset.max_instrumental_length,\n",
        "        dec_max_seq_len = dataset.max_vocal_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_tie_embed = True,\n",
        "        dec_tie_embed = True,\n",
        "        enc_reversible = True,\n",
        "        dec_reversible = True\n",
        "    ).to(device)\n",
        "\n",
        "    def valid_events(vocab, previous):\n",
        "        if all(previous < 0):\n",
        "            valid_events.waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "            valid_events.ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "            valid_events.offs = [vocab['_OFF_']]\n",
        "            valid_events.boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "            valid_events.phonemes = [i for e, i in vocab.items() if not '_' in e or e == '_R_']\n",
        "            valid_events.notes_on = torch.tensor([False]).expand(previous.size(0), -1)\n",
        "            return torch.tensor(valid_events.waits + valid_events.phonemes + valid_events.boundaries).expand(previous.size(0), -1).to(device)\n",
        "        else:\n",
        "            valids = []\n",
        "            for i, p in enumerate(previous):\n",
        "                if p == valid_events.waits[-1]:\n",
        "                    v = valid_events.waits + (valid_events.offs if valid_events.notes_on[i] else valid_events.boundaries + valid_events.phonemes)\n",
        "                elif p in valid_events.waits:\n",
        "                    v = valid_events.offs if valid_events.notes_on[i] else valid_events.boundaries + valid_events.phonemes\n",
        "                elif p in valid_events.ons:\n",
        "                    valid_events.notes_on[i] = True\n",
        "                    v = valid_events.waits\n",
        "                elif p in valid_events.offs:\n",
        "                    valid_events.notes_on[i] = False\n",
        "                    v = valid_events.waits + valid_events.boundaries + valid_events.phonemes\n",
        "                elif p in valid_events.boundaries:\n",
        "                    if p == valid_events.boundaries[-1]:\n",
        "                        v = valid_events.boundaries[:-1] + valid_events.phonemes\n",
        "                    else:\n",
        "                        v = valid_events.phonemes\n",
        "                else:\n",
        "                    v = valid_events.ons\n",
        "                valids.append(v)\n",
        "            return torch.tensor(valids).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(args.pretrained_model))\n",
        "\n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=args.validate_batch_size, collate_fn=collate_fn_zero_pad)\n",
        "    vals = ([v for v in val_loader_ if v[-1] in ['W/E/U/TRWEUHA12903D01A39/e9710a3f0160b067065e190038fbffaa.mid',\n",
        "                                                     'F/X/L/TRFXLIH128F9308ACD/01006f8d14cc866a3bca857f14d5b0fe.mid',\n",
        "                                                     'L/W/P/TRLWPRD128F424FF0B/6217065d714d93ee66e3069fe7237f07.mid',\n",
        "                                                     'K/Y/H/TRKYHRD128F9302FDE/6b9e2c4794953a1af54549d27bd0689f.mid',\n",
        "                                                     'B/Y/U/TRBYUSU12903CF113E/d02da3544d75f07c668305af590ae38e.mid']])\n",
        "    # val_loader = cycle(val_loader_)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\"Let's go!\")\n",
        "        constrain_fn = partial(valid_events, dataset.vocal_vocab)\n",
        "        for v in vals:\n",
        "            start_time = time.time()\n",
        "            (instrumental, instrumental_mask), (expected_vocals, expected_vocals_mask), file = v\n",
        "            instrumental = instrumental[0].view(1, -1)\n",
        "            instrumental_mask = instrumental_mask[0].view(1, -1)\n",
        "            \n",
        "            # <bos> token\n",
        "            vocals_start = torch.ones(1,1).long()\n",
        "            print(file)\n",
        "            vocals = model.generate(instrumental.to(device),\n",
        "                                                  vocals_start.to(device),\n",
        "                                                  seq_len=dataset.max_vocal_length,\n",
        "                                                  enc_mask=instrumental_mask.to(device),\n",
        "                                                  eos_token=2,\n",
        "                                                  constrain_fn=constrain_fn)\n",
        "            decoded_vocals = dataset.decode(vocals[0], seq_type='vocals')\n",
        "\n",
        "            print((time.time() - start_time)/len(decoded_vocals))\n",
        "            with open(os.path.join(args.save_dir, 'the_output_examples.txt'), 'a') as f:\n",
        "                f.write(\"{}:\\n\\n{}\\n----------------\\n\\n\"\\\n",
        "                                .format(file, decoded_vocals))\n",
        "            vsm = valid_structure_metric(vocals[0], dataset.vocal_vocab)\n",
        "            print(\"Valid Structure Metric: {}\".format(vsm))\n",
        "            print(\"------------------\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_vanilla.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wfo-Vtrs7nr",
        "outputId": "2a44042e-2c93-454b-8f04-41b78d82ae06"
      },
      "source": [
        "!python3 generate_vanilla.py -df drive/MyDrive/vanilla_performer/dataset.parquet -v drive/MyDrive/vanilla_performer/vanilla_ -pm drive/MyDrive/vanilla_performer/full/model.pt -sd drive/MyDrive/vanilla_performer/full -maxi 50000"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's go!\n",
            "W/E/U/TRWEUHA12903D01A39/e9710a3f0160b067065e190038fbffaa.mid\n",
            "0.16945118553682262\n",
            "Valid Structure Metric: 1.0005252100840336\n",
            "------------------\n",
            "F/X/L/TRFXLIH128F9308ACD/01006f8d14cc866a3bca857f14d5b0fe.mid\n",
            "0.21083993482078933\n",
            "Valid Structure Metric: 1.0006305170239596\n",
            "------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2GDy17zUvcT",
        "outputId": "c6449a14-8054-4909-9fbf-fc26aec79088"
      },
      "source": [
        "!python3 generate_vanilla.py -df drive/MyDrive/vanilla_performer/dataset_chords.parquet -v drive/MyDrive/vanilla_performer/vanilla_chords_ -pm drive/MyDrive/vanilla_performer/chords/model.pt -sd drive/MyDrive/vanilla_performer/chords"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's go!\n",
            "F/X/L/TRFXLIH128F9308ACD/01006f8d14cc866a3bca857f14d5b0fe.mid\n",
            "0.15604436592378704\n",
            "Valid Structure Metric: 1.0005109862033725\n",
            "------------------\n",
            "W/E/U/TRWEUHA12903D01A39/e9710a3f0160b067065e190038fbffaa.mid\n",
            "0.15821199813992187\n",
            "Valid Structure Metric: 1.0004081632653061\n",
            "------------------\n",
            "L/W/P/TRLWPRD128F424FF0B/6217065d714d93ee66e3069fe7237f07.mid\n",
            "0.29391738516472055\n",
            "Valid Structure Metric: 1.000184706316956\n",
            "------------------\n",
            "B/Y/U/TRBYUSU12903CF113E/d02da3544d75f07c668305af590ae38e.mid\n",
            "0.11257976717419095\n",
            "Valid Structure Metric: 1.0011123470522802\n",
            "------------------\n",
            "K/Y/H/TRKYHRD128F9302FDE/6b9e2c4794953a1af54549d27bd0689f.mid\n",
            "0.13933594978981503\n",
            "Valid Structure Metric: 1.0006702412868633\n",
            "------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I02flv4lp7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "7983ae7d-f47e-4322-d0d9-993bd169c3ea"
      },
      "source": [
        "input()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9c8b639daf2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}