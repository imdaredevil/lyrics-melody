{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decoupled_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qux-zfGDXrVk",
        "outputId": "d61ed68e-754b-45e3-f50e-41f3988c7113"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install deepspeed==0.3.10\n",
        "!pip install transformers\n",
        "!git clone https://github.com/gulnazaki/performer-pytorch.git\n",
        "!pip install ./performer-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.19.5)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.3.0\n",
            "Collecting deepspeed==0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bd/b2b544ca1286252e9a559b1508e64d0d61af7a73b6bf6737568858128e11/deepspeed-0.3.10.tar.gz (281kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (0.8.2+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 52.3MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed==0.3.10) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed==0.3.10) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed==0.3.10) (54.0.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.10-cp37-none-any.whl size=272624 sha256=fb38326166644181f143ab8e1fb68d0fdaefce9c625b0f1091a882efee63e7d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/3c/9c/39a16330874a2c55f61fe2c501e120258975d509177ffdcda7\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.10 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=6dc5ce37256f9f1d88b58d87e11491277f3252f931c888f9eb49df372352e429\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n",
            "Cloning into 'performer-pytorch'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 523 (delta 64), reused 64 (delta 32), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (523/523), 35.02 MiB | 54.00 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n",
            "Processing ./performer-pytorch\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/86/f1df73868c1c433a9184d94e86cdd970951ecf14d8b556b41302febb9a12/local_attention-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (1.7.1+cu101)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: performer-pytorch, axial-positional-embedding\n",
            "  Building wheel for performer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for performer-pytorch: filename=performer_pytorch-0.15.0-cp37-none-any.whl size=12557 sha256=fabb8790925b92065f7752a0674b8b5faf6ba09a6aee3dfe2eef5d269cdea9f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/73/93/041f7dd55e6f33ef90455a36e217ed2811faeb9dd9fe343159\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp37-none-any.whl size=2905 sha256=c33dafef946dcf0b61b5bac7724de8ba8223fe5e15a0846810d90b09cbedd365\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built performer-pytorch axial-positional-embedding\n",
            "Installing collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 local-attention-1.2.2 performer-pytorch-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF38wBLZY3hn",
        "outputId": "f04edd3b-b088-44d2-b7cd-645b11020148"
      },
      "source": [
        "%%writefile decoupled_performer.py\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from performer_pytorch.performer_pytorch import PerformerLM\n",
        "from performer_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n",
        "\n",
        "ENC_PREFIX = 'enc_'\n",
        "LM_PREFIX = 'lm_'\n",
        "DEC_PREFIX = 'dec_'\n",
        "\n",
        "def group_dict_by_key(cond, d):\n",
        "    return_val = [dict(),dict()]\n",
        "    for key in d.keys():\n",
        "        match = bool(cond(key))\n",
        "        ind = int(not match)\n",
        "        return_val[ind][key] = d[key]\n",
        "    return (*return_val,)\n",
        "\n",
        "def string_begins_with(prefix, str):\n",
        "    return bool(re.match(f'^{prefix}', str))\n",
        "\n",
        "def group_by_key_prefix(prefix, d):\n",
        "    return group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n",
        "\n",
        "def group_by_key_prefix_and_remove_prefix(prefix, d):\n",
        "    kwargs_with_prefix, kwargs = group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n",
        "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
        "    return kwargs_without_prefix, kwargs\n",
        "\n",
        "def extract_enc_lm_dec_kwargs(kwargs):\n",
        "    enc_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(ENC_PREFIX, kwargs)\n",
        "    lm_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(LM_PREFIX, kwargs)\n",
        "    dec_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(DEC_PREFIX, kwargs)\n",
        "    return enc_kwargs, lm_kwargs, dec_kwargs, kwargs\n",
        "\n",
        "def extract_and_set_enc_lm_dec_kwargs(kwargs):\n",
        "    enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_enc_lm_dec_kwargs(kwargs)\n",
        "    if 'mask' in enc_kwargs:\n",
        "        dec_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "    if 'mask' in lm_kwargs:\n",
        "        dec_kwargs.setdefault('second_context_mask', lm_kwargs['mask'][:, :-1])\n",
        "    return enc_kwargs, lm_kwargs, dec_kwargs, kwargs\n",
        "\n",
        "class DecoupledPerformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        tie_token_embeds = False,\n",
        "        no_projection = False,\n",
        "        pretrained_lm = \"\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, _ = extract_enc_lm_dec_kwargs(kwargs)\n",
        "        \n",
        "        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs and 'dim' not in lm_kwargs\n",
        "\n",
        "        enc_kwargs['dim'] = lm_kwargs['dim'] = dec_kwargs['dim'] = dim\n",
        "        enc_kwargs['no_projection'] = lm_kwargs['no_projection'] = dec_kwargs['no_projection'] = no_projection\n",
        "\n",
        "        lm_kwargs['causal'] = True\n",
        "        # cross attention has to be set explicitly\n",
        "        if not 'cross_attend' in lm_kwargs:\n",
        "            lm_kwargs['cross_attend'] = False\n",
        "        \n",
        "        self.lm_cross_attending = lm_kwargs['cross_attend']\n",
        "\n",
        "        dec_kwargs['causal'] = True\n",
        "        dec_kwargs['cross_attend'] = True\n",
        "        dec_kwargs['second_cross_attend'] = True\n",
        "\n",
        "        enc = PerformerLM(**enc_kwargs)\n",
        "        lm = PerformerLM(**lm_kwargs)\n",
        "        dec = PerformerLM(**dec_kwargs)\n",
        "\n",
        "        if tie_token_embeds:\n",
        "            enc.token_emb = lm.token_emb = dec.token_emb\n",
        "\n",
        "        self.enc = enc\n",
        "        if pretrained_lm:\n",
        "            pretrained = torch.load(pretrained_lm)\n",
        "            from collections import OrderedDict\n",
        "            new_pretrained = OrderedDict()\n",
        "            if lm_kwargs['reversible']:\n",
        "                if lm_kwargs['cross_attend']:\n",
        "                    for k, v in pretrained.items():\n",
        "                        if len(k.split('.')) >= 5:\n",
        "                            new_pretrained['performer.net.blocks.{}.{}'.format(int(k.split('.')[3])*2, k.split('.', 4)[-1])] = pretrained[k]\n",
        "                        else:\n",
        "                            new_pretrained[k] = pretrained[k]\n",
        "                else:\n",
        "                    new_pretrained = pretrained\n",
        "            else:\n",
        "                for k, v in pretrained.items():\n",
        "                    if len(k.split('.')) >= 5 and k.split('.')[4] == 'f':\n",
        "                        new_pretrained['performer.net.layers.{}.0.{}'.format(k.split('.')[3], k.split('.', 6)[-1])] = pretrained[k]\n",
        "                    elif len(k.split('.')) >= 5 and k.split('.')[4] == 'g':\n",
        "                        new_pretrained['performer.net.layers.{}.{}.{}'.format(k.split('.')[3], 2 if lm_kwargs['cross_attend'] else 1, k.split('.', 6)[-1])] = pretrained[k]\n",
        "                    else:\n",
        "                        new_pretrained[k] = pretrained[k]\n",
        "            lm.load_state_dict(new_pretrained, strict=False)\n",
        "            print(\"Loaded pretrained language model: {}\".format(pretrained_lm))\n",
        "        self.lm = AutoregressiveWrapper(lm)\n",
        "        self.dec = AutoregressiveWrapper(dec)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, instrumental, lyrics_start, lyrics_len, vocals_start, vocals_len, **kwargs):\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_and_set_enc_lm_dec_kwargs(kwargs)\n",
        "        instrumental_encodings = self.enc(instrumental, return_encodings = True, **enc_kwargs)\n",
        "        if self.lm_cross_attending:\n",
        "            lm_kwargs.setdefault('context', instrumental_encodings)\n",
        "            lm_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "        lyrics_encodings, lyrics = self.lm.generate(lyrics_start, lyrics_len, return_also_encodings = True, **{**lm_kwargs, **kwargs})\n",
        "        vocals = self.dec.generate(vocals_start, vocals_len, context = instrumental_encodings, second_context = lyrics_encodings, **{**dec_kwargs, **kwargs})\n",
        "        return lyrics, vocals\n",
        "\n",
        "    def forward(self, instrumental, lyrics, vocals, **kwargs):\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_and_set_enc_lm_dec_kwargs(kwargs)\n",
        "        instrumental_encodings = self.enc(instrumental, return_encodings = True, **enc_kwargs)\n",
        "        if self.lm_cross_attending:\n",
        "            lm_kwargs.setdefault('context', instrumental_encodings)\n",
        "            lm_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "        lyrics_encodings, lyrics_loss = self.lm(lyrics, return_also_encodings = True, **lm_kwargs)\n",
        "        vocals_loss = self.dec(vocals, context = instrumental_encodings, second_context = lyrics_encodings, **dec_kwargs)\n",
        "        return lyrics_loss + vocals_loss\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing decoupled_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PeFMLjhYtmO",
        "outputId": "ebf70536-ed75-413f-96f4-b0b7a878f7f2"
      },
      "source": [
        "%%writefile generate_decoupled.py\n",
        "\n",
        "from decoupled_performer import DecoupledPerformer\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from itertools import cycle\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import AutoTokenizer\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Train Decoupled Performer on Lakh Midi Dataset Instruments-Lyrics-Vocal Melody')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "\n",
        "    parser.add_argument('--pretrained-model', '-pm', type=str,\n",
        "                        help='Pretrained model to load')\n",
        "\n",
        "    parser.add_argument('--tokenizer', '-tok', type=str,\n",
        "                        help='Hugginface tokenizer to use')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, required=True,\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-instrumental-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer instrumental sequences')\n",
        "\n",
        "    parser.add_argument('--max-lyrics-sequence-length', '-maxl', type=int, default=1024,\n",
        "                        help='If provided it will truncate samples with longer lyrics sequences')\n",
        "    \n",
        "    parser.add_argument('--max-vocal-sequence-length', '-maxv', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer vocal melody sequences')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--validate-size', '-vs', type=int, default=40,\n",
        "                        help='Will calculate average of validation loss for n batches')\n",
        "\n",
        "    parser.add_argument('--validate-batch-size', '-vss', type=int, default=1,\n",
        "                        help='Batch size for validation dataset')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class DecoupledDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_instrumental_length, max_lyrics_length, max_vocal_length, tokenizer):\n",
        "        super().__init__()\n",
        "        instrumental_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.instrumental_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_instrumental_vocab = {l: w for w, l in self.instrumental_vocab.items()}\n",
        "            self.vocal_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_vocal_vocab = {l: w for w, l in self.vocal_vocab.items()}\n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, use_fast=True)\n",
        "            \n",
        "        df = pd.read_parquet(dataset_file)\n",
        "\n",
        "        self.files = list(df['file'])\n",
        "        self.instrumental = [self.encode(json.loads(f), seq_type='instrumental', max_length=max_instrumental_length) for f in df[instrumental_type]]\n",
        "        self.lyrics = []\n",
        "        self.vocals = []\n",
        "        for lyric, vocal in zip(df['lyrics'], df['vocal']):\n",
        "            l = json.loads(lyric)\n",
        "            v = json.loads(vocal)\n",
        "            encoded_lyrics, max_syllables = self.encode(l, seq_type='lyrics', max_length=max_lyrics_length)\n",
        "            self.lyrics.append(encoded_lyrics)\n",
        "            self.vocals.append(self.encode(v, seq_type='vocals', max_length=max_vocal_length, max_syllables=max_syllables))\n",
        "\n",
        "        self.max_instrumental_length = max([len(f) for f in self.instrumental])\n",
        "        self.max_lyrics_length = max([len(f) for f in self.lyrics])\n",
        "        self.max_vocal_length = max([len(f) for f in self.vocals])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instrumental[index], self.lyrics[index], self.vocals[index]), self.files[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def truncate(self, sequence, max_length):\n",
        "        if max_length >= 0:\n",
        "            return sequence[:max_length]\n",
        "        return sequence\n",
        "\n",
        "    def encode(self, event_sequence, seq_type, max_length=-1, max_syllables=-1):\n",
        "        if seq_type == 'instrumental':\n",
        "            return torch.tensor([self.instrumental_vocab[e] for e in self.truncate(event_sequence, max_length - 1)] + [self.instrumental_vocab['<eos>']])\n",
        "        elif seq_type == 'lyrics':\n",
        "            tokenized = self.tokenizer(''.join(event_sequence), max_length=max_length - 2, truncation=True, return_overflowing_tokens=True)\n",
        "            if len(tokenized.encodings) == 2:\n",
        "                last_word_index = tokenized[0].word_ids[-1]\n",
        "                if last_word_index == tokenized[1].word_ids[0]:\n",
        "                    tokens = [tokenized[0].tokens[i] for i in range(len(tokenized[0])) if tokenized[0].word_ids[i] < last_word_index]\n",
        "                else:\n",
        "                    tokens = tokenized[0].tokens\n",
        "                size = len(self.tokenizer.convert_tokens_to_string(tokens).strip())\n",
        "                max_syllables = 0\n",
        "                chars = 0\n",
        "                for l in event_sequence:\n",
        "                    chars += len(l)\n",
        "                    if chars > size:\n",
        "                        break\n",
        "                    max_syllables += 1                \n",
        "                ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            else:\n",
        "                ids = tokenized[0].ids\n",
        "            return torch.tensor([self.tokenizer.bos_token_id] + ids + [self.tokenizer.eos_token_id]), max_syllables\n",
        "        else:\n",
        "            if max_syllables >= 0:\n",
        "                last_index = -1\n",
        "                syllables = 0\n",
        "                for i, e in enumerate(event_sequence):\n",
        "                    if '_' not in e:\n",
        "                        syllables += 1\n",
        "                        if syllables > max_syllables:\n",
        "                            last_index = i\n",
        "                            break\n",
        "                if last_index >= 0:\n",
        "                    event_sequence = event_sequence[:last_index]\n",
        "            return torch.tensor([self.vocal_vocab['<bos>']] + [self.vocal_vocab[e] for e in self.truncate([e for e in event_sequence if '_' in e], max_length - 2)] + [self.vocal_vocab['<eos>']])\n",
        "\n",
        "    def decode(self, event_sequence, seq_type, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "            true_size = size\n",
        "        if seq_type == 'instrumental':\n",
        "            return [self.reverse_instrumental_vocab[i.item()] for i in event_sequence[:true_size]]\n",
        "        elif seq_type == 'lyrics':\n",
        "            return self.tokenizer.decode(event_sequence[:true_size])\n",
        "        else:\n",
        "            return [self.reverse_vocal_vocab[o.item()] for o in event_sequence[:true_size]]\n",
        "\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    data, files = zip(*batch)\n",
        "    instrumental, lyrics, vocals = zip(*data)\n",
        "    batch_size = len(files)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        instrumental = instrumental[0].view(1, -1)\n",
        "        vocals = vocals[0].view(1, -1)\n",
        "        lyrics = lyrics[0].view(1, -1)\n",
        "        instrumental_masks = torch.ones_like(instrumental).bool()\n",
        "        vocal_masks = torch.ones_like(vocals).bool()\n",
        "        lyrics_masks = torch.ones_like(lyrics).bool()\n",
        "        return (instrumental.long(), instrumental_masks), (lyrics.long(), lyrics_masks), (vocals.long(), vocal_masks), files[0]\n",
        "\n",
        "    instrumental_lengths = [seq.size(0) for seq in instrumental]\n",
        "    instrumental_max_length = max(instrumental_lengths)\n",
        "    instrumental_masks = torch.arange(instrumental_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(instrumental_lengths).view(-1, 1)\n",
        "    padded_instrumental = torch.zeros(batch_size, instrumental_max_length)\n",
        "    for i, l in enumerate(instrumental_lengths):\n",
        "        padded_instrumental[i, :l] = instrumental[i]\n",
        "\n",
        "    lyrics_lengths = [seq.size(0) for seq in lyrics]\n",
        "    lyrics_max_length = max(lyrics_lengths)\n",
        "    lyrics_masks = torch.arange(lyrics_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(lyrics_lengths).view(-1, 1)\n",
        "    padded_lyrics = torch.zeros(batch_size, lyrics_max_length)\n",
        "    for i, l in enumerate(lyrics_lengths):\n",
        "        padded_lyrics[i, :l] = lyrics[i]\n",
        "\n",
        "    vocal_lengths = [seq.size(0) for seq in vocals]\n",
        "    vocal_max_length = max(vocal_lengths)\n",
        "    vocal_masks = torch.arange(vocal_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(vocal_lengths).view(-1, 1)\n",
        "    padded_vocals = torch.zeros(batch_size, vocal_max_length)\n",
        "    for i, l in enumerate(vocal_lengths):\n",
        "        padded_vocals[i, :l] = vocals[i]\n",
        "\n",
        "    return (padded_instrumental.long(), instrumental_masks), (padded_lyrics.long(), lyrics_masks), (padded_vocals.long(), vocal_masks), files\n",
        "\n",
        "\n",
        "def valid_structure_metric(sequence, vocab):\n",
        "    def get_valids_for_next(e, note_was_on):\n",
        "        if e == waits[-1]:\n",
        "            valid_events = waits + offs + boundaries + phonemes + ons\n",
        "        elif e in waits:\n",
        "            valid_events = offs + boundaries + phonemes + ons\n",
        "        elif e in ons:\n",
        "            note_was_on = True\n",
        "            valid_events = waits\n",
        "        elif e in offs:\n",
        "            note_was_on = False\n",
        "            valid_events = waits + boundaries + phonemes + ons\n",
        "        elif e in boundaries:\n",
        "            if e == boundaries[-1]:\n",
        "                valid_events = boundaries[:-1] + phonemes + ons\n",
        "            else:\n",
        "                valid_events = phonemes + ons\n",
        "        else:\n",
        "            valid_events = ons\n",
        "        return valid_events, note_was_on\n",
        "\n",
        "    sequence = sequence.tolist()\n",
        "    waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "    ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "    offs = [vocab['_OFF_']]\n",
        "    boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "    phonemes = [vocab['_R_']]\n",
        "    \n",
        "    valid_count = 0\n",
        "    valid_events = waits + boundaries + ons\n",
        "    note_was_on = False\n",
        "    for e in sequence:\n",
        "        if e in valid_events and \\\n",
        "        (e not in ons or note_was_on == False) and \\\n",
        "        (e not in offs or note_was_on == True):\n",
        "            valid_count += 1\n",
        "        valid_events, note_was_on = get_valids_for_next(e, note_was_on)\n",
        "\n",
        "    size = len(sequence) - 1 if sequence[-1] == 2 else len(sequence)\n",
        "    if size == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return valid_count / size\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = DecoupledDataset(dataset_file=args.dataset_file,\n",
        "                               monophonic=args.monophonic,\n",
        "                               vocabulary_prefix=args.vocabulary_prefix,\n",
        "                               max_instrumental_length=args.max_instrumental_sequence_length,\n",
        "                               max_lyrics_length=args.max_lyrics_sequence_length,\n",
        "                               max_vocal_length=args.max_vocal_sequence_length,\n",
        "                               tokenizer=args.tokenizer)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    model = DecoupledPerformer(\n",
        "        dim = 768,\n",
        "        enc_heads = 6,\n",
        "        lm_heads = 12,\n",
        "        dec_heads = 6,\n",
        "        enc_depth = 6,\n",
        "        lm_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_ff_chunks = 10,\n",
        "        lm_ff_chunks = 1,\n",
        "        dec_ff_chunks = 10,\n",
        "        enc_num_tokens = len(dataset.instrumental_vocab),\n",
        "        lm_num_tokens = len(dataset.tokenizer),\n",
        "        dec_num_tokens = len(dataset.vocal_vocab),\n",
        "        enc_max_seq_len = dataset.max_instrumental_length,\n",
        "        lm_max_seq_len = args.max_lyrics_sequence_length,\n",
        "        dec_max_seq_len = dataset.max_vocal_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        lm_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        lm_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        lm_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_tie_embed = True,\n",
        "        lm_tie_embed = True,\n",
        "        dec_tie_embed = True,\n",
        "        enc_reversible = True,\n",
        "        lm_reversible = True,\n",
        "        dec_reversible = True,\n",
        "        # pretrained_lm = args.pretrained_language_model,\n",
        "        # lm_cross_attend = True\n",
        "    ).to(device)\n",
        "\n",
        "    def valid_events(vocab, previous):\n",
        "        if all(previous < 0):\n",
        "            valid_events.waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "            valid_events.ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "            valid_events.offs = [vocab['_OFF_']]\n",
        "            valid_events.boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "            valid_events.phonemes = [0, 1, 2] + [vocab['_R_']]\n",
        "            valid_events.notes_on = torch.tensor([False]).expand(previous.size(0), -1)\n",
        "            return torch.tensor(valid_events.waits + valid_events.ons + valid_events.boundaries).expand(previous.size(0), -1).to(device)\n",
        "        else:\n",
        "            valids = []\n",
        "            for i, p in enumerate(previous):\n",
        "                if p == valid_events.waits[-1]:\n",
        "                    v = valid_events.waits + (valid_events.offs if valid_events.notes_on[i] else valid_events.boundaries + valid_events.phonemes + valid_events.ons)\n",
        "                elif p in valid_events.waits:\n",
        "                    v = valid_events.offs if valid_events.notes_on[i] else valid_events.boundaries + valid_events.phonemes + valid_events.ons\n",
        "                elif p in valid_events.ons:\n",
        "                    valid_events.notes_on[i] = True\n",
        "                    v = valid_events.waits\n",
        "                elif p in valid_events.offs:\n",
        "                    valid_events.notes_on[i] = False\n",
        "                    v = valid_events.waits + valid_events.boundaries + valid_events.phonemes + valid_events.ons\n",
        "                elif p in valid_events.boundaries:\n",
        "                    if p == valid_events.boundaries[-1]:\n",
        "                        v = valid_events.boundaries[:-1] + valid_events.phonemes + valid_events.ons\n",
        "                    else:\n",
        "                        v = valid_events.phonemes + valid_events.ons\n",
        "                else:\n",
        "                    v = valid_events.ons\n",
        "                valids.append(v)\n",
        "            return torch.tensor(valids).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(args.pretrained_model))\n",
        "\n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=args.validate_batch_size, collate_fn=collate_fn_zero_pad)\n",
        "    vals = ([v for v in val_loader_ if v[-1] in ['W/E/U/TRWEUHA12903D01A39/e9710a3f0160b067065e190038fbffaa.mid',\n",
        "                                                     'F/X/L/TRFXLIH128F9308ACD/01006f8d14cc866a3bca857f14d5b0fe.mid',\n",
        "                                                     'L/W/P/TRLWPRD128F424FF0B/6217065d714d93ee66e3069fe7237f07.mid',\n",
        "                                                     'K/Y/H/TRKYHRD128F9302FDE/6b9e2c4794953a1af54549d27bd0689f.mid',\n",
        "                                                     'B/Y/U/TRBYUSU12903CF113E/d02da3544d75f07c668305af590ae38e.mid']])\n",
        "    # val_loader = cycle(val_loader_)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\"Let's go!\")\n",
        "        constrain_fn = partial(valid_events, dataset.vocal_vocab)\n",
        "        for v in vals:\n",
        "            start_time = time.time()\n",
        "            (instrumental, instrumental_mask), (expected_lyrics, expected_lyrics_mask), (expected_vocals, expected_vocals_mask), file = v\n",
        "\n",
        "            instrumental = instrumental[0].view(1, -1)\n",
        "            instrumental_mask = instrumental_mask[0].view(1, -1)\n",
        "            \n",
        "            # <bos> token\n",
        "            vocals_start = torch.ones(1,1).long()\n",
        "            lyrics_start = torch.full((1,1), dataset.tokenizer.bos_token_id).long()\n",
        "\n",
        "            print(file)\n",
        "\n",
        "            lyrics, vocals = model.generate(instrumental=instrumental.to(device),\n",
        "                                                          lyrics_start=lyrics_start.to(device),\n",
        "                                                          lyrics_len=args.max_lyrics_sequence_length,\n",
        "                                                          vocals_start=vocals_start.to(device),\n",
        "                                                          vocals_len=dataset.max_vocal_length,\n",
        "                                                          enc_mask=instrumental_mask.to(device),\n",
        "                                                          lm_eos_token=dataset.tokenizer.eos_token_id,\n",
        "                                                          dec_eos_token=2,\n",
        "                                                          dec_constrain_fn=constrain_fn)\n",
        "            decoded_lyrics = dataset.decode(lyrics[0], seq_type='lyrics')\n",
        "            decoded_vocals = dataset.decode(vocals[0], seq_type='vocals')\n",
        "\n",
        "            print((time.time() - start_time)/(len(decoded_vocals)+len(lyrics[0])))\n",
        "            with open(os.path.join(args.save_dir, 'the_output_examples.txt'), 'a') as f:\n",
        "                f.write(\"{}:\\n\\n{}\\n----------------\\n{}\\n----------------\\n\\n\"\\\n",
        "                                .format(file, decoded_lyrics, decoded_vocals))\n",
        "            \n",
        "            vsm = valid_structure_metric(vocals[0], dataset.vocal_vocab)\n",
        "            print(\"Valid Structure Metric: {}\".format(vsm))\n",
        "            print(\"------------------\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_decoupled.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf6fKZWNZAwk",
        "outputId": "1de6ed73-a1ac-4493-b6f0-f1b29792d441"
      },
      "source": [
        "!python3 generate_decoupled.py -df drive/MyDrive/vanilla_performer/dataset_chords.parquet -v drive/MyDrive/decoupled_performer/decoupled_chords_ -pm drive/MyDrive/decoupled_performer/chords/model.pt -tok distilgpt2 -sd drive/MyDrive/decoupled_performer/chords"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's go!\n",
            "F/X/L/TRFXLIH128F9308ACD/01006f8d14cc866a3bca857f14d5b0fe.mid\n",
            "2021-03-05 01:16:55.636762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "0.11611199835008944\n",
            "Valid Structure Metric: 1.0\n",
            "------------------\n",
            "W/E/U/TRWEUHA12903D01A39/e9710a3f0160b067065e190038fbffaa.mid\n",
            "0.10427241342844981\n",
            "Valid Structure Metric: 1.0\n",
            "------------------\n",
            "L/W/P/TRLWPRD128F424FF0B/6217065d714d93ee66e3069fe7237f07.mid\n",
            "0.1800594878961324\n",
            "Valid Structure Metric: 1.0\n",
            "------------------\n",
            "B/Y/U/TRBYUSU12903CF113E/d02da3544d75f07c668305af590ae38e.mid\n",
            "0.08003446646814066\n",
            "Valid Structure Metric: 1.0\n",
            "------------------\n",
            "K/Y/H/TRKYHRD128F9302FDE/6b9e2c4794953a1af54549d27bd0689f.mid\n",
            "0.14112857899852305\n",
            "Valid Structure Metric: 1.0\n",
            "------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}